{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PaddleClas进行AI艺术鉴赏预测\n",
    "1. 下载好数据集--可以再下载PaddleClas进行相关训练 -- 数据集名称：AI艺术鉴赏挑战赛 - 看画猜作者  （已经传到AIStudio上）\n",
    "2. 修改学习率调整机制、网络模型等进行再训练,以求最高的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-19T02:34:19.348006Z",
     "iopub.status.busy": "2021-11-19T02:34:19.347604Z",
     "iopub.status.idle": "2021-11-19T02:34:21.948133Z",
     "shell.execute_reply": "2021-11-19T02:34:21.947254Z",
     "shell.execute_reply.started": "2021-11-19T02:34:19.347974Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np           \n",
    "import pandas as pds         \n",
    "from matplotlib import pyplot as plt    \n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph.nn as nn\n",
    "from paddle.fluid import dygraph\n",
    "from paddle.fluid.dygraph import Layer\n",
    "from paddle.fluid.dygraph.nn import Linear, Conv2D, Pool2D, BatchNorm, LayerNorm, Dropout\n",
    "from paddle.fluid.layers import resize_nearest, flatten              # 向上采样\n",
    "from PIL import Image\n",
    "from math import ceil            # ceil 是一个向上取整\n",
    "\n",
    "import os\n",
    "import zipfile as zipf\n",
    "import random      \n",
    "\n",
    "random.seed(10)      # 随机数种子\n",
    "np.random.seed(10)   # 随机数种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、查看数据集长度和分类情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T02:50:51.538011Z",
     "iopub.status.busy": "2021-11-18T02:50:51.536782Z",
     "iopub.status.idle": "2021-11-18T02:50:51.553326Z",
     "shell.execute_reply": "2021-11-18T02:50:51.552531Z",
     "shell.execute_reply.started": "2021-11-18T02:50:51.537959Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets of length is 7227.\n",
      "The labels of range is [0, 48].\n"
     ]
    }
   ],
   "source": [
    "# 加载指定的csv训练集文件--得到训练标签以及数据集长度信息\n",
    "def csv_load(csv_path = 'work/datasets/train.csv'):\n",
    "    \n",
    "    csv_data = pds.read_csv(csv_path)    # pandas读取csv文件\n",
    "    csv_data = pds.DataFrame(csv_data)   # 转换为DataFrame格式\n",
    "    csv_data = np.array(csv_data)        # 将pandas.DataFrame转化未numpy数据格式\n",
    "    \n",
    "    # 数据集由图片文件id+对应的label组成\n",
    "    image_ids, lable_ids = csv_data[:,0], csv_data[:,1]    # 将数据划分--narray数据\n",
    "    length_img = len(image_ids)                            # 图片数量--训练集长度\n",
    "    range_label = [lable_ids.min(), lable_ids.max()]       # 标签范围\n",
    "\n",
    "    print(\"The datasets of length is {0}.\".format(length_img))   # 打印信息\n",
    "    print(\"The labels of range is {0}.\".format(range_label))\n",
    "\n",
    "    return image_ids, lable_ids\n",
    "\n",
    "# 执行数据信息获取\n",
    "image_id, label_id = csv_load()  # 49个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、划分数据 -- 由于只有一个训练集，需要人为手动划分数据集--提供eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T02:50:55.141866Z",
     "iopub.status.busy": "2021-11-18T02:50:55.140746Z",
     "iopub.status.idle": "2021-11-18T02:50:55.149665Z",
     "shell.execute_reply": "2021-11-18T02:50:55.148801Z",
     "shell.execute_reply.started": "2021-11-18T02:50:55.141812Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def data_divide(image_id=[], label_id=[],train_percen=0.7, eval_percen=0.2, test_percen=0.1):\n",
    "    '''\n",
    "        默认以为：0.7， 0.2， 0.1划分数据集\n",
    "    '''\n",
    "    rate = train_percen + eval_percen + test_percen  # 总占比\n",
    "    assert rate <= 1.0 and rate >=rate, \\\n",
    "            \"--train_percen({0:.2f}) + eval_percen({1:.2f}) + test_percen({2:.2f}) = {3:.2f}, however it is not 1!\".format(train_percen, eval_percen, test_percen, train_percen + eval_percen + test_percen)\n",
    "\n",
    "    lens = len(image_id)  # 数据集长度\n",
    "\n",
    "    # 各部分索引范围\n",
    "    indexs_train_start = 0\n",
    "    indexs_train_end = int(lens * train_percen)\n",
    "    indexs_eval_start = indexs_train_end\n",
    "    indexs_eval_end = int(lens * (train_percen+eval_percen))\n",
    "    indexs_test_start = indexs_eval_end\n",
    "    indexs_test_end = lens\n",
    "    \n",
    "    # 不需要打乱\n",
    "    shuffle_indexs = [i for i in range(lens)]   # 设置数据集index\n",
    "    # np.random.shuffle(shuffle_indexs)          # 打乱index--然后按index取数据即可\n",
    "    \n",
    "    train_data = (image_id[shuffle_indexs[indexs_train_start:indexs_train_end]], \\\n",
    "                    label_id[shuffle_indexs[indexs_train_start:indexs_train_end]])\n",
    "\n",
    "    eval_data = (image_id[shuffle_indexs[indexs_eval_start:indexs_eval_end]], \\\n",
    "                    label_id[shuffle_indexs[indexs_eval_start:indexs_eval_end]])\n",
    "    test_data = (image_id[shuffle_indexs[indexs_test_start:indexs_test_end]], \\\n",
    "                    label_id[shuffle_indexs[indexs_test_start:indexs_test_end]])\n",
    "    \n",
    "    return train_data, eval_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T02:50:58.433494Z",
     "iopub.status.busy": "2021-11-18T02:50:58.432741Z",
     "iopub.status.idle": "2021-11-18T02:50:58.445454Z",
     "shell.execute_reply": "2021-11-18T02:50:58.444435Z",
     "shell.execute_reply.started": "2021-11-18T02:50:58.433422Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 3 2 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "# 执行数据集划分\n",
    "train_datas, eval_datas, test_datas = data_divide(image_id=image_id, label_id=label_id, train_percen=0.8, eval_percen=0.2, test_percen=0.0)\n",
    "train_img_id , train_lab_id = train_datas\n",
    "eval_img_id , eval_lab_id = eval_datas\n",
    "test_img_id , test_lab_id = test_datas  # 无test数据\n",
    "print(train_img_id[0:10])\n",
    "print(train_lab_id[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、产生paddle_clas需要的文本索引\n",
    "\n",
    "1. train.txt -- 包含划分好的数据集id，以及分类序号\n",
    "2. eval.txtt -- 包含划分好的数据集id，以及分类序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T02:51:01.048839Z",
     "iopub.status.busy": "2021-11-18T02:51:01.047718Z",
     "iopub.status.idle": "2021-11-18T02:51:01.081724Z",
     "shell.execute_reply": "2021-11-18T02:51:01.081047Z",
     "shell.execute_reply.started": "2021-11-18T02:51:01.048782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 给获得的数据进行拼接--image 《==》 label\n",
    "train_data = np.concatenate((np.array(train_img_id).reshape((-1, 1)) , np.array(train_lab_id).reshape((-1, 1))) , axis=-1).astype('str')\n",
    "eval_data = np.concatenate((np.array(eval_img_id).reshape((-1, 1)) , np.array(eval_lab_id).reshape((-1, 1))) , axis=-1).astype('str')\n",
    "\n",
    "# 给训练数据序号添加图片类型后缀\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i, 0] = str(train_data[i, 0] + r'.jpg')\n",
    "# 给eval保存\n",
    "for i in range(len(eval_data)):\n",
    "    eval_data[i, 0] = str(eval_data[i, 0] + r'.jpg')\n",
    "\n",
    "# 将训练数据文本保存\n",
    "train_data = pds.DataFrame(train_data)\n",
    "train_data.to_csv('work/datasets/train_clas.txt', sep=' ', header=0, index=0)    # 保存paddle-cls需要的数据索引文本\n",
    "# 将eval数据文本保存\n",
    "eval_data = pds.DataFrame(eval_data)\n",
    "eval_data.to_csv('work/datasets/eval_clas.txt', sep=' ', header=0, index=0)    # 保存paddle-cls需要的数据索引文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、开始使用PaddleClas进行训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-19T08:24:05.050832Z",
     "iopub.status.busy": "2021-11-19T08:24:05.050295Z",
     "iopub.status.idle": "2021-11-19T08:24:05.055829Z",
     "shell.execute_reply": "2021-11-19T08:24:05.055294Z",
     "shell.execute_reply.started": "2021-11-19T08:24:05.050791Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/PaddleClas\n"
     ]
    }
   ],
   "source": [
    "# 跳转工作路径\n",
    "%cd /home/aistudio/PaddleClas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T09:12:15.291524Z",
     "iopub.status.busy": "2021-11-18T09:12:15.290474Z",
     "iopub.status.idle": "2021-11-18T09:12:15.298272Z",
     "shell.execute_reply": "2021-11-18T09:12:15.297617Z",
     "shell.execute_reply.started": "2021-11-18T09:12:15.291483Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/PaddleClas\n",
      "env: PYTHONPATH=.:$PYTHONPATH  # 配置python解释器路径\n",
      "env: CUDA_VISIBLE_DEVICES=0    # 配置CUDA驱动号\n"
     ]
    }
   ],
   "source": [
    "# 下载需要的依赖项 -- 第一次配置环境需要下载\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade -r requirements.txt  # 能使用就不需要再操作了\n",
    "\n",
    "%env PYTHONPATH=.:$PYTHONPATH  # 配置python解释器路径\n",
    "%env CUDA_VISIBLE_DEVICES=0    # 配置CUDA驱动号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:43:00.076203Z",
     "iopub.status.busy": "2021-11-18T11:43:00.075445Z",
     "iopub.status.idle": "2021-11-18T11:43:00.465967Z",
     "shell.execute_reply": "2021-11-18T11:43:00.465160Z",
     "shell.execute_reply.started": "2021-11-18T11:43:00.076169Z"
    }
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0       #设置GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 下载预训练模型 -- 根据需要进行修改\n",
    "# !python tools/download.py -a ResNet50_ACNet_deploy -p ./pretrained -d True\n",
    "# !python tools/download.py -a Res2Net50_26w_4s -p ./pretrained -d True\n",
    "# !python tools/download.py -a ResNeXt152_vd_32x4d -p ./pretrained -d True \n",
    "# !python tools/download.py -a EfficientNetB4\t -p ./pretrained -d True \n",
    "# !python tools/download.py -a EfficientNetB3\t -p ./pretrained -d True \n",
    "# !python tools/download.py -a ResNet50_ACNet_deploy -p ./pretrained/ -d True\n",
    "!python tools/download.py -a ResNeXt101_32x8d_wsl -p ./pretrained/ -d True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 注意查看yml模型文件中的训练集路径以及验证集路径--及时修改成自己的数据集路径\n",
    "#EfficientB4\n",
    "!python -m paddle.distributed.launch \\\n",
    "    tools/train.py \\\n",
    "        -c ./configs/EfficientNet/EfficientNetB4.yaml \\\n",
    "        # -o pretrained_model=./output/EfficientNetB4/best_model/ppcls    #基于已训练的部分继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNeXt101_32x8d_wsl\n",
    "!python -m paddle.distributed.launch tools/train.py -c ./configs/ResNeXt101_32x8d_wsl.yaml\\\n",
    "-o pretrained_model=./output/ResNeXt101_32x8d_wsl/best_model/ppcls    #基于已训练的部分继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对ResNeXt101_32x8d_wsl更换策略继续训练\n",
    "!python -m paddle.distributed.launch tools/train.py -c ./configs/ResNeXt101_32x8d_wsl-2.yaml -o pretrained_model=./output/ResNeXt101_32x8d_wsl/best_model/ppcls    #基于已训练的部分继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T08:24:32.599895Z",
     "iopub.status.busy": "2021-11-19T08:24:32.598935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  Configuration Arguments -----------\n",
      "cluster_node_ips: 127.0.0.1\n",
      "log_dir: None\n",
      "log_level: 20\n",
      "node_ip: 127.0.0.1\n",
      "print_config: True\n",
      "selected_gpus: None\n",
      "started_port: None\n",
      "training_script: tools/train.py\n",
      "training_script_args: ['-c', './configs/EfficientNet/EfficientNetB3.yaml', '-o', 'pretrained_model=./output/EfficientNetB3/best_model/ppcls']\n",
      "use_paddlecloud: False\n",
      "------------------------------------------------\n",
      "INFO 2021-11-19 16:24:33,938 launch.py:210] get cluster from args:job_server:None pods:['rank:0 id:None addr:127.0.0.1 port:None visible_gpu:[] trainers:[\"gpu:[\\'0\\'] endpoint:127.0.0.1:52857 rank:0\"]'] job_stage_flag:None hdfs:None\n",
      "INFO 2021-11-19 16:24:33,940 utils.py:367] start trainer proc:['/opt/conda/envs/python35-paddle120-env/bin/python', '-u', 'tools/train.py', '-c', './configs/EfficientNet/EfficientNetB3.yaml', '-o', 'pretrained_model=./output/EfficientNetB3/best_model/ppcls'] env:{'FLAGS_selected_gpus': '0', 'PADDLE_TRAINER_ID': '0', 'PADDLE_CURRENT_ENDPOINT': '127.0.0.1:52857', 'PADDLE_TRAINERS_NUM': '1', 'PADDLE_TRAINER_ENDPOINTS': '127.0.0.1:52857'}\n",
      "2021-11-19 16:24:35,020-INFO: \n",
      "===========================================================\n",
      "==        PaddleClas is powered by PaddlePaddle !        ==\n",
      "===========================================================\n",
      "==                                                       ==\n",
      "==   For more info please go to the following website.   ==\n",
      "==                                                       ==\n",
      "==       https://github.com/PaddlePaddle/PaddleClas      ==\n",
      "===========================================================\n",
      "\n",
      "2021-11-19 16:24:35,020-INFO: ARCHITECTURE : \n",
      "2021-11-19 16:24:35,020-INFO:     name : EfficientNetB3\n",
      "2021-11-19 16:24:35,020-INFO: ------------------------------------------------------------\n",
      "2021-11-19 16:24:35,020-INFO: LEARNING_RATE : \n",
      "2021-11-19 16:24:35,020-INFO:     function : CosineWarmup\n",
      "2021-11-19 16:24:35,020-INFO:     params : \n",
      "2021-11-19 16:24:35,020-INFO:         lr : 0.001\n",
      "2021-11-19 16:24:35,020-INFO: ------------------------------------------------------------\n",
      "2021-11-19 16:24:35,020-INFO: OPTIMIZER : \n",
      "2021-11-19 16:24:35,020-INFO:     function : Momentum\n",
      "2021-11-19 16:24:35,021-INFO:     params : \n",
      "2021-11-19 16:24:35,021-INFO:         momentum : 0.9\n",
      "2021-11-19 16:24:35,021-INFO:     regularizer : \n",
      "2021-11-19 16:24:35,021-INFO:         factor : 1e-06\n",
      "2021-11-19 16:24:35,021-INFO:         function : L2\n",
      "2021-11-19 16:24:35,021-INFO: ------------------------------------------------------------\n",
      "2021-11-19 16:24:35,021-INFO: TRAIN : \n",
      "2021-11-19 16:24:35,021-INFO:     batch_size : 24\n",
      "2021-11-19 16:24:35,021-INFO:     data_dir : /home/aistudio/work/datasets/train/\n",
      "2021-11-19 16:24:35,021-INFO:     file_list : /home/aistudio/work/datasets/train_clas.txt\n",
      "2021-11-19 16:24:35,021-INFO:     mix : \n",
      "2021-11-19 16:24:35,021-INFO:         MixupOperator : \n",
      "2021-11-19 16:24:35,021-INFO:             alpha : 0.2\n",
      "2021-11-19 16:24:35,021-INFO:     num_workers : 10\n",
      "2021-11-19 16:24:35,021-INFO:     shuffle_seed : 0\n",
      "2021-11-19 16:24:35,021-INFO:     transforms : \n",
      "2021-11-19 16:24:35,021-INFO:         DecodeImage : \n",
      "2021-11-19 16:24:35,021-INFO:             channel_first : False\n",
      "2021-11-19 16:24:35,021-INFO:             to_np : False\n",
      "2021-11-19 16:24:35,021-INFO:             to_rgb : True\n",
      "2021-11-19 16:24:35,021-INFO:         ResizeImage : \n",
      "2021-11-19 16:24:35,021-INFO:             resize_short : 660\n",
      "2021-11-19 16:24:35,021-INFO:         RandCropImage : \n",
      "2021-11-19 16:24:35,021-INFO:             size : 600\n",
      "2021-11-19 16:24:35,021-INFO:         RandFlipImage : \n",
      "2021-11-19 16:24:35,021-INFO:             flip_code : 1\n",
      "2021-11-19 16:24:35,021-INFO:         NormalizeImage : \n",
      "2021-11-19 16:24:35,021-INFO:             mean : [0.485, 0.456, 0.406]\n",
      "2021-11-19 16:24:35,022-INFO:             order : \n",
      "2021-11-19 16:24:35,022-INFO:             scale : 1./255.\n",
      "2021-11-19 16:24:35,022-INFO:             std : [0.229, 0.224, 0.225]\n",
      "2021-11-19 16:24:35,022-INFO:         ToCHWImage : None\n",
      "2021-11-19 16:24:35,022-INFO: ------------------------------------------------------------\n",
      "2021-11-19 16:24:35,022-INFO: VALID : \n",
      "2021-11-19 16:24:35,022-INFO:     batch_size : 16\n",
      "2021-11-19 16:24:35,022-INFO:     data_dir : /home/aistudio/work/datasets/train/\n",
      "2021-11-19 16:24:35,022-INFO:     file_list : /home/aistudio/work/datasets/eval_clas.txt\n",
      "2021-11-19 16:24:35,022-INFO:     num_workers : 10\n",
      "2021-11-19 16:24:35,022-INFO:     shuffle_seed : 0\n",
      "2021-11-19 16:24:35,022-INFO:     transforms : \n",
      "2021-11-19 16:24:35,022-INFO:         DecodeImage : \n",
      "2021-11-19 16:24:35,022-INFO:             channel_first : False\n",
      "2021-11-19 16:24:35,022-INFO:             to_np : False\n",
      "2021-11-19 16:24:35,022-INFO:             to_rgb : True\n",
      "2021-11-19 16:24:35,022-INFO:         ResizeImage : \n",
      "2021-11-19 16:24:35,022-INFO:             resize_short : 660\n",
      "2021-11-19 16:24:35,022-INFO:         CropImage : \n",
      "2021-11-19 16:24:35,022-INFO:             size : 600\n",
      "2021-11-19 16:24:35,022-INFO:         NormalizeImage : \n",
      "2021-11-19 16:24:35,022-INFO:             mean : [0.485, 0.456, 0.406]\n",
      "2021-11-19 16:24:35,022-INFO:             order : \n",
      "2021-11-19 16:24:35,022-INFO:             scale : 1.0/255.0\n",
      "2021-11-19 16:24:35,022-INFO:             std : [0.229, 0.224, 0.225]\n",
      "2021-11-19 16:24:35,022-INFO:         ToCHWImage : None\n",
      "2021-11-19 16:24:35,022-INFO: ------------------------------------------------------------\n",
      "2021-11-19 16:24:35,022-INFO: classes_num : 49\n",
      "2021-11-19 16:24:35,023-INFO: epochs : 400\n",
      "2021-11-19 16:24:35,023-INFO: image_shape : [3, 600, 600]\n",
      "2021-11-19 16:24:35,023-INFO: ls_epsilon : 0.1\n",
      "2021-11-19 16:24:35,023-INFO: mode : train\n",
      "2021-11-19 16:24:35,023-INFO: model_save_dir : ./output/\n",
      "2021-11-19 16:24:35,023-INFO: pretrained_model : ./output/EfficientNetB3/best_model/ppcls\n",
      "2021-11-19 16:24:35,023-INFO: save_interval : 10\n",
      "2021-11-19 16:24:35,023-INFO: topk : 5\n",
      "2021-11-19 16:24:35,023-INFO: total_images : 7227\n",
      "2021-11-19 16:24:35,023-INFO: use_mix : True\n",
      "2021-11-19 16:24:35,023-INFO: valid_interval : 10\n",
      "2021-11-19 16:24:35,023-INFO: validate : True\n",
      "\n",
      "\n",
      "API is deprecated since 2.0.0 Please use FleetAPI instead.\n",
      "WIKI: https://github.com/PaddlePaddle/Fleet/blob/develop/markdown_doc/transpiler\n",
      "\n",
      "        \n",
      "W1119 16:24:38.633275   437 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0\n",
      "W1119 16:24:38.638700   437 device_context.cc:260] device: 0, cuDNN Version: 7.6.\n",
      "2021-11-19 16:24:41,774-INFO: Loading parameters from ./output/EfficientNetB3/best_model/ppcls...\n",
      "2021-11-19 16:24:42,322-INFO: Finish initing model from ['./output/EfficientNetB3/best_model/ppcls']\n",
      "2021-11-19 16:24:51,992-INFO: epoch:0   train step:0    loss:  2.4662 lr: 0.000000 elapse: 9.649s\n",
      "2021-11-19 16:24:52,691-INFO: epoch:0   train step:1    loss:  2.3521 lr: 0.000001 elapse: 0.699s\n",
      "2021-11-19 16:24:53,404-INFO: epoch:0   train step:2    loss:  1.0144 lr: 0.000001 elapse: 0.713s\n",
      "2021-11-19 16:24:54,191-INFO: epoch:0   train step:3    loss:  1.4346 lr: 0.000002 elapse: 0.787s\n",
      "2021-11-19 16:24:54,993-INFO: epoch:0   train step:4    loss:  0.9873 lr: 0.000003 elapse: 0.802s\n",
      "2021-11-19 16:24:55,701-INFO: epoch:0   train step:5    loss:  1.7452 lr: 0.000003 elapse: 0.708s\n",
      "2021-11-19 16:24:56,489-INFO: epoch:0   train step:6    loss:  1.0037 lr: 0.000004 elapse: 0.788s\n",
      "2021-11-19 16:24:57,303-INFO: epoch:0   train step:7    loss:  0.8649 lr: 0.000005 elapse: 0.814s\n",
      "2021-11-19 16:24:58,091-INFO: epoch:0   train step:8    loss:  1.8260 lr: 0.000005 elapse: 0.788s\n",
      "2021-11-19 16:24:58,802-INFO: epoch:0   train step:9    loss:  1.1207 lr: 0.000006 elapse: 0.711s\n",
      "2021-11-19 16:24:59,503-INFO: epoch:0   train step:10   loss:  1.1861 lr: 0.000007 elapse: 0.701s\n",
      "2021-11-19 16:25:06,591-INFO: epoch:0   train step:20   loss:  0.9093 lr: 0.000013 elapse: 0.699s\n",
      "2021-11-19 16:25:13,691-INFO: epoch:0   train step:30   loss:  2.5202 lr: 0.000020 elapse: 0.699s\n",
      "2021-11-19 16:25:20,691-INFO: epoch:0   train step:40   loss:  2.1886 lr: 0.000027 elapse: 0.697s\n",
      "2021-11-19 16:25:27,723-INFO: epoch:0   train step:50   loss:  1.1583 lr: 0.000033 elapse: 0.719s\n",
      "2021-11-19 16:25:34,791-INFO: epoch:0   train step:60   loss:  2.0428 lr: 0.000040 elapse: 0.767s\n",
      "2021-11-19 16:25:41,791-INFO: epoch:0   train step:70   loss:  1.1516 lr: 0.000047 elapse: 0.699s\n",
      "2021-11-19 16:25:48,791-INFO: epoch:0   train step:80   loss:  1.1027 lr: 0.000053 elapse: 0.699s\n",
      "2021-11-19 16:25:55,806-INFO: epoch:0   train step:90   loss:  1.2428 lr: 0.000060 elapse: 0.704s\n",
      "2021-11-19 16:26:03,206-INFO: epoch:0   train step:100  loss:  0.9955 lr: 0.000066 elapse: 0.713s\n",
      "2021-11-19 16:26:10,295-INFO: epoch:0   train step:110  loss:  1.0292 lr: 0.000073 elapse: 0.701s\n",
      "2021-11-19 16:26:17,391-INFO: epoch:0   train step:120  loss:  0.9451 lr: 0.000080 elapse: 0.763s\n",
      "2021-11-19 16:26:24,391-INFO: epoch:0   train step:130  loss:  2.4245 lr: 0.000086 elapse: 0.699s\n",
      "2021-11-19 16:26:31,391-INFO: epoch:0   train step:140  loss:  0.9283 lr: 0.000093 elapse: 0.689s\n",
      "2021-11-19 16:26:38,401-INFO: epoch:0   train step:150  loss:  1.0006 lr: 0.000100 elapse: 0.686s\n",
      "2021-11-19 16:26:45,491-INFO: epoch:0   train step:160  loss:  1.8500 lr: 0.000106 elapse: 0.698s\n",
      "2021-11-19 16:26:52,491-INFO: epoch:0   train step:170  loss:  2.6065 lr: 0.000113 elapse: 0.698s\n",
      "2021-11-19 16:26:59,491-INFO: epoch:0   train step:180  loss:  2.2707 lr: 0.000120 elapse: 0.698s\n",
      "2021-11-19 16:27:06,891-INFO: epoch:0   train step:190  loss:  1.3076 lr: 0.000126 elapse: 0.701s\n",
      "2021-11-19 16:27:13,801-INFO: epoch:0   train step:200  loss:  0.9459 lr: 0.000133 elapse: 0.666s\n",
      "2021-11-19 16:27:20,465-INFO: epoch:0   train step:210  loss:  2.9016 lr: 0.000140 elapse: 0.668s\n",
      "2021-11-19 16:27:27,203-INFO: epoch:0   train step:220  loss:  0.8826 lr: 0.000146 elapse: 0.738s\n",
      "2021-11-19 16:27:34,065-INFO: epoch:0   train step:230  loss:  0.9075 lr: 0.000153 elapse: 0.742s\n",
      "2021-11-19 16:27:40,147-INFO: END epoch:0   train loss_avg:  1.3977  elapse_sum: 177.799s\n",
      "2021-11-19 16:27:45,003-INFO: epoch:0   valid step:0    loss:  1.3525 top1: 0.6875 top5: 1.0000 elapse: 4.854s\n",
      "2021-11-19 16:27:46,215-INFO: epoch:0   valid step:1    loss:  1.1004 top1: 0.8750 top5: 1.0000 elapse: 1.213s\n",
      "2021-11-19 16:27:47,606-INFO: epoch:0   valid step:2    loss:  1.3281 top1: 0.8750 top5: 0.9375 elapse: 1.391s\n",
      "2021-11-19 16:27:48,236-INFO: epoch:0   valid step:3    loss:  1.0503 top1: 0.9375 top5: 1.0000 elapse: 0.630s\n",
      "2021-11-19 16:27:48,462-INFO: epoch:0   valid step:4    loss:  1.0925 top1: 0.9375 top5: 1.0000 elapse: 0.226s\n",
      "2021-11-19 16:27:48,719-INFO: epoch:0   valid step:5    loss:  1.1906 top1: 0.8125 top5: 1.0000 elapse: 0.257s\n",
      "2021-11-19 16:27:48,930-INFO: epoch:0   valid step:6    loss:  0.9271 top1: 1.0000 top5: 1.0000 elapse: 0.211s\n",
      "2021-11-19 16:27:49,180-INFO: epoch:0   valid step:7    loss:  1.3298 top1: 0.7500 top5: 0.9375 elapse: 0.250s\n",
      "2021-11-19 16:27:49,381-INFO: epoch:0   valid step:8    loss:  1.9923 top1: 0.6250 top5: 0.8750 elapse: 0.201s\n",
      "2021-11-19 16:27:49,620-INFO: epoch:0   valid step:9    loss:  1.5872 top1: 0.7500 top5: 0.8750 elapse: 0.239s\n",
      "2021-11-19 16:27:49,832-INFO: epoch:0   valid step:10   loss:  1.3355 top1: 0.7500 top5: 0.9375 elapse: 0.212s\n",
      "2021-11-19 16:27:52,347-INFO: epoch:0   valid step:20   loss:  1.4318 top1: 0.7500 top5: 1.0000 elapse: 0.213s\n",
      "2021-11-19 16:27:54,439-INFO: epoch:0   valid step:30   loss:  1.2787 top1: 0.8750 top5: 0.9375 elapse: 0.188s\n",
      "2021-11-19 16:27:56,320-INFO: epoch:0   valid step:40   loss:  1.1051 top1: 0.8750 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 16:27:58,216-INFO: epoch:0   valid step:50   loss:  1.3938 top1: 0.7500 top5: 0.9375 elapse: 0.192s\n",
      "2021-11-19 16:28:00,188-INFO: epoch:0   valid step:60   loss:  1.1270 top1: 0.9375 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 16:28:02,163-INFO: epoch:0   valid step:70   loss:  1.1878 top1: 0.8125 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 16:28:04,252-INFO: epoch:0   valid step:80   loss:  1.4598 top1: 0.8125 top5: 0.9375 elapse: 0.188s\n",
      "2021-11-19 16:28:06,071-INFO: END epoch:0   valid loss_avg:  1.2470 top1_avg: 0.8472 top5_avg: 0.9632 elapse_sum: 25.916s\n",
      "2021-11-19 16:28:06,071-INFO: The best top1 acc 0.84722, in epoch: 0\n",
      "2021-11-19 16:28:07,235-INFO: Already save model in ./output/EfficientNetB3/best_model\n",
      "2021-11-19 16:28:08,327-INFO: Already save model in ./output/EfficientNetB3/0\n",
      "2021-11-19 16:28:15,993-INFO: epoch:1   train step:0    loss:  1.7590 lr: 0.000159 elapse: 7.664s\n",
      "2021-11-19 16:28:25,003-INFO: epoch:1   train step:10   loss:  0.8611 lr: 0.000166 elapse: 0.699s\n",
      "2021-11-19 16:28:32,025-INFO: epoch:1   train step:20   loss:  0.8500 lr: 0.000173 elapse: 0.722s\n",
      "2021-11-19 16:28:39,191-INFO: epoch:1   train step:30   loss:  1.1672 lr: 0.000179 elapse: 0.695s\n",
      "2021-11-19 16:28:46,201-INFO: epoch:1   train step:40   loss:  1.6759 lr: 0.000186 elapse: 0.698s\n",
      "2021-11-19 16:28:53,402-INFO: epoch:1   train step:50   loss:  2.1963 lr: 0.000193 elapse: 0.776s\n",
      "2021-11-19 16:29:00,791-INFO: epoch:1   train step:60   loss:  1.2627 lr: 0.000199 elapse: 0.755s\n",
      "2021-11-19 16:29:07,891-INFO: epoch:1   train step:70   loss:  0.9617 lr: 0.000206 elapse: 0.699s\n",
      "2021-11-19 16:29:15,124-INFO: epoch:1   train step:80   loss:  1.5114 lr: 0.000213 elapse: 0.722s\n",
      "2021-11-19 16:29:22,193-INFO: epoch:1   train step:90   loss:  1.5432 lr: 0.000219 elapse: 0.701s\n",
      "2021-11-19 16:29:29,191-INFO: epoch:1   train step:100  loss:  1.1789 lr: 0.000226 elapse: 0.700s\n",
      "2021-11-19 16:29:36,203-INFO: epoch:1   train step:110  loss:  0.9709 lr: 0.000233 elapse: 0.699s\n",
      "2021-11-19 16:29:43,291-INFO: epoch:1   train step:120  loss:  2.8353 lr: 0.000239 elapse: 0.698s\n",
      "2021-11-19 16:29:50,306-INFO: epoch:1   train step:130  loss:  2.3848 lr: 0.000246 elapse: 0.702s\n",
      "2021-11-19 16:29:57,502-INFO: epoch:1   train step:140  loss:  1.0156 lr: 0.000252 elapse: 0.699s\n",
      "2021-11-19 16:30:04,702-INFO: epoch:1   train step:150  loss:  1.2978 lr: 0.000259 elapse: 0.699s\n",
      "2021-11-19 16:30:11,826-INFO: epoch:1   train step:160  loss:  0.9393 lr: 0.000266 elapse: 0.723s\n",
      "2021-11-19 16:30:19,003-INFO: epoch:1   train step:170  loss:  1.4568 lr: 0.000272 elapse: 0.699s\n",
      "2021-11-19 16:30:26,091-INFO: epoch:1   train step:180  loss:  0.9686 lr: 0.000279 elapse: 0.699s\n",
      "2021-11-19 16:30:33,105-INFO: epoch:1   train step:190  loss:  1.5496 lr: 0.000286 elapse: 0.698s\n",
      "2021-11-19 16:30:40,114-INFO: epoch:1   train step:200  loss:  2.1226 lr: 0.000292 elapse: 0.678s\n",
      "2021-11-19 16:30:46,811-INFO: epoch:1   train step:210  loss:  2.0912 lr: 0.000299 elapse: 0.667s\n",
      "2021-11-19 16:30:53,707-INFO: epoch:1   train step:220  loss:  1.1445 lr: 0.000306 elapse: 0.675s\n",
      "2021-11-19 16:31:00,616-INFO: epoch:1   train step:230  loss:  1.7076 lr: 0.000312 elapse: 0.668s\n",
      "2021-11-19 16:31:06,650-INFO: END epoch:1   train loss_avg:  1.3873  elapse_sum: 178.319s\n",
      "2021-11-19 16:31:14,391-INFO: epoch:2   train step:0    loss:  1.2392 lr: 0.000319 elapse: 7.740s\n",
      "2021-11-19 16:31:23,704-INFO: epoch:2   train step:10   loss:  0.9289 lr: 0.000326 elapse: 0.700s\n",
      "2021-11-19 16:31:30,795-INFO: epoch:2   train step:20   loss:  0.9149 lr: 0.000332 elapse: 0.768s\n",
      "2021-11-19 16:31:37,993-INFO: epoch:2   train step:30   loss:  1.5533 lr: 0.000339 elapse: 0.764s\n",
      "2021-11-19 16:31:45,011-INFO: epoch:2   train step:40   loss:  1.0972 lr: 0.000346 elapse: 0.708s\n",
      "2021-11-19 16:31:52,122-INFO: epoch:2   train step:50   loss:  0.9860 lr: 0.000352 elapse: 0.720s\n",
      "2021-11-19 16:31:59,191-INFO: epoch:2   train step:60   loss:  0.8495 lr: 0.000359 elapse: 0.764s\n",
      "2021-11-19 16:32:06,191-INFO: epoch:2   train step:70   loss:  1.0676 lr: 0.000365 elapse: 0.698s\n",
      "2021-11-19 16:32:13,302-INFO: epoch:2   train step:80   loss:  1.0097 lr: 0.000372 elapse: 0.698s\n",
      "2021-11-19 16:32:20,391-INFO: epoch:2   train step:90   loss:  2.5839 lr: 0.000379 elapse: 0.697s\n",
      "2021-11-19 16:32:27,791-INFO: epoch:2   train step:100  loss:  1.2089 lr: 0.000385 elapse: 0.699s\n",
      "2021-11-19 16:32:34,803-INFO: epoch:2   train step:110  loss:  0.9976 lr: 0.000392 elapse: 0.700s\n",
      "2021-11-19 16:32:41,905-INFO: epoch:2   train step:120  loss:  1.2064 lr: 0.000399 elapse: 0.702s\n",
      "2021-11-19 16:32:48,991-INFO: epoch:2   train step:130  loss:  2.3724 lr: 0.000405 elapse: 0.698s\n",
      "2021-11-19 16:32:56,091-INFO: epoch:2   train step:140  loss:  0.9905 lr: 0.000412 elapse: 0.688s\n",
      "2021-11-19 16:33:03,193-INFO: epoch:2   train step:150  loss:  2.7259 lr: 0.000419 elapse: 0.700s\n",
      "2021-11-19 16:33:10,307-INFO: epoch:2   train step:160  loss:  0.9578 lr: 0.000425 elapse: 0.700s\n",
      "2021-11-19 16:33:17,419-INFO: epoch:2   train step:170  loss:  2.5862 lr: 0.000432 elapse: 0.716s\n",
      "2021-11-19 16:33:24,491-INFO: epoch:2   train step:180  loss:  1.0333 lr: 0.000439 elapse: 0.698s\n",
      "2021-11-19 16:33:31,891-INFO: epoch:2   train step:190  loss:  1.7850 lr: 0.000445 elapse: 0.764s\n",
      "2021-11-19 16:33:38,803-INFO: epoch:2   train step:200  loss:  0.9166 lr: 0.000452 elapse: 0.667s\n",
      "2021-11-19 16:33:45,496-INFO: epoch:2   train step:210  loss:  1.2410 lr: 0.000458 elapse: 0.665s\n",
      "2021-11-19 16:33:52,432-INFO: epoch:2   train step:220  loss:  1.6847 lr: 0.000465 elapse: 0.666s\n",
      "2021-11-19 16:33:59,366-INFO: epoch:2   train step:230  loss:  0.9019 lr: 0.000472 elapse: 0.665s\n",
      "2021-11-19 16:34:05,349-INFO: END epoch:2   train loss_avg:  1.4625  elapse_sum: 178.696s\n",
      "2021-11-19 16:34:13,693-INFO: epoch:3   train step:0    loss:  2.4502 lr: 0.000478 elapse: 8.343s\n",
      "2021-11-19 16:34:22,291-INFO: epoch:3   train step:10   loss:  1.3686 lr: 0.000485 elapse: 0.699s\n",
      "2021-11-19 16:34:29,291-INFO: epoch:3   train step:20   loss:  0.9054 lr: 0.000492 elapse: 0.698s\n",
      "2021-11-19 16:34:36,691-INFO: epoch:3   train step:30   loss:  1.0791 lr: 0.000498 elapse: 0.765s\n",
      "2021-11-19 16:34:43,791-INFO: epoch:3   train step:40   loss:  2.3544 lr: 0.000505 elapse: 0.763s\n",
      "2021-11-19 16:34:50,903-INFO: epoch:3   train step:50   loss:  1.9013 lr: 0.000512 elapse: 0.740s\n",
      "2021-11-19 16:34:58,103-INFO: epoch:3   train step:60   loss:  1.1026 lr: 0.000518 elapse: 0.700s\n",
      "2021-11-19 16:35:05,192-INFO: epoch:3   train step:70   loss:  0.9694 lr: 0.000525 elapse: 0.697s\n",
      "2021-11-19 16:35:12,202-INFO: epoch:3   train step:80   loss:  0.9903 lr: 0.000532 elapse: 0.699s\n",
      "2021-11-19 16:35:19,329-INFO: epoch:3   train step:90   loss:  1.1361 lr: 0.000538 elapse: 0.724s\n",
      "2021-11-19 16:35:26,492-INFO: epoch:3   train step:100  loss:  2.3903 lr: 0.000545 elapse: 0.700s\n",
      "2021-11-19 16:35:33,503-INFO: epoch:3   train step:110  loss:  1.1110 lr: 0.000551 elapse: 0.700s\n",
      "2021-11-19 16:35:40,804-INFO: epoch:3   train step:120  loss:  1.0277 lr: 0.000558 elapse: 0.701s\n",
      "2021-11-19 16:35:47,891-INFO: epoch:3   train step:130  loss:  2.4648 lr: 0.000565 elapse: 0.698s\n",
      "2021-11-19 16:35:55,125-INFO: epoch:3   train step:140  loss:  1.2813 lr: 0.000571 elapse: 0.722s\n",
      "2021-11-19 16:36:02,291-INFO: epoch:3   train step:150  loss:  1.2123 lr: 0.000578 elapse: 0.699s\n",
      "2021-11-19 16:36:09,391-INFO: epoch:3   train step:160  loss:  2.4428 lr: 0.000585 elapse: 0.697s\n",
      "2021-11-19 16:36:16,495-INFO: epoch:3   train step:170  loss:  2.7197 lr: 0.000591 elapse: 0.770s\n",
      "2021-11-19 16:36:23,501-INFO: epoch:3   train step:180  loss:  1.6089 lr: 0.000598 elapse: 0.708s\n",
      "2021-11-19 16:36:30,503-INFO: epoch:3   train step:190  loss:  2.1188 lr: 0.000605 elapse: 0.711s\n",
      "2021-11-19 16:36:37,505-INFO: epoch:3   train step:200  loss:  1.0597 lr: 0.000611 elapse: 0.667s\n",
      "2021-11-19 16:36:44,199-INFO: epoch:3   train step:210  loss:  0.9051 lr: 0.000618 elapse: 0.666s\n",
      "2021-11-19 16:36:51,198-INFO: epoch:3   train step:220  loss:  0.8515 lr: 0.000625 elapse: 0.667s\n",
      "2021-11-19 16:36:58,172-INFO: epoch:3   train step:230  loss:  1.0108 lr: 0.000631 elapse: 0.667s\n",
      "2021-11-19 16:37:04,168-INFO: END epoch:3   train loss_avg:  1.3875  elapse_sum: 178.816s\n",
      "2021-11-19 16:37:11,792-INFO: epoch:4   train step:0    loss:  1.3676 lr: 0.000638 elapse: 7.623s\n",
      "2021-11-19 16:37:20,604-INFO: epoch:4   train step:10   loss:  2.6267 lr: 0.000645 elapse: 0.702s\n",
      "2021-11-19 16:37:27,692-INFO: epoch:4   train step:20   loss:  1.1425 lr: 0.000651 elapse: 0.690s\n",
      "2021-11-19 16:37:34,791-INFO: epoch:4   train step:30   loss:  1.4429 lr: 0.000658 elapse: 0.699s\n",
      "2021-11-19 16:37:41,901-INFO: epoch:4   train step:40   loss:  1.0700 lr: 0.000664 elapse: 0.780s\n",
      "2021-11-19 16:37:49,119-INFO: epoch:4   train step:50   loss:  1.0481 lr: 0.000671 elapse: 0.715s\n",
      "2021-11-19 16:37:56,391-INFO: epoch:4   train step:60   loss:  0.9354 lr: 0.000678 elapse: 0.699s\n",
      "2021-11-19 16:38:03,491-INFO: epoch:4   train step:70   loss:  1.0412 lr: 0.000684 elapse: 0.764s\n",
      "2021-11-19 16:38:10,591-INFO: epoch:4   train step:80   loss:  0.9937 lr: 0.000691 elapse: 0.699s\n",
      "2021-11-19 16:38:17,601-INFO: epoch:4   train step:90   loss:  1.4577 lr: 0.000698 elapse: 0.698s\n",
      "2021-11-19 16:38:24,791-INFO: epoch:4   train step:100  loss:  0.9834 lr: 0.000704 elapse: 0.767s\n",
      "2021-11-19 16:38:31,901-INFO: epoch:4   train step:110  loss:  1.8679 lr: 0.000711 elapse: 0.775s\n",
      "2021-11-19 16:38:38,891-INFO: epoch:4   train step:120  loss:  0.8370 lr: 0.000718 elapse: 0.696s\n",
      "2021-11-19 16:38:45,924-INFO: epoch:4   train step:130  loss:  0.9885 lr: 0.000724 elapse: 0.721s\n",
      "2021-11-19 16:38:53,525-INFO: epoch:4   train step:140  loss:  0.9416 lr: 0.000731 elapse: 0.721s\n",
      "2021-11-19 16:39:00,604-INFO: epoch:4   train step:150  loss:  0.9636 lr: 0.000738 elapse: 0.712s\n",
      "2021-11-19 16:39:07,691-INFO: epoch:4   train step:160  loss:  1.1265 lr: 0.000744 elapse: 0.699s\n",
      "2021-11-19 16:39:14,803-INFO: epoch:4   train step:170  loss:  1.4831 lr: 0.000751 elapse: 0.701s\n",
      "2021-11-19 16:39:21,903-INFO: epoch:4   train step:180  loss:  0.8766 lr: 0.000757 elapse: 0.697s\n",
      "2021-11-19 16:39:29,093-INFO: epoch:4   train step:190  loss:  1.8268 lr: 0.000764 elapse: 0.700s\n",
      "2021-11-19 16:39:36,111-INFO: epoch:4   train step:200  loss:  2.5119 lr: 0.000771 elapse: 0.677s\n",
      "2021-11-19 16:39:42,791-INFO: epoch:4   train step:210  loss:  1.3202 lr: 0.000777 elapse: 0.666s\n",
      "2021-11-19 16:39:49,732-INFO: epoch:4   train step:220  loss:  0.9624 lr: 0.000784 elapse: 0.666s\n",
      "2021-11-19 16:39:56,560-INFO: epoch:4   train step:230  loss:  1.0642 lr: 0.000791 elapse: 0.666s\n",
      "2021-11-19 16:40:02,894-INFO: END epoch:4   train loss_avg:  1.3488  elapse_sum: 178.724s\n",
      "2021-11-19 16:40:10,693-INFO: epoch:5   train step:0    loss:  2.2035 lr: 0.000797 elapse: 7.796s\n",
      "2021-11-19 16:40:19,792-INFO: epoch:5   train step:10   loss:  0.9174 lr: 0.000804 elapse: 0.700s\n",
      "2021-11-19 16:40:26,802-INFO: epoch:5   train step:20   loss:  2.0708 lr: 0.000811 elapse: 0.700s\n",
      "2021-11-19 16:40:33,901-INFO: epoch:5   train step:30   loss:  0.8735 lr: 0.000817 elapse: 0.709s\n",
      "2021-11-19 16:40:40,903-INFO: epoch:5   train step:40   loss:  2.3760 lr: 0.000824 elapse: 0.700s\n",
      "2021-11-19 16:40:47,940-INFO: epoch:5   train step:50   loss:  1.0437 lr: 0.000831 elapse: 0.722s\n",
      "2021-11-19 16:40:55,234-INFO: epoch:5   train step:60   loss:  1.0856 lr: 0.000837 elapse: 0.732s\n",
      "2021-11-19 16:41:02,502-INFO: epoch:5   train step:70   loss:  1.2545 lr: 0.000844 elapse: 0.698s\n",
      "2021-11-19 16:41:09,591-INFO: epoch:5   train step:80   loss:  0.8803 lr: 0.000850 elapse: 0.698s\n",
      "2021-11-19 16:41:16,719-INFO: epoch:5   train step:90   loss:  1.0492 lr: 0.000857 elapse: 0.698s\n",
      "2021-11-19 16:41:23,901-INFO: epoch:5   train step:100  loss:  2.0714 lr: 0.000864 elapse: 0.698s\n",
      "2021-11-19 16:41:31,002-INFO: epoch:5   train step:110  loss:  1.0759 lr: 0.000870 elapse: 0.698s\n",
      "2021-11-19 16:41:37,991-INFO: epoch:5   train step:120  loss:  0.9705 lr: 0.000877 elapse: 0.699s\n",
      "2021-11-19 16:41:45,091-INFO: epoch:5   train step:130  loss:  1.0422 lr: 0.000884 elapse: 0.699s\n",
      "2021-11-19 16:41:52,303-INFO: epoch:5   train step:140  loss:  2.1513 lr: 0.000890 elapse: 0.721s\n",
      "2021-11-19 16:41:59,437-INFO: epoch:5   train step:150  loss:  1.0341 lr: 0.000897 elapse: 0.731s\n",
      "2021-11-19 16:42:06,803-INFO: epoch:5   train step:160  loss:  1.0957 lr: 0.000904 elapse: 0.699s\n",
      "2021-11-19 16:42:13,904-INFO: epoch:5   train step:170  loss:  1.8622 lr: 0.000910 elapse: 0.701s\n",
      "2021-11-19 16:42:20,992-INFO: epoch:5   train step:180  loss:  0.8847 lr: 0.000917 elapse: 0.700s\n",
      "2021-11-19 16:42:28,194-INFO: epoch:5   train step:190  loss:  0.9792 lr: 0.000924 elapse: 0.702s\n",
      "2021-11-19 16:42:35,111-INFO: epoch:5   train step:200  loss:  0.9648 lr: 0.000930 elapse: 0.669s\n",
      "2021-11-19 16:42:41,790-INFO: epoch:5   train step:210  loss:  1.2584 lr: 0.000937 elapse: 0.668s\n",
      "2021-11-19 16:42:48,738-INFO: epoch:5   train step:220  loss:  1.0171 lr: 0.000944 elapse: 0.665s\n",
      "2021-11-19 16:42:55,701-INFO: epoch:5   train step:230  loss:  2.4431 lr: 0.000950 elapse: 0.666s\n",
      "2021-11-19 16:43:01,742-INFO: END epoch:5   train loss_avg:  1.4254  elapse_sum: 178.845s\n",
      "2021-11-19 16:43:09,892-INFO: epoch:6   train step:0    loss:  1.0979 lr: 0.000957 elapse: 8.148s\n",
      "2021-11-19 16:43:19,091-INFO: epoch:6   train step:10   loss:  1.7735 lr: 0.000963 elapse: 0.699s\n",
      "2021-11-19 16:43:26,191-INFO: epoch:6   train step:20   loss:  1.0565 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 16:43:33,291-INFO: epoch:6   train step:30   loss:  2.1194 lr: 0.000977 elapse: 0.700s\n",
      "2021-11-19 16:43:40,392-INFO: epoch:6   train step:40   loss:  2.0741 lr: 0.000983 elapse: 0.700s\n",
      "2021-11-19 16:43:47,391-INFO: epoch:6   train step:50   loss:  1.6241 lr: 0.000990 elapse: 0.697s\n",
      "2021-11-19 16:43:54,501-INFO: epoch:6   train step:60   loss:  1.0440 lr: 0.000997 elapse: 0.695s\n",
      "2021-11-19 16:44:01,602-INFO: epoch:6   train step:70   loss:  1.0687 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:44:08,702-INFO: epoch:6   train step:80   loss:  0.8615 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:44:16,091-INFO: epoch:6   train step:90   loss:  1.1270 lr: 0.001000 elapse: 0.799s\n",
      "2021-11-19 16:44:23,203-INFO: epoch:6   train step:100  loss:  1.9038 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:44:30,302-INFO: epoch:6   train step:110  loss:  1.2487 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:44:37,321-INFO: epoch:6   train step:120  loss:  0.9477 lr: 0.001000 elapse: 0.717s\n",
      "2021-11-19 16:44:44,317-INFO: epoch:6   train step:130  loss:  1.0382 lr: 0.001000 elapse: 0.711s\n",
      "2021-11-19 16:44:51,591-INFO: epoch:6   train step:140  loss:  2.2380 lr: 0.001000 elapse: 0.893s\n",
      "2021-11-19 16:44:58,602-INFO: epoch:6   train step:150  loss:  0.9071 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:45:05,702-INFO: epoch:6   train step:160  loss:  2.2488 lr: 0.001000 elapse: 0.769s\n",
      "2021-11-19 16:45:12,781-INFO: epoch:6   train step:170  loss:  0.8754 lr: 0.001000 elapse: 0.746s\n",
      "2021-11-19 16:45:20,256-INFO: epoch:6   train step:180  loss:  1.1012 lr: 0.001000 elapse: 0.749s\n",
      "2021-11-19 16:45:27,522-INFO: epoch:6   train step:190  loss:  1.4824 lr: 0.001000 elapse: 0.719s\n",
      "2021-11-19 16:45:34,611-INFO: epoch:6   train step:200  loss:  2.6006 lr: 0.001000 elapse: 0.677s\n",
      "2021-11-19 16:45:41,337-INFO: epoch:6   train step:210  loss:  1.0510 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 16:45:48,274-INFO: epoch:6   train step:220  loss:  1.7982 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 16:45:55,204-INFO: epoch:6   train step:230  loss:  1.1162 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 16:46:01,243-INFO: END epoch:6   train loss_avg:  1.4398  elapse_sum: 179.497s\n",
      "2021-11-19 16:46:08,904-INFO: epoch:7   train step:0    loss:  0.9883 lr: 0.001000 elapse: 7.660s\n",
      "2021-11-19 16:46:17,707-INFO: epoch:7   train step:10   loss:  1.4946 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 16:46:24,944-INFO: epoch:7   train step:20   loss:  1.0641 lr: 0.001000 elapse: 0.731s\n",
      "2021-11-19 16:46:32,002-INFO: epoch:7   train step:30   loss:  0.9611 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:46:39,091-INFO: epoch:7   train step:40   loss:  0.9893 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:46:46,102-INFO: epoch:7   train step:50   loss:  1.2846 lr: 0.001000 elapse: 0.695s\n",
      "2021-11-19 16:46:53,303-INFO: epoch:7   train step:60   loss:  1.3473 lr: 0.001000 elapse: 0.751s\n",
      "2021-11-19 16:47:00,504-INFO: epoch:7   train step:70   loss:  1.3157 lr: 0.001000 elapse: 0.744s\n",
      "2021-11-19 16:47:07,802-INFO: epoch:7   train step:80   loss:  2.6363 lr: 0.001000 elapse: 0.777s\n",
      "2021-11-19 16:47:14,902-INFO: epoch:7   train step:90   loss:  0.9096 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:47:21,991-INFO: epoch:7   train step:100  loss:  1.1831 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:47:29,347-INFO: epoch:7   train step:110  loss:  1.4575 lr: 0.001000 elapse: 0.801s\n",
      "2021-11-19 16:47:36,491-INFO: epoch:7   train step:120  loss:  1.6034 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:47:43,503-INFO: epoch:7   train step:130  loss:  2.7308 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:47:50,624-INFO: epoch:7   train step:140  loss:  1.1871 lr: 0.001000 elapse: 0.721s\n",
      "2021-11-19 16:47:57,801-INFO: epoch:7   train step:150  loss:  0.9913 lr: 0.001000 elapse: 0.707s\n",
      "2021-11-19 16:48:04,903-INFO: epoch:7   train step:160  loss:  1.1838 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:48:12,005-INFO: epoch:7   train step:170  loss:  2.1895 lr: 0.001000 elapse: 0.702s\n",
      "2021-11-19 16:48:19,125-INFO: epoch:7   train step:180  loss:  2.2410 lr: 0.001000 elapse: 0.718s\n",
      "2021-11-19 16:48:26,291-INFO: epoch:7   train step:190  loss:  1.3171 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:48:33,457-INFO: epoch:7   train step:200  loss:  1.3365 lr: 0.001000 elapse: 0.696s\n",
      "2021-11-19 16:48:40,162-INFO: epoch:7   train step:210  loss:  0.9520 lr: 0.001000 elapse: 0.666s\n",
      "2021-11-19 16:48:47,108-INFO: epoch:7   train step:220  loss:  2.0923 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 16:48:54,048-INFO: epoch:7   train step:230  loss:  0.9191 lr: 0.001000 elapse: 0.802s\n",
      "2021-11-19 16:49:00,104-INFO: END epoch:7   train loss_avg:  1.3394  elapse_sum: 178.858s\n",
      "2021-11-19 16:49:07,993-INFO: epoch:8   train step:0    loss:  1.6928 lr: 0.001000 elapse: 7.887s\n",
      "2021-11-19 16:49:16,901-INFO: epoch:8   train step:10   loss:  1.8147 lr: 0.001000 elapse: 0.696s\n",
      "2021-11-19 16:49:24,001-INFO: epoch:8   train step:20   loss:  1.0562 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:49:31,124-INFO: epoch:8   train step:30   loss:  1.0321 lr: 0.001000 elapse: 0.721s\n",
      "2021-11-19 16:49:38,503-INFO: epoch:8   train step:40   loss:  1.5932 lr: 0.001000 elapse: 0.778s\n",
      "2021-11-19 16:49:45,701-INFO: epoch:8   train step:50   loss:  0.8364 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:49:52,861-INFO: epoch:8   train step:60   loss:  2.4164 lr: 0.001000 elapse: 0.741s\n",
      "2021-11-19 16:49:59,902-INFO: epoch:8   train step:70   loss:  0.9775 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:50:07,091-INFO: epoch:8   train step:80   loss:  0.8682 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:50:14,202-INFO: epoch:8   train step:90   loss:  1.2112 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:50:21,291-INFO: epoch:8   train step:100  loss:  1.1033 lr: 0.001000 elapse: 0.765s\n",
      "2021-11-19 16:50:28,391-INFO: epoch:8   train step:110  loss:  1.4153 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:50:35,491-INFO: epoch:8   train step:120  loss:  1.1577 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:50:42,902-INFO: epoch:8   train step:130  loss:  1.1020 lr: 0.001000 elapse: 0.776s\n",
      "2021-11-19 16:50:49,991-INFO: epoch:8   train step:140  loss:  2.6218 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:50:57,024-INFO: epoch:8   train step:150  loss:  1.0593 lr: 0.001000 elapse: 0.718s\n",
      "2021-11-19 16:51:04,092-INFO: epoch:8   train step:160  loss:  2.6117 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:51:11,103-INFO: epoch:8   train step:170  loss:  0.8553 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:51:18,403-INFO: epoch:8   train step:180  loss:  2.1306 lr: 0.001000 elapse: 0.711s\n",
      "2021-11-19 16:51:25,602-INFO: epoch:8   train step:190  loss:  2.3243 lr: 0.001000 elapse: 0.710s\n",
      "2021-11-19 16:51:32,543-INFO: epoch:8   train step:200  loss:  0.9240 lr: 0.001000 elapse: 0.676s\n",
      "2021-11-19 16:51:39,215-INFO: epoch:8   train step:210  loss:  2.3039 lr: 0.001000 elapse: 0.666s\n",
      "2021-11-19 16:51:46,172-INFO: epoch:8   train step:220  loss:  0.8871 lr: 0.001000 elapse: 0.669s\n",
      "2021-11-19 16:51:53,171-INFO: epoch:8   train step:230  loss:  0.9214 lr: 0.001000 elapse: 0.738s\n",
      "2021-11-19 16:51:59,297-INFO: END epoch:8   train loss_avg:  1.4132  elapse_sum: 179.190s\n",
      "2021-11-19 16:52:06,905-INFO: epoch:9   train step:0    loss:  1.0784 lr: 0.001000 elapse: 7.606s\n",
      "2021-11-19 16:52:15,901-INFO: epoch:9   train step:10   loss:  0.9304 lr: 0.001000 elapse: 0.707s\n",
      "2021-11-19 16:52:23,001-INFO: epoch:9   train step:20   loss:  1.2493 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:52:30,006-INFO: epoch:9   train step:30   loss:  0.8734 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 16:52:37,091-INFO: epoch:9   train step:40   loss:  0.8569 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:52:44,203-INFO: epoch:9   train step:50   loss:  0.8226 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 16:52:51,734-INFO: epoch:9   train step:60   loss:  0.9037 lr: 0.001000 elapse: 0.728s\n",
      "2021-11-19 16:52:58,991-INFO: epoch:9   train step:70   loss:  1.0089 lr: 0.001000 elapse: 0.772s\n",
      "2021-11-19 16:53:06,124-INFO: epoch:9   train step:80   loss:  0.9012 lr: 0.001000 elapse: 0.722s\n",
      "2021-11-19 16:53:13,291-INFO: epoch:9   train step:90   loss:  2.3431 lr: 0.001000 elapse: 0.762s\n",
      "2021-11-19 16:53:20,391-INFO: epoch:9   train step:100  loss:  2.5633 lr: 0.001000 elapse: 0.694s\n",
      "2021-11-19 16:53:27,502-INFO: epoch:9   train step:110  loss:  1.9196 lr: 0.001000 elapse: 0.695s\n",
      "2021-11-19 16:53:34,603-INFO: epoch:9   train step:120  loss:  2.5578 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:53:41,604-INFO: epoch:9   train step:130  loss:  2.5753 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 16:53:48,607-INFO: epoch:9   train step:140  loss:  0.9353 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 16:53:56,105-INFO: epoch:9   train step:150  loss:  1.0375 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 16:54:03,202-INFO: epoch:9   train step:160  loss:  0.9234 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:54:10,391-INFO: epoch:9   train step:170  loss:  0.9555 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:54:17,402-INFO: epoch:9   train step:180  loss:  2.1651 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:54:24,403-INFO: epoch:9   train step:190  loss:  0.8659 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:54:31,437-INFO: epoch:9   train step:200  loss:  1.0296 lr: 0.001000 elapse: 0.676s\n",
      "2021-11-19 16:54:38,151-INFO: epoch:9   train step:210  loss:  2.1207 lr: 0.001000 elapse: 0.666s\n",
      "2021-11-19 16:54:45,037-INFO: epoch:9   train step:220  loss:  2.5871 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 16:54:51,841-INFO: epoch:9   train step:230  loss:  1.6479 lr: 0.001000 elapse: 0.675s\n",
      "2021-11-19 16:54:58,173-INFO: END epoch:9   train loss_avg:  1.3934  elapse_sum: 178.873s\n",
      "2021-11-19 16:55:05,893-INFO: epoch:10  train step:0    loss:  2.0185 lr: 0.001000 elapse: 7.718s\n",
      "2021-11-19 16:55:14,822-INFO: epoch:10  train step:10   loss:  1.3285 lr: 0.001000 elapse: 0.720s\n",
      "2021-11-19 16:55:21,905-INFO: epoch:10  train step:20   loss:  2.7195 lr: 0.001000 elapse: 0.703s\n",
      "2021-11-19 16:55:28,991-INFO: epoch:10  train step:30   loss:  0.9466 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:55:36,004-INFO: epoch:10  train step:40   loss:  2.7191 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 16:55:43,100-INFO: epoch:10  train step:50   loss:  1.0635 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 16:55:50,303-INFO: epoch:10  train step:60   loss:  0.9392 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 16:55:57,346-INFO: epoch:10  train step:70   loss:  0.8930 lr: 0.001000 elapse: 0.742s\n",
      "2021-11-19 16:56:04,603-INFO: epoch:10  train step:80   loss:  1.1684 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:56:11,691-INFO: epoch:10  train step:90   loss:  0.9228 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:56:18,991-INFO: epoch:10  train step:100  loss:  1.0315 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:56:26,002-INFO: epoch:10  train step:110  loss:  1.1990 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:56:33,209-INFO: epoch:10  train step:120  loss:  1.3573 lr: 0.001000 elapse: 0.706s\n",
      "2021-11-19 16:56:40,203-INFO: epoch:10  train step:130  loss:  1.0745 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:56:47,236-INFO: epoch:10  train step:140  loss:  0.8492 lr: 0.001000 elapse: 0.728s\n",
      "2021-11-19 16:56:54,493-INFO: epoch:10  train step:150  loss:  2.3719 lr: 0.001000 elapse: 0.770s\n",
      "2021-11-19 16:57:01,520-INFO: epoch:10  train step:160  loss:  1.0233 lr: 0.001000 elapse: 0.716s\n",
      "2021-11-19 16:57:08,824-INFO: epoch:10  train step:170  loss:  1.3891 lr: 0.001000 elapse: 0.717s\n",
      "2021-11-19 16:57:15,903-INFO: epoch:10  train step:180  loss:  1.9708 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 16:57:22,902-INFO: epoch:10  train step:190  loss:  2.2014 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:57:29,840-INFO: epoch:10  train step:200  loss:  2.1771 lr: 0.001000 elapse: 0.679s\n",
      "2021-11-19 16:57:36,532-INFO: epoch:10  train step:210  loss:  1.1031 lr: 0.001000 elapse: 0.675s\n",
      "2021-11-19 16:57:43,376-INFO: epoch:10  train step:220  loss:  0.9609 lr: 0.001000 elapse: 0.679s\n",
      "2021-11-19 16:57:50,304-INFO: epoch:10  train step:230  loss:  0.8548 lr: 0.001000 elapse: 0.670s\n",
      "2021-11-19 16:57:56,416-INFO: END epoch:10  train loss_avg:  1.4422  elapse_sum: 178.241s\n",
      "2021-11-19 16:58:00,893-INFO: epoch:10  valid step:0    loss:  1.1868 top1: 0.8750 top5: 1.0000 elapse: 4.475s\n",
      "2021-11-19 16:58:06,659-INFO: epoch:10  valid step:10   loss:  1.2461 top1: 0.9375 top5: 1.0000 elapse: 0.311s\n",
      "2021-11-19 16:58:09,263-INFO: epoch:10  valid step:20   loss:  1.2812 top1: 0.7500 top5: 1.0000 elapse: 0.229s\n",
      "2021-11-19 16:58:11,428-INFO: epoch:10  valid step:30   loss:  1.2081 top1: 0.8125 top5: 1.0000 elapse: 0.189s\n",
      "2021-11-19 16:58:13,314-INFO: epoch:10  valid step:40   loss:  1.1552 top1: 0.8750 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 16:58:15,192-INFO: epoch:10  valid step:50   loss:  1.0992 top1: 0.8750 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 16:58:17,075-INFO: epoch:10  valid step:60   loss:  1.7455 top1: 0.7500 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 16:58:19,041-INFO: epoch:10  valid step:70   loss:  1.5119 top1: 0.7500 top5: 0.9375 elapse: 0.280s\n",
      "2021-11-19 16:58:20,931-INFO: epoch:10  valid step:80   loss:  1.7621 top1: 0.6875 top5: 0.8125 elapse: 0.187s\n",
      "2021-11-19 16:58:22,873-INFO: END epoch:10  valid loss_avg:  1.2369 top1_avg: 0.8486 top5_avg: 0.9639 elapse_sum: 26.455s\n",
      "2021-11-19 16:58:22,873-INFO: The best top1 acc 0.84861, in epoch: 10\n",
      "2021-11-19 16:58:24,001-INFO: Already save model in ./output/EfficientNetB3/best_model\n",
      "2021-11-19 16:58:25,116-INFO: Already save model in ./output/EfficientNetB3/10\n",
      "2021-11-19 16:58:33,192-INFO: epoch:11  train step:0    loss:  0.8390 lr: 0.001000 elapse: 8.075s\n",
      "2021-11-19 16:58:41,802-INFO: epoch:11  train step:10   loss:  1.1221 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 16:58:48,891-INFO: epoch:11  train step:20   loss:  0.9846 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:58:55,991-INFO: epoch:11  train step:30   loss:  1.1602 lr: 0.001000 elapse: 0.771s\n",
      "2021-11-19 16:59:03,001-INFO: epoch:11  train step:40   loss:  0.9904 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 16:59:10,107-INFO: epoch:11  train step:50   loss:  0.8484 lr: 0.001000 elapse: 0.715s\n",
      "2021-11-19 16:59:17,417-INFO: epoch:11  train step:60   loss:  0.8986 lr: 0.001000 elapse: 0.714s\n",
      "2021-11-19 16:59:24,893-INFO: epoch:11  train step:70   loss:  1.9935 lr: 0.001000 elapse: 1.089s\n",
      "2021-11-19 16:59:32,018-INFO: epoch:11  train step:80   loss:  2.0020 lr: 0.001000 elapse: 0.714s\n",
      "2021-11-19 16:59:39,191-INFO: epoch:11  train step:90   loss:  1.0881 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 16:59:46,201-INFO: epoch:11  train step:100  loss:  1.0007 lr: 0.001000 elapse: 0.709s\n",
      "2021-11-19 16:59:53,325-INFO: epoch:11  train step:110  loss:  1.1414 lr: 0.001000 elapse: 0.721s\n",
      "2021-11-19 17:00:00,491-INFO: epoch:11  train step:120  loss:  1.9389 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 17:00:07,601-INFO: epoch:11  train step:130  loss:  1.1706 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 17:00:14,717-INFO: epoch:11  train step:140  loss:  2.5242 lr: 0.001000 elapse: 0.793s\n",
      "2021-11-19 17:00:22,017-INFO: epoch:11  train step:150  loss:  1.0338 lr: 0.001000 elapse: 0.711s\n",
      "2021-11-19 17:00:29,023-INFO: epoch:11  train step:160  loss:  1.5047 lr: 0.001000 elapse: 0.716s\n",
      "2021-11-19 17:00:36,207-INFO: epoch:11  train step:170  loss:  1.0280 lr: 0.001000 elapse: 0.704s\n",
      "2021-11-19 17:00:43,346-INFO: epoch:11  train step:180  loss:  1.0274 lr: 0.001000 elapse: 0.743s\n",
      "2021-11-19 17:00:50,464-INFO: epoch:11  train step:190  loss:  1.3563 lr: 0.001000 elapse: 0.670s\n",
      "2021-11-19 17:00:57,535-INFO: epoch:11  train step:200  loss:  1.2719 lr: 0.001000 elapse: 0.677s\n",
      "2021-11-19 17:01:04,202-INFO: epoch:11  train step:210  loss:  1.2691 lr: 0.001000 elapse: 0.666s\n",
      "2021-11-19 17:01:11,151-INFO: epoch:11  train step:220  loss:  1.5999 lr: 0.001000 elapse: 0.668s\n",
      "2021-11-19 17:01:18,077-INFO: epoch:11  train step:230  loss:  0.9325 lr: 0.001000 elapse: 0.668s\n",
      "2021-11-19 17:01:24,093-INFO: END epoch:11  train loss_avg:  1.3526  elapse_sum: 178.974s\n",
      "2021-11-19 17:01:31,914-INFO: epoch:12  train step:0    loss:  1.1063 lr: 0.001000 elapse: 7.819s\n",
      "2021-11-19 17:01:40,703-INFO: epoch:12  train step:10   loss:  1.4972 lr: 0.001000 elapse: 0.700s\n",
      "2021-11-19 17:01:47,702-INFO: epoch:12  train step:20   loss:  0.9881 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 17:01:54,891-INFO: epoch:12  train step:30   loss:  1.2044 lr: 0.001000 elapse: 0.765s\n",
      "2021-11-19 17:02:01,991-INFO: epoch:12  train step:40   loss:  1.0043 lr: 0.001000 elapse: 0.765s\n",
      "2021-11-19 17:02:09,133-INFO: epoch:12  train step:50   loss:  1.0666 lr: 0.001000 elapse: 0.728s\n",
      "2021-11-19 17:02:16,303-INFO: epoch:12  train step:60   loss:  0.9022 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 17:02:23,612-INFO: epoch:12  train step:70   loss:  2.2467 lr: 0.001000 elapse: 0.750s\n",
      "2021-11-19 17:02:30,901-INFO: epoch:12  train step:80   loss:  1.0093 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 17:02:37,917-INFO: epoch:12  train step:90   loss:  2.1829 lr: 0.001000 elapse: 0.698s\n",
      "2021-11-19 17:02:45,001-INFO: epoch:12  train step:100  loss:  1.0282 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 17:02:52,225-INFO: epoch:12  train step:110  loss:  0.9331 lr: 0.001000 elapse: 0.721s\n",
      "2021-11-19 17:02:59,304-INFO: epoch:12  train step:120  loss:  0.9425 lr: 0.001000 elapse: 0.712s\n",
      "2021-11-19 17:03:06,303-INFO: epoch:12  train step:130  loss:  0.9057 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 17:03:13,502-INFO: epoch:12  train step:140  loss:  2.0512 lr: 0.001000 elapse: 0.710s\n",
      "2021-11-19 17:03:20,591-INFO: epoch:12  train step:150  loss:  2.1766 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 17:03:27,903-INFO: epoch:12  train step:160  loss:  1.0003 lr: 0.001000 elapse: 0.758s\n",
      "2021-11-19 17:03:35,147-INFO: epoch:12  train step:170  loss:  0.8426 lr: 0.001000 elapse: 0.740s\n",
      "2021-11-19 17:03:42,291-INFO: epoch:12  train step:180  loss:  2.2951 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 17:03:49,291-INFO: epoch:12  train step:190  loss:  0.8985 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 17:03:56,346-INFO: epoch:12  train step:200  loss:  1.8293 lr: 0.001000 elapse: 0.678s\n",
      "2021-11-19 17:04:03,033-INFO: epoch:12  train step:210  loss:  0.9531 lr: 0.001000 elapse: 0.666s\n",
      "2021-11-19 17:04:09,931-INFO: epoch:12  train step:220  loss:  0.9408 lr: 0.001000 elapse: 0.667s\n",
      "2021-11-19 17:04:16,922-INFO: epoch:12  train step:230  loss:  1.1273 lr: 0.001000 elapse: 0.671s\n",
      "2021-11-19 17:04:22,946-INFO: END epoch:12  train loss_avg:  1.3458  elapse_sum: 178.850s\n",
      "2021-11-19 17:04:30,592-INFO: epoch:13  train step:0    loss:  2.4052 lr: 0.001000 elapse: 7.645s\n",
      "2021-11-19 17:04:39,991-INFO: epoch:13  train step:10   loss:  0.9077 lr: 0.001000 elapse: 0.699s\n",
      "2021-11-19 17:04:46,993-INFO: epoch:13  train step:20   loss:  0.9451 lr: 0.001000 elapse: 0.701s\n",
      "2021-11-19 17:04:54,002-INFO: epoch:13  train step:30   loss:  1.2787 lr: 0.001000 elapse: 0.695s\n",
      "2021-11-19 17:05:01,104-INFO: epoch:13  train step:40   loss:  0.9947 lr: 0.001000 elapse: 0.712s\n",
      "2021-11-19 17:05:08,202-INFO: epoch:13  train step:50   loss:  1.1681 lr: 0.001000 elapse: 0.697s\n",
      "2021-11-19 17:05:15,215-INFO: epoch:13  train step:60   loss:  2.5633 lr: 0.001000 elapse: 0.713s\n",
      "2021-11-19 17:05:22,305-INFO: epoch:13  train step:70   loss:  2.5661 lr: 0.001000 elapse: 0.702s\n",
      "2021-11-19 17:05:29,301-INFO: epoch:13  train step:80   loss:  2.4920 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:05:36,503-INFO: epoch:13  train step:90   loss:  2.1424 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:05:43,802-INFO: epoch:13  train step:100  loss:  0.9274 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:05:50,824-INFO: epoch:13  train step:110  loss:  0.9644 lr: 0.000999 elapse: 0.714s\n",
      "2021-11-19 17:05:57,926-INFO: epoch:13  train step:120  loss:  0.9742 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:06:05,103-INFO: epoch:13  train step:130  loss:  1.9984 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:06:12,117-INFO: epoch:13  train step:140  loss:  2.3763 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:06:19,301-INFO: epoch:13  train step:150  loss:  0.9884 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:06:26,325-INFO: epoch:13  train step:160  loss:  1.0719 lr: 0.000999 elapse: 0.720s\n",
      "2021-11-19 17:06:33,491-INFO: epoch:13  train step:170  loss:  1.6384 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:06:40,591-INFO: epoch:13  train step:180  loss:  0.8342 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:06:47,891-INFO: epoch:13  train step:190  loss:  2.2084 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:06:54,936-INFO: epoch:13  train step:200  loss:  1.9684 lr: 0.000999 elapse: 0.678s\n",
      "2021-11-19 17:07:01,638-INFO: epoch:13  train step:210  loss:  0.9073 lr: 0.000999 elapse: 0.668s\n",
      "2021-11-19 17:07:08,531-INFO: epoch:13  train step:220  loss:  0.9385 lr: 0.000999 elapse: 0.666s\n",
      "2021-11-19 17:07:15,442-INFO: epoch:13  train step:230  loss:  0.9227 lr: 0.000999 elapse: 0.667s\n",
      "2021-11-19 17:07:21,444-INFO: END epoch:13  train loss_avg:  1.3842  elapse_sum: 178.496s\n",
      "2021-11-19 17:07:29,293-INFO: epoch:14  train step:0    loss:  0.9077 lr: 0.000999 elapse: 7.847s\n",
      "2021-11-19 17:07:38,101-INFO: epoch:14  train step:10   loss:  1.9359 lr: 0.000999 elapse: 0.696s\n",
      "2021-11-19 17:07:45,102-INFO: epoch:14  train step:20   loss:  0.8217 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:07:52,502-INFO: epoch:14  train step:30   loss:  2.0233 lr: 0.000999 elapse: 0.710s\n",
      "2021-11-19 17:07:59,602-INFO: epoch:14  train step:40   loss:  0.9535 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:08:06,603-INFO: epoch:14  train step:50   loss:  0.9018 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:08:13,805-INFO: epoch:14  train step:60   loss:  1.0563 lr: 0.000999 elapse: 0.712s\n",
      "2021-11-19 17:08:21,001-INFO: epoch:14  train step:70   loss:  2.2611 lr: 0.000999 elapse: 0.772s\n",
      "2021-11-19 17:08:28,102-INFO: epoch:14  train step:80   loss:  1.0193 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:08:35,293-INFO: epoch:14  train step:90   loss:  0.8558 lr: 0.000999 elapse: 0.701s\n",
      "2021-11-19 17:08:42,303-INFO: epoch:14  train step:100  loss:  1.5529 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:08:49,327-INFO: epoch:14  train step:110  loss:  2.2343 lr: 0.000999 elapse: 0.723s\n",
      "2021-11-19 17:08:56,691-INFO: epoch:14  train step:120  loss:  0.8729 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:09:03,702-INFO: epoch:14  train step:130  loss:  1.9677 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:09:10,703-INFO: epoch:14  train step:140  loss:  1.1578 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:09:17,725-INFO: epoch:14  train step:150  loss:  1.4359 lr: 0.000999 elapse: 0.708s\n",
      "2021-11-19 17:09:24,907-INFO: epoch:14  train step:160  loss:  0.9382 lr: 0.000999 elapse: 0.704s\n",
      "2021-11-19 17:09:31,991-INFO: epoch:14  train step:170  loss:  1.3943 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:09:39,093-INFO: epoch:14  train step:180  loss:  1.0277 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:09:46,191-INFO: epoch:14  train step:190  loss:  2.4777 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:09:53,210-INFO: epoch:14  train step:200  loss:  2.5307 lr: 0.000999 elapse: 0.677s\n",
      "2021-11-19 17:09:59,912-INFO: epoch:14  train step:210  loss:  0.9127 lr: 0.000999 elapse: 0.672s\n",
      "2021-11-19 17:10:06,834-INFO: epoch:14  train step:220  loss:  2.3996 lr: 0.000999 elapse: 0.675s\n",
      "2021-11-19 17:10:13,770-INFO: epoch:14  train step:230  loss:  0.8414 lr: 0.000999 elapse: 0.677s\n",
      "2021-11-19 17:10:19,782-INFO: END epoch:14  train loss_avg:  1.4239  elapse_sum: 178.335s\n",
      "2021-11-19 17:10:27,292-INFO: epoch:15  train step:0    loss:  0.9616 lr: 0.000999 elapse: 7.508s\n",
      "2021-11-19 17:10:36,392-INFO: epoch:15  train step:10   loss:  1.8739 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:10:43,416-INFO: epoch:15  train step:20   loss:  1.1393 lr: 0.000999 elapse: 0.713s\n",
      "2021-11-19 17:10:50,483-INFO: epoch:15  train step:30   loss:  1.0151 lr: 0.000999 elapse: 0.690s\n",
      "2021-11-19 17:10:57,703-INFO: epoch:15  train step:40   loss:  0.9112 lr: 0.000999 elapse: 0.778s\n",
      "2021-11-19 17:11:05,091-INFO: epoch:15  train step:50   loss:  1.2179 lr: 0.000999 elapse: 0.767s\n",
      "2021-11-19 17:11:12,202-INFO: epoch:15  train step:60   loss:  1.0368 lr: 0.000999 elapse: 0.695s\n",
      "2021-11-19 17:11:19,317-INFO: epoch:15  train step:70   loss:  2.0461 lr: 0.000999 elapse: 0.712s\n",
      "2021-11-19 17:11:26,491-INFO: epoch:15  train step:80   loss:  1.2603 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:11:33,501-INFO: epoch:15  train step:90   loss:  1.0374 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:11:40,691-INFO: epoch:15  train step:100  loss:  1.0334 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:11:47,791-INFO: epoch:15  train step:110  loss:  2.6846 lr: 0.000999 elapse: 0.770s\n",
      "2021-11-19 17:11:54,802-INFO: epoch:15  train step:120  loss:  2.0021 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:12:02,004-INFO: epoch:15  train step:130  loss:  0.9236 lr: 0.000999 elapse: 0.786s\n",
      "2021-11-19 17:12:09,403-INFO: epoch:15  train step:140  loss:  2.2296 lr: 0.000999 elapse: 0.679s\n",
      "2021-11-19 17:12:16,502-INFO: epoch:15  train step:150  loss:  1.6650 lr: 0.000999 elapse: 0.778s\n",
      "2021-11-19 17:12:23,602-INFO: epoch:15  train step:160  loss:  1.0281 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:12:30,702-INFO: epoch:15  train step:170  loss:  2.5763 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:12:37,804-INFO: epoch:15  train step:180  loss:  1.4515 lr: 0.000999 elapse: 0.701s\n",
      "2021-11-19 17:12:44,891-INFO: epoch:15  train step:190  loss:  1.5276 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:12:51,849-INFO: epoch:15  train step:200  loss:  1.3120 lr: 0.000999 elapse: 0.677s\n",
      "2021-11-19 17:12:58,541-INFO: epoch:15  train step:210  loss:  1.2287 lr: 0.000999 elapse: 0.673s\n",
      "2021-11-19 17:13:05,495-INFO: epoch:15  train step:220  loss:  0.9844 lr: 0.000999 elapse: 0.819s\n",
      "2021-11-19 17:13:12,361-INFO: epoch:15  train step:230  loss:  1.0908 lr: 0.000999 elapse: 0.666s\n",
      "2021-11-19 17:13:18,683-INFO: END epoch:15  train loss_avg:  1.3833  elapse_sum: 178.897s\n",
      "2021-11-19 17:13:26,692-INFO: epoch:16  train step:0    loss:  1.1267 lr: 0.000999 elapse: 8.008s\n",
      "2021-11-19 17:13:35,503-INFO: epoch:16  train step:10   loss:  0.8724 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:13:42,596-INFO: epoch:16  train step:20   loss:  0.9642 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:13:49,603-INFO: epoch:16  train step:30   loss:  0.9039 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:13:56,792-INFO: epoch:16  train step:40   loss:  1.5164 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:14:03,812-INFO: epoch:16  train step:50   loss:  1.3372 lr: 0.000999 elapse: 0.706s\n",
      "2021-11-19 17:14:10,945-INFO: epoch:16  train step:60   loss:  1.8764 lr: 0.000999 elapse: 0.793s\n",
      "2021-11-19 17:14:18,291-INFO: epoch:16  train step:70   loss:  1.8512 lr: 0.000999 elapse: 0.764s\n",
      "2021-11-19 17:14:25,303-INFO: epoch:16  train step:80   loss:  1.1377 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:14:32,392-INFO: epoch:16  train step:90   loss:  1.2930 lr: 0.000999 elapse: 0.767s\n",
      "2021-11-19 17:14:39,501-INFO: epoch:16  train step:100  loss:  1.5401 lr: 0.000999 elapse: 0.774s\n",
      "2021-11-19 17:14:46,602-INFO: epoch:16  train step:110  loss:  0.8697 lr: 0.000999 elapse: 0.710s\n",
      "2021-11-19 17:14:53,923-INFO: epoch:16  train step:120  loss:  1.2400 lr: 0.000999 elapse: 0.715s\n",
      "2021-11-19 17:15:01,203-INFO: epoch:16  train step:130  loss:  0.8323 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:15:08,293-INFO: epoch:16  train step:140  loss:  0.9339 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:15:15,503-INFO: epoch:16  train step:150  loss:  0.8852 lr: 0.000999 elapse: 0.713s\n",
      "2021-11-19 17:15:22,791-INFO: epoch:16  train step:160  loss:  0.9164 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:15:29,891-INFO: epoch:16  train step:170  loss:  1.8011 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:15:36,902-INFO: epoch:16  train step:180  loss:  1.4233 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:15:44,080-INFO: epoch:16  train step:190  loss:  1.1687 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:15:51,151-INFO: epoch:16  train step:200  loss:  1.0101 lr: 0.000999 elapse: 0.676s\n",
      "2021-11-19 17:15:57,881-INFO: epoch:16  train step:210  loss:  1.3835 lr: 0.000999 elapse: 0.666s\n",
      "2021-11-19 17:16:04,825-INFO: epoch:16  train step:220  loss:  1.0522 lr: 0.000999 elapse: 0.667s\n",
      "2021-11-19 17:16:11,748-INFO: epoch:16  train step:230  loss:  1.2701 lr: 0.000999 elapse: 0.677s\n",
      "2021-11-19 17:16:17,788-INFO: END epoch:16  train loss_avg:  1.2920  elapse_sum: 179.102s\n",
      "2021-11-19 17:16:25,794-INFO: epoch:17  train step:0    loss:  1.0305 lr: 0.000999 elapse: 8.004s\n",
      "2021-11-19 17:16:35,091-INFO: epoch:17  train step:10   loss:  0.9318 lr: 0.000999 elapse: 0.773s\n",
      "2021-11-19 17:16:42,193-INFO: epoch:17  train step:20   loss:  0.9549 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:16:49,291-INFO: epoch:17  train step:30   loss:  2.4071 lr: 0.000999 elapse: 0.767s\n",
      "2021-11-19 17:16:56,302-INFO: epoch:17  train step:40   loss:  1.5039 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:17:03,401-INFO: epoch:17  train step:50   loss:  1.1533 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:17:10,603-INFO: epoch:17  train step:60   loss:  1.0722 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:17:17,904-INFO: epoch:17  train step:70   loss:  0.8642 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:17:25,106-INFO: epoch:17  train step:80   loss:  1.5012 lr: 0.000999 elapse: 0.693s\n",
      "2021-11-19 17:17:32,602-INFO: epoch:17  train step:90   loss:  0.9938 lr: 0.000999 elapse: 0.777s\n",
      "2021-11-19 17:17:39,703-INFO: epoch:17  train step:100  loss:  0.9620 lr: 0.000999 elapse: 0.694s\n",
      "2021-11-19 17:17:46,823-INFO: epoch:17  train step:110  loss:  2.0364 lr: 0.000999 elapse: 0.715s\n",
      "2021-11-19 17:17:53,925-INFO: epoch:17  train step:120  loss:  0.9291 lr: 0.000999 elapse: 0.722s\n",
      "2021-11-19 17:18:01,002-INFO: epoch:17  train step:130  loss:  0.9017 lr: 0.000999 elapse: 0.697s\n",
      "2021-11-19 17:18:08,016-INFO: epoch:17  train step:140  loss:  0.9409 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:18:15,003-INFO: epoch:17  train step:150  loss:  0.9376 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:18:22,102-INFO: epoch:17  train step:160  loss:  0.8992 lr: 0.000999 elapse: 0.709s\n",
      "2021-11-19 17:18:29,106-INFO: epoch:17  train step:170  loss:  1.0126 lr: 0.000999 elapse: 0.701s\n",
      "2021-11-19 17:18:36,509-INFO: epoch:17  train step:180  loss:  0.9164 lr: 0.000999 elapse: 0.705s\n",
      "2021-11-19 17:18:43,601-INFO: epoch:17  train step:190  loss:  1.8677 lr: 0.000999 elapse: 0.696s\n",
      "2021-11-19 17:18:50,547-INFO: epoch:17  train step:200  loss:  0.9219 lr: 0.000999 elapse: 0.681s\n",
      "2021-11-19 17:18:57,221-INFO: epoch:17  train step:210  loss:  2.2749 lr: 0.000999 elapse: 0.669s\n",
      "2021-11-19 17:19:04,215-INFO: epoch:17  train step:220  loss:  0.9831 lr: 0.000999 elapse: 0.682s\n",
      "2021-11-19 17:19:11,175-INFO: epoch:17  train step:230  loss:  1.4765 lr: 0.000999 elapse: 0.799s\n",
      "2021-11-19 17:19:17,303-INFO: END epoch:17  train loss_avg:  1.3761  elapse_sum: 179.512s\n",
      "2021-11-19 17:19:25,293-INFO: epoch:18  train step:0    loss:  1.9873 lr: 0.000999 elapse: 7.988s\n",
      "2021-11-19 17:19:34,125-INFO: epoch:18  train step:10   loss:  1.6168 lr: 0.000999 elapse: 0.722s\n",
      "2021-11-19 17:19:41,403-INFO: epoch:18  train step:20   loss:  1.7865 lr: 0.000999 elapse: 0.699s\n",
      "2021-11-19 17:19:48,423-INFO: epoch:18  train step:30   loss:  0.8336 lr: 0.000999 elapse: 0.704s\n",
      "2021-11-19 17:19:55,791-INFO: epoch:18  train step:40   loss:  1.2013 lr: 0.000999 elapse: 0.772s\n",
      "2021-11-19 17:20:02,891-INFO: epoch:18  train step:50   loss:  1.2563 lr: 0.000999 elapse: 0.767s\n",
      "2021-11-19 17:20:09,992-INFO: epoch:18  train step:60   loss:  0.8906 lr: 0.000999 elapse: 0.700s\n",
      "2021-11-19 17:20:17,003-INFO: epoch:18  train step:70   loss:  1.0895 lr: 0.000999 elapse: 0.698s\n",
      "2021-11-19 17:20:24,113-INFO: epoch:18  train step:80   loss:  1.7059 lr: 0.000999 elapse: 0.710s\n",
      "2021-11-19 17:20:31,224-INFO: epoch:18  train step:90   loss:  2.4283 lr: 0.000999 elapse: 0.720s\n",
      "2021-11-19 17:20:38,403-INFO: epoch:18  train step:100  loss:  2.4556 lr: 0.000999 elapse: 0.711s\n",
      "2021-11-19 17:20:45,705-INFO: epoch:18  train step:110  loss:  1.8233 lr: 0.000999 elapse: 0.701s\n",
      "2021-11-19 17:20:52,825-INFO: epoch:18  train step:120  loss:  0.9672 lr: 0.000998 elapse: 0.722s\n",
      "2021-11-19 17:20:59,993-INFO: epoch:18  train step:130  loss:  1.0365 lr: 0.000998 elapse: 0.770s\n",
      "2021-11-19 17:21:07,091-INFO: epoch:18  train step:140  loss:  0.8895 lr: 0.000998 elapse: 0.766s\n",
      "2021-11-19 17:21:14,091-INFO: epoch:18  train step:150  loss:  1.9668 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:21:21,191-INFO: epoch:18  train step:160  loss:  0.9096 lr: 0.000998 elapse: 0.773s\n",
      "2021-11-19 17:21:28,392-INFO: epoch:18  train step:170  loss:  1.1106 lr: 0.000998 elapse: 0.768s\n",
      "2021-11-19 17:21:35,501-INFO: epoch:18  train step:180  loss:  2.5671 lr: 0.000998 elapse: 0.695s\n",
      "2021-11-19 17:21:42,503-INFO: epoch:18  train step:190  loss:  0.9622 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:21:49,819-INFO: epoch:18  train step:200  loss:  0.9211 lr: 0.000998 elapse: 0.678s\n",
      "2021-11-19 17:21:56,556-INFO: epoch:18  train step:210  loss:  2.4887 lr: 0.000998 elapse: 0.692s\n",
      "2021-11-19 17:22:03,527-INFO: epoch:18  train step:220  loss:  2.2272 lr: 0.000998 elapse: 0.677s\n",
      "2021-11-19 17:22:10,495-INFO: epoch:18  train step:230  loss:  0.8746 lr: 0.000998 elapse: 0.676s\n",
      "2021-11-19 17:22:16,585-INFO: END epoch:18  train loss_avg:  1.3660  elapse_sum: 179.279s\n",
      "2021-11-19 17:22:24,092-INFO: epoch:19  train step:0    loss:  2.3710 lr: 0.000998 elapse: 7.506s\n",
      "2021-11-19 17:22:33,402-INFO: epoch:19  train step:10   loss:  0.9172 lr: 0.000998 elapse: 0.697s\n",
      "2021-11-19 17:22:40,604-INFO: epoch:19  train step:20   loss:  1.1280 lr: 0.000998 elapse: 0.709s\n",
      "2021-11-19 17:22:47,648-INFO: epoch:19  train step:30   loss:  1.2872 lr: 0.000998 elapse: 0.743s\n",
      "2021-11-19 17:22:55,004-INFO: epoch:19  train step:40   loss:  0.9113 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:23:02,102-INFO: epoch:19  train step:50   loss:  1.6409 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:23:09,102-INFO: epoch:19  train step:60   loss:  1.0708 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:23:16,205-INFO: epoch:19  train step:70   loss:  1.4272 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:23:23,302-INFO: epoch:19  train step:80   loss:  1.0142 lr: 0.000998 elapse: 0.696s\n",
      "2021-11-19 17:23:30,409-INFO: epoch:19  train step:90   loss:  1.0520 lr: 0.000998 elapse: 0.704s\n",
      "2021-11-19 17:23:37,604-INFO: epoch:19  train step:100  loss:  2.4987 lr: 0.000998 elapse: 0.701s\n",
      "2021-11-19 17:23:44,704-INFO: epoch:19  train step:110  loss:  1.7383 lr: 0.000998 elapse: 0.712s\n",
      "2021-11-19 17:23:51,891-INFO: epoch:19  train step:120  loss:  0.9999 lr: 0.000998 elapse: 0.798s\n",
      "2021-11-19 17:23:59,214-INFO: epoch:19  train step:130  loss:  1.4456 lr: 0.000998 elapse: 0.710s\n",
      "2021-11-19 17:24:06,402-INFO: epoch:19  train step:140  loss:  0.9171 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:24:13,506-INFO: epoch:19  train step:150  loss:  1.6798 lr: 0.000998 elapse: 0.703s\n",
      "2021-11-19 17:24:20,622-INFO: epoch:19  train step:160  loss:  0.9515 lr: 0.000998 elapse: 0.719s\n",
      "2021-11-19 17:24:27,802-INFO: epoch:19  train step:170  loss:  2.3827 lr: 0.000998 elapse: 0.696s\n",
      "2021-11-19 17:24:34,802-INFO: epoch:19  train step:180  loss:  2.3445 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:24:41,993-INFO: epoch:19  train step:190  loss:  1.4827 lr: 0.000998 elapse: 0.772s\n",
      "2021-11-19 17:24:48,938-INFO: epoch:19  train step:200  loss:  0.9023 lr: 0.000998 elapse: 0.678s\n",
      "2021-11-19 17:24:55,628-INFO: epoch:19  train step:210  loss:  0.8915 lr: 0.000998 elapse: 0.668s\n",
      "2021-11-19 17:25:02,498-INFO: epoch:19  train step:220  loss:  2.0837 lr: 0.000998 elapse: 0.681s\n",
      "2021-11-19 17:25:09,317-INFO: epoch:19  train step:230  loss:  0.8806 lr: 0.000998 elapse: 0.667s\n",
      "2021-11-19 17:25:15,498-INFO: END epoch:19  train loss_avg:  1.3871  elapse_sum: 178.910s\n",
      "2021-11-19 17:25:23,292-INFO: epoch:20  train step:0    loss:  2.0324 lr: 0.000998 elapse: 7.793s\n",
      "2021-11-19 17:25:32,091-INFO: epoch:20  train step:10   loss:  0.9250 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:25:39,302-INFO: epoch:20  train step:20   loss:  1.9456 lr: 0.000998 elapse: 0.757s\n",
      "2021-11-19 17:25:46,401-INFO: epoch:20  train step:30   loss:  0.9008 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:25:53,691-INFO: epoch:20  train step:40   loss:  0.8604 lr: 0.000998 elapse: 0.771s\n",
      "2021-11-19 17:26:01,035-INFO: epoch:20  train step:50   loss:  2.1210 lr: 0.000998 elapse: 0.781s\n",
      "2021-11-19 17:26:08,212-INFO: epoch:20  train step:60   loss:  2.1164 lr: 0.000998 elapse: 0.709s\n",
      "2021-11-19 17:26:15,392-INFO: epoch:20  train step:70   loss:  2.1221 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:26:22,401-INFO: epoch:20  train step:80   loss:  1.0018 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:26:29,591-INFO: epoch:20  train step:90   loss:  0.9466 lr: 0.000998 elapse: 0.684s\n",
      "2021-11-19 17:26:36,694-INFO: epoch:20  train step:100  loss:  1.0827 lr: 0.000998 elapse: 0.767s\n",
      "2021-11-19 17:26:43,703-INFO: epoch:20  train step:110  loss:  0.9477 lr: 0.000998 elapse: 0.684s\n",
      "2021-11-19 17:26:50,920-INFO: epoch:20  train step:120  loss:  1.2689 lr: 0.000998 elapse: 0.774s\n",
      "2021-11-19 17:26:58,024-INFO: epoch:20  train step:130  loss:  1.0616 lr: 0.000998 elapse: 0.718s\n",
      "2021-11-19 17:27:05,438-INFO: epoch:20  train step:140  loss:  0.8701 lr: 0.000998 elapse: 0.745s\n",
      "2021-11-19 17:27:12,703-INFO: epoch:20  train step:150  loss:  1.0067 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:27:19,801-INFO: epoch:20  train step:160  loss:  2.1905 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:27:26,817-INFO: epoch:20  train step:170  loss:  0.8366 lr: 0.000998 elapse: 0.714s\n",
      "2021-11-19 17:27:33,891-INFO: epoch:20  train step:180  loss:  0.8807 lr: 0.000998 elapse: 0.766s\n",
      "2021-11-19 17:27:41,003-INFO: epoch:20  train step:190  loss:  2.5059 lr: 0.000998 elapse: 0.711s\n",
      "2021-11-19 17:27:48,012-INFO: epoch:20  train step:200  loss:  1.6263 lr: 0.000998 elapse: 0.676s\n",
      "2021-11-19 17:27:54,696-INFO: epoch:20  train step:210  loss:  0.9254 lr: 0.000998 elapse: 0.669s\n",
      "2021-11-19 17:28:01,704-INFO: epoch:20  train step:220  loss:  0.8827 lr: 0.000998 elapse: 0.831s\n",
      "2021-11-19 17:28:08,656-INFO: epoch:20  train step:230  loss:  1.0105 lr: 0.000998 elapse: 0.808s\n",
      "2021-11-19 17:28:14,909-INFO: END epoch:20  train loss_avg:  1.3725  elapse_sum: 179.408s\n",
      "2021-11-19 17:28:19,599-INFO: epoch:20  valid step:0    loss:  1.2772 top1: 0.8125 top5: 1.0000 elapse: 4.688s\n",
      "2021-11-19 17:28:24,883-INFO: epoch:20  valid step:10   loss:  1.6391 top1: 0.6875 top5: 0.8125 elapse: 0.243s\n",
      "2021-11-19 17:28:27,222-INFO: epoch:20  valid step:20   loss:  1.3758 top1: 0.8125 top5: 0.9375 elapse: 0.235s\n",
      "2021-11-19 17:28:29,346-INFO: epoch:20  valid step:30   loss:  1.5548 top1: 0.8125 top5: 0.8750 elapse: 0.188s\n",
      "2021-11-19 17:28:31,231-INFO: epoch:20  valid step:40   loss:  1.1083 top1: 0.9375 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 17:28:33,114-INFO: epoch:20  valid step:50   loss:  1.4952 top1: 0.7500 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 17:28:34,998-INFO: epoch:20  valid step:60   loss:  1.3295 top1: 0.8750 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 17:28:36,979-INFO: epoch:20  valid step:70   loss:  1.1148 top1: 0.8750 top5: 1.0000 elapse: 0.191s\n",
      "2021-11-19 17:28:38,958-INFO: epoch:20  valid step:80   loss:  1.5899 top1: 0.6875 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 17:28:40,764-INFO: END epoch:20  valid loss_avg:  1.2329 top1_avg: 0.8597 top5_avg: 0.9597 elapse_sum: 25.853s\n",
      "2021-11-19 17:28:40,764-INFO: The best top1 acc 0.85972, in epoch: 20\n",
      "2021-11-19 17:28:41,822-INFO: Already save model in ./output/EfficientNetB3/best_model\n",
      "2021-11-19 17:28:42,894-INFO: Already save model in ./output/EfficientNetB3/20\n",
      "2021-11-19 17:28:50,992-INFO: epoch:21  train step:0    loss:  2.4698 lr: 0.000998 elapse: 8.097s\n",
      "2021-11-19 17:28:59,706-INFO: epoch:21  train step:10   loss:  0.9290 lr: 0.000998 elapse: 0.701s\n",
      "2021-11-19 17:29:06,803-INFO: epoch:21  train step:20   loss:  2.0377 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:29:14,037-INFO: epoch:21  train step:30   loss:  1.5271 lr: 0.000998 elapse: 0.800s\n",
      "2021-11-19 17:29:21,291-INFO: epoch:21  train step:40   loss:  0.9309 lr: 0.000998 elapse: 0.766s\n",
      "2021-11-19 17:29:28,435-INFO: epoch:21  train step:50   loss:  1.1409 lr: 0.000998 elapse: 0.711s\n",
      "2021-11-19 17:29:35,696-INFO: epoch:21  train step:60   loss:  1.5317 lr: 0.000998 elapse: 0.771s\n",
      "2021-11-19 17:29:42,691-INFO: epoch:21  train step:70   loss:  2.5856 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:29:49,901-INFO: epoch:21  train step:80   loss:  0.9803 lr: 0.000998 elapse: 0.710s\n",
      "2021-11-19 17:29:57,103-INFO: epoch:21  train step:90   loss:  1.4393 lr: 0.000998 elapse: 0.697s\n",
      "2021-11-19 17:30:04,202-INFO: epoch:21  train step:100  loss:  1.8767 lr: 0.000998 elapse: 0.699s\n",
      "2021-11-19 17:30:11,204-INFO: epoch:21  train step:110  loss:  0.9348 lr: 0.000998 elapse: 0.700s\n",
      "2021-11-19 17:30:18,449-INFO: epoch:21  train step:120  loss:  1.9956 lr: 0.000998 elapse: 0.724s\n",
      "2021-11-19 17:30:25,704-INFO: epoch:21  train step:130  loss:  1.9923 lr: 0.000998 elapse: 0.701s\n",
      "2021-11-19 17:30:32,801-INFO: epoch:21  train step:140  loss:  0.9643 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:30:39,818-INFO: epoch:21  train step:150  loss:  0.9821 lr: 0.000998 elapse: 0.698s\n",
      "2021-11-19 17:30:47,023-INFO: epoch:21  train step:160  loss:  1.4840 lr: 0.000998 elapse: 0.718s\n",
      "2021-11-19 17:30:54,218-INFO: epoch:21  train step:170  loss:  2.2240 lr: 0.000998 elapse: 0.712s\n",
      "2021-11-19 17:31:01,423-INFO: epoch:21  train step:180  loss:  2.4729 lr: 0.000998 elapse: 0.721s\n",
      "2021-11-19 17:31:08,601-INFO: epoch:21  train step:190  loss:  0.8399 lr: 0.000998 elapse: 0.697s\n",
      "2021-11-19 17:31:15,536-INFO: epoch:21  train step:200  loss:  1.4098 lr: 0.000998 elapse: 0.678s\n",
      "2021-11-19 17:31:22,267-INFO: epoch:21  train step:210  loss:  1.0707 lr: 0.000998 elapse: 0.667s\n",
      "2021-11-19 17:31:29,224-INFO: epoch:21  train step:220  loss:  0.8367 lr: 0.000998 elapse: 0.677s\n",
      "2021-11-19 17:31:36,057-INFO: epoch:21  train step:230  loss:  2.0533 lr: 0.000998 elapse: 0.668s\n",
      "2021-11-19 17:31:42,305-INFO: END epoch:21  train loss_avg:  1.4024  elapse_sum: 179.408s\n",
      "2021-11-19 17:31:50,197-INFO: epoch:22  train step:0    loss:  2.4504 lr: 0.000998 elapse: 7.890s\n",
      "2021-11-19 17:31:59,079-INFO: epoch:22  train step:10   loss:  1.0066 lr: 0.000998 elapse: 0.686s\n",
      "2021-11-19 17:32:06,391-INFO: epoch:22  train step:20   loss:  0.8239 lr: 0.000997 elapse: 0.788s\n",
      "2021-11-19 17:32:13,502-INFO: epoch:22  train step:30   loss:  1.0942 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:32:20,591-INFO: epoch:22  train step:40   loss:  2.0251 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:32:28,001-INFO: epoch:22  train step:50   loss:  1.0401 lr: 0.000997 elapse: 0.710s\n",
      "2021-11-19 17:32:35,017-INFO: epoch:22  train step:60   loss:  1.7915 lr: 0.000997 elapse: 0.697s\n",
      "2021-11-19 17:32:42,291-INFO: epoch:22  train step:70   loss:  0.9502 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:32:49,320-INFO: epoch:22  train step:80   loss:  0.8856 lr: 0.000997 elapse: 0.716s\n",
      "2021-11-19 17:32:56,591-INFO: epoch:22  train step:90   loss:  0.9132 lr: 0.000997 elapse: 0.697s\n",
      "2021-11-19 17:33:03,618-INFO: epoch:22  train step:100  loss:  1.4036 lr: 0.000997 elapse: 0.715s\n",
      "2021-11-19 17:33:10,701-INFO: epoch:22  train step:110  loss:  0.9848 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:33:17,801-INFO: epoch:22  train step:120  loss:  1.7206 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:33:24,806-INFO: epoch:22  train step:130  loss:  1.0064 lr: 0.000997 elapse: 0.703s\n",
      "2021-11-19 17:33:32,204-INFO: epoch:22  train step:140  loss:  0.9194 lr: 0.000997 elapse: 0.710s\n",
      "2021-11-19 17:33:39,302-INFO: epoch:22  train step:150  loss:  0.9885 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:33:46,505-INFO: epoch:22  train step:160  loss:  1.1204 lr: 0.000997 elapse: 0.700s\n",
      "2021-11-19 17:33:53,704-INFO: epoch:22  train step:170  loss:  1.4213 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:34:01,005-INFO: epoch:22  train step:180  loss:  0.9502 lr: 0.000997 elapse: 0.700s\n",
      "2021-11-19 17:34:08,101-INFO: epoch:22  train step:190  loss:  1.1558 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:34:15,133-INFO: epoch:22  train step:200  loss:  1.0664 lr: 0.000997 elapse: 0.676s\n",
      "2021-11-19 17:34:21,829-INFO: epoch:22  train step:210  loss:  1.3395 lr: 0.000997 elapse: 0.668s\n",
      "2021-11-19 17:34:28,787-INFO: epoch:22  train step:220  loss:  1.1901 lr: 0.000997 elapse: 0.667s\n",
      "2021-11-19 17:34:35,756-INFO: epoch:22  train step:230  loss:  1.4871 lr: 0.000997 elapse: 0.688s\n",
      "2021-11-19 17:34:41,791-INFO: END epoch:22  train loss_avg:  1.3920  elapse_sum: 179.483s\n",
      "2021-11-19 17:34:49,997-INFO: epoch:23  train step:0    loss:  0.9033 lr: 0.000997 elapse: 8.205s\n",
      "2021-11-19 17:34:58,702-INFO: epoch:23  train step:10   loss:  0.9784 lr: 0.000997 elapse: 0.700s\n",
      "2021-11-19 17:35:05,726-INFO: epoch:23  train step:20   loss:  0.8891 lr: 0.000997 elapse: 0.719s\n",
      "2021-11-19 17:35:12,891-INFO: epoch:23  train step:30   loss:  1.9600 lr: 0.000997 elapse: 0.767s\n",
      "2021-11-19 17:35:19,903-INFO: epoch:23  train step:40   loss:  0.9878 lr: 0.000997 elapse: 0.683s\n",
      "2021-11-19 17:35:27,101-INFO: epoch:23  train step:50   loss:  0.9901 lr: 0.000997 elapse: 0.682s\n",
      "2021-11-19 17:35:34,305-INFO: epoch:23  train step:60   loss:  1.9587 lr: 0.000997 elapse: 0.763s\n",
      "2021-11-19 17:35:41,802-INFO: epoch:23  train step:70   loss:  0.9797 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:35:48,991-INFO: epoch:23  train step:80   loss:  1.5797 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:35:56,302-INFO: epoch:23  train step:90   loss:  0.8986 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:36:03,302-INFO: epoch:23  train step:100  loss:  2.6991 lr: 0.000997 elapse: 0.697s\n",
      "2021-11-19 17:36:10,491-INFO: epoch:23  train step:110  loss:  2.3496 lr: 0.000997 elapse: 0.767s\n",
      "2021-11-19 17:36:17,502-INFO: epoch:23  train step:120  loss:  0.9112 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:36:24,517-INFO: epoch:23  train step:130  loss:  0.8992 lr: 0.000997 elapse: 0.708s\n",
      "2021-11-19 17:36:31,618-INFO: epoch:23  train step:140  loss:  1.7981 lr: 0.000997 elapse: 0.714s\n",
      "2021-11-19 17:36:38,691-INFO: epoch:23  train step:150  loss:  0.9617 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:36:46,002-INFO: epoch:23  train step:160  loss:  0.9148 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:36:53,101-INFO: epoch:23  train step:170  loss:  0.9563 lr: 0.000997 elapse: 0.695s\n",
      "2021-11-19 17:37:00,192-INFO: epoch:23  train step:180  loss:  1.0534 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:37:07,292-INFO: epoch:23  train step:190  loss:  0.9864 lr: 0.000997 elapse: 0.700s\n",
      "2021-11-19 17:37:14,231-INFO: epoch:23  train step:200  loss:  0.8774 lr: 0.000997 elapse: 0.676s\n",
      "2021-11-19 17:37:20,906-INFO: epoch:23  train step:210  loss:  2.2016 lr: 0.000997 elapse: 0.668s\n",
      "2021-11-19 17:37:27,855-INFO: epoch:23  train step:220  loss:  0.8750 lr: 0.000997 elapse: 0.668s\n",
      "2021-11-19 17:37:34,783-INFO: epoch:23  train step:230  loss:  1.2557 lr: 0.000997 elapse: 0.667s\n",
      "2021-11-19 17:37:40,775-INFO: END epoch:23  train loss_avg:  1.3869  elapse_sum: 178.981s\n",
      "2021-11-19 17:37:49,307-INFO: epoch:24  train step:0    loss:  0.8718 lr: 0.000997 elapse: 8.530s\n",
      "2021-11-19 17:37:58,191-INFO: epoch:24  train step:10   loss:  0.8562 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:38:05,225-INFO: epoch:24  train step:20   loss:  0.8605 lr: 0.000997 elapse: 0.722s\n",
      "2021-11-19 17:38:12,391-INFO: epoch:24  train step:30   loss:  1.4422 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:38:19,502-INFO: epoch:24  train step:40   loss:  2.5318 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:38:26,701-INFO: epoch:24  train step:50   loss:  1.2624 lr: 0.000997 elapse: 0.778s\n",
      "2021-11-19 17:38:33,701-INFO: epoch:24  train step:60   loss:  2.4441 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:38:40,944-INFO: epoch:24  train step:70   loss:  2.1185 lr: 0.000997 elapse: 0.737s\n",
      "2021-11-19 17:38:48,203-INFO: epoch:24  train step:80   loss:  0.8562 lr: 0.000997 elapse: 0.701s\n",
      "2021-11-19 17:38:55,503-INFO: epoch:24  train step:90   loss:  1.6350 lr: 0.000997 elapse: 0.764s\n",
      "2021-11-19 17:39:02,702-INFO: epoch:24  train step:100  loss:  1.0832 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:39:09,902-INFO: epoch:24  train step:110  loss:  1.1380 lr: 0.000997 elapse: 0.709s\n",
      "2021-11-19 17:39:16,907-INFO: epoch:24  train step:120  loss:  0.8810 lr: 0.000997 elapse: 0.702s\n",
      "2021-11-19 17:39:23,991-INFO: epoch:24  train step:130  loss:  1.1076 lr: 0.000997 elapse: 0.699s\n",
      "2021-11-19 17:39:31,104-INFO: epoch:24  train step:140  loss:  0.9522 lr: 0.000997 elapse: 0.763s\n",
      "2021-11-19 17:39:38,104-INFO: epoch:24  train step:150  loss:  1.7970 lr: 0.000997 elapse: 0.700s\n",
      "2021-11-19 17:39:45,291-INFO: epoch:24  train step:160  loss:  1.6513 lr: 0.000997 elapse: 0.764s\n",
      "2021-11-19 17:39:52,503-INFO: epoch:24  train step:170  loss:  1.8037 lr: 0.000997 elapse: 0.742s\n",
      "2021-11-19 17:39:59,805-INFO: epoch:24  train step:180  loss:  2.7826 lr: 0.000997 elapse: 0.712s\n",
      "2021-11-19 17:40:07,003-INFO: epoch:24  train step:190  loss:  1.1922 lr: 0.000997 elapse: 0.698s\n",
      "2021-11-19 17:40:14,033-INFO: epoch:24  train step:200  loss:  2.1028 lr: 0.000997 elapse: 0.677s\n",
      "2021-11-19 17:40:20,723-INFO: epoch:24  train step:210  loss:  0.9054 lr: 0.000997 elapse: 0.671s\n",
      "2021-11-19 17:40:27,686-INFO: epoch:24  train step:220  loss:  1.7778 lr: 0.000997 elapse: 0.669s\n",
      "2021-11-19 17:40:34,662-INFO: epoch:24  train step:230  loss:  0.8651 lr: 0.000996 elapse: 0.667s\n",
      "2021-11-19 17:40:40,677-INFO: END epoch:24  train loss_avg:  1.3616  elapse_sum: 179.899s\n",
      "2021-11-19 17:40:48,592-INFO: epoch:25  train step:0    loss:  1.1975 lr: 0.000996 elapse: 7.914s\n",
      "2021-11-19 17:40:57,491-INFO: epoch:25  train step:10   loss:  1.6656 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:41:04,717-INFO: epoch:25  train step:20   loss:  1.9648 lr: 0.000996 elapse: 0.749s\n",
      "2021-11-19 17:41:11,906-INFO: epoch:25  train step:30   loss:  0.9060 lr: 0.000996 elapse: 0.702s\n",
      "2021-11-19 17:41:18,944-INFO: epoch:25  train step:40   loss:  1.4400 lr: 0.000996 elapse: 0.724s\n",
      "2021-11-19 17:41:26,191-INFO: epoch:25  train step:50   loss:  2.3498 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:41:33,291-INFO: epoch:25  train step:60   loss:  2.1127 lr: 0.000996 elapse: 0.698s\n",
      "2021-11-19 17:41:40,292-INFO: epoch:25  train step:70   loss:  1.1637 lr: 0.000996 elapse: 0.698s\n",
      "2021-11-19 17:41:47,305-INFO: epoch:25  train step:80   loss:  1.1173 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:41:54,491-INFO: epoch:25  train step:90   loss:  1.2948 lr: 0.000996 elapse: 0.741s\n",
      "2021-11-19 17:42:01,503-INFO: epoch:25  train step:100  loss:  0.8702 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:42:08,804-INFO: epoch:25  train step:110  loss:  1.0052 lr: 0.000996 elapse: 0.713s\n",
      "2021-11-19 17:42:16,161-INFO: epoch:25  train step:120  loss:  1.3168 lr: 0.000996 elapse: 0.759s\n",
      "2021-11-19 17:42:23,306-INFO: epoch:25  train step:130  loss:  2.1034 lr: 0.000996 elapse: 0.753s\n",
      "2021-11-19 17:42:30,444-INFO: epoch:25  train step:140  loss:  2.2989 lr: 0.000996 elapse: 0.739s\n",
      "2021-11-19 17:42:37,591-INFO: epoch:25  train step:150  loss:  1.2449 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:42:44,691-INFO: epoch:25  train step:160  loss:  0.9832 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:42:51,805-INFO: epoch:25  train step:170  loss:  0.8286 lr: 0.000996 elapse: 0.702s\n",
      "2021-11-19 17:42:58,821-INFO: epoch:25  train step:180  loss:  1.9367 lr: 0.000996 elapse: 0.714s\n",
      "2021-11-19 17:43:06,001-INFO: epoch:25  train step:190  loss:  2.4233 lr: 0.000996 elapse: 0.697s\n",
      "2021-11-19 17:43:13,073-INFO: epoch:25  train step:200  loss:  0.9159 lr: 0.000996 elapse: 0.678s\n",
      "2021-11-19 17:43:19,767-INFO: epoch:25  train step:210  loss:  1.0770 lr: 0.000996 elapse: 0.667s\n",
      "2021-11-19 17:43:26,757-INFO: epoch:25  train step:220  loss:  0.9261 lr: 0.000996 elapse: 0.676s\n",
      "2021-11-19 17:43:33,667-INFO: epoch:25  train step:230  loss:  1.1731 lr: 0.000996 elapse: 0.771s\n",
      "2021-11-19 17:43:39,801-INFO: END epoch:25  train loss_avg:  1.3369  elapse_sum: 179.121s\n",
      "2021-11-19 17:43:47,892-INFO: epoch:26  train step:0    loss:  1.6916 lr: 0.000996 elapse: 8.089s\n",
      "2021-11-19 17:43:56,602-INFO: epoch:26  train step:10   loss:  2.4620 lr: 0.000996 elapse: 0.698s\n",
      "2021-11-19 17:44:03,691-INFO: epoch:26  train step:20   loss:  1.8939 lr: 0.000996 elapse: 0.766s\n",
      "2021-11-19 17:44:10,703-INFO: epoch:26  train step:30   loss:  0.8904 lr: 0.000996 elapse: 0.680s\n",
      "2021-11-19 17:44:17,936-INFO: epoch:26  train step:40   loss:  2.1778 lr: 0.000996 elapse: 0.730s\n",
      "2021-11-19 17:44:25,091-INFO: epoch:26  train step:50   loss:  0.9499 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:44:32,301-INFO: epoch:26  train step:60   loss:  1.5514 lr: 0.000996 elapse: 0.698s\n",
      "2021-11-19 17:44:39,305-INFO: epoch:26  train step:70   loss:  1.1362 lr: 0.000996 elapse: 0.701s\n",
      "2021-11-19 17:44:46,429-INFO: epoch:26  train step:80   loss:  0.8867 lr: 0.000996 elapse: 0.726s\n",
      "2021-11-19 17:44:53,803-INFO: epoch:26  train step:90   loss:  1.4173 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:45:01,004-INFO: epoch:26  train step:100  loss:  1.0258 lr: 0.000996 elapse: 0.712s\n",
      "2021-11-19 17:45:08,002-INFO: epoch:26  train step:110  loss:  0.9072 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:45:15,007-INFO: epoch:26  train step:120  loss:  1.1656 lr: 0.000996 elapse: 0.688s\n",
      "2021-11-19 17:45:22,452-INFO: epoch:26  train step:130  loss:  0.9525 lr: 0.000996 elapse: 0.790s\n",
      "2021-11-19 17:45:29,591-INFO: epoch:26  train step:140  loss:  2.3043 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:45:36,691-INFO: epoch:26  train step:150  loss:  2.6388 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:45:43,703-INFO: epoch:26  train step:160  loss:  2.3828 lr: 0.000996 elapse: 0.697s\n",
      "2021-11-19 17:45:50,891-INFO: epoch:26  train step:170  loss:  2.4223 lr: 0.000996 elapse: 0.772s\n",
      "2021-11-19 17:45:57,991-INFO: epoch:26  train step:180  loss:  1.2653 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:46:05,092-INFO: epoch:26  train step:190  loss:  0.9230 lr: 0.000996 elapse: 0.689s\n",
      "2021-11-19 17:46:12,058-INFO: epoch:26  train step:200  loss:  0.8326 lr: 0.000996 elapse: 0.677s\n",
      "2021-11-19 17:46:18,758-INFO: epoch:26  train step:210  loss:  1.2508 lr: 0.000996 elapse: 0.666s\n",
      "2021-11-19 17:46:25,741-INFO: epoch:26  train step:220  loss:  0.8758 lr: 0.000996 elapse: 0.668s\n",
      "2021-11-19 17:46:32,674-INFO: epoch:26  train step:230  loss:  0.9286 lr: 0.000996 elapse: 0.668s\n",
      "2021-11-19 17:46:38,671-INFO: END epoch:26  train loss_avg:  1.3445  elapse_sum: 178.867s\n",
      "2021-11-19 17:46:46,893-INFO: epoch:27  train step:0    loss:  1.7414 lr: 0.000996 elapse: 8.220s\n",
      "2021-11-19 17:46:55,793-INFO: epoch:27  train step:10   loss:  0.8604 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:47:02,802-INFO: epoch:27  train step:20   loss:  0.8834 lr: 0.000996 elapse: 0.696s\n",
      "2021-11-19 17:47:09,902-INFO: epoch:27  train step:30   loss:  1.0587 lr: 0.000996 elapse: 0.699s\n",
      "2021-11-19 17:47:17,018-INFO: epoch:27  train step:40   loss:  0.9712 lr: 0.000996 elapse: 0.715s\n",
      "2021-11-19 17:47:24,102-INFO: epoch:27  train step:50   loss:  1.4029 lr: 0.000996 elapse: 0.698s\n",
      "2021-11-19 17:47:31,501-INFO: epoch:27  train step:60   loss:  1.6325 lr: 0.000996 elapse: 0.755s\n",
      "2021-11-19 17:47:38,617-INFO: epoch:27  train step:70   loss:  0.8692 lr: 0.000996 elapse: 0.713s\n",
      "2021-11-19 17:47:45,722-INFO: epoch:27  train step:80   loss:  1.0557 lr: 0.000996 elapse: 0.719s\n",
      "2021-11-19 17:47:52,991-INFO: epoch:27  train step:90   loss:  1.4247 lr: 0.000996 elapse: 0.785s\n",
      "2021-11-19 17:48:00,191-INFO: epoch:27  train step:100  loss:  1.0088 lr: 0.000996 elapse: 0.787s\n",
      "2021-11-19 17:48:07,291-INFO: epoch:27  train step:110  loss:  1.0378 lr: 0.000995 elapse: 0.764s\n",
      "2021-11-19 17:48:14,402-INFO: epoch:27  train step:120  loss:  0.8887 lr: 0.000995 elapse: 0.778s\n",
      "2021-11-19 17:48:21,493-INFO: epoch:27  train step:130  loss:  1.2902 lr: 0.000995 elapse: 0.701s\n",
      "2021-11-19 17:48:28,508-INFO: epoch:27  train step:140  loss:  2.1427 lr: 0.000995 elapse: 0.705s\n",
      "2021-11-19 17:48:36,002-INFO: epoch:27  train step:150  loss:  2.5216 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:48:43,192-INFO: epoch:27  train step:160  loss:  0.9983 lr: 0.000995 elapse: 0.769s\n",
      "2021-11-19 17:48:50,304-INFO: epoch:27  train step:170  loss:  0.8455 lr: 0.000995 elapse: 0.701s\n",
      "2021-11-19 17:48:57,403-INFO: epoch:27  train step:180  loss:  2.2289 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:49:04,591-INFO: epoch:27  train step:190  loss:  0.8766 lr: 0.000995 elapse: 0.762s\n",
      "2021-11-19 17:49:11,525-INFO: epoch:27  train step:200  loss:  2.6604 lr: 0.000995 elapse: 0.667s\n",
      "2021-11-19 17:49:18,199-INFO: epoch:27  train step:210  loss:  2.0080 lr: 0.000995 elapse: 0.667s\n",
      "2021-11-19 17:49:25,087-INFO: epoch:27  train step:220  loss:  1.2315 lr: 0.000995 elapse: 0.666s\n",
      "2021-11-19 17:49:32,030-INFO: epoch:27  train step:230  loss:  2.5061 lr: 0.000995 elapse: 0.668s\n",
      "2021-11-19 17:49:38,046-INFO: END epoch:27  train loss_avg:  1.3110  elapse_sum: 179.373s\n",
      "2021-11-19 17:49:45,792-INFO: epoch:28  train step:0    loss:  1.6940 lr: 0.000995 elapse: 7.744s\n",
      "2021-11-19 17:49:54,702-INFO: epoch:28  train step:10   loss:  1.0266 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:50:01,802-INFO: epoch:28  train step:20   loss:  0.8248 lr: 0.000995 elapse: 0.766s\n",
      "2021-11-19 17:50:09,001-INFO: epoch:28  train step:30   loss:  2.5098 lr: 0.000995 elapse: 0.753s\n",
      "2021-11-19 17:50:16,101-INFO: epoch:28  train step:40   loss:  2.3780 lr: 0.000995 elapse: 0.709s\n",
      "2021-11-19 17:50:23,191-INFO: epoch:28  train step:50   loss:  1.1095 lr: 0.000995 elapse: 0.697s\n",
      "2021-11-19 17:50:30,217-INFO: epoch:28  train step:60   loss:  0.8723 lr: 0.000995 elapse: 0.712s\n",
      "2021-11-19 17:50:37,493-INFO: epoch:28  train step:70   loss:  1.5854 lr: 0.000995 elapse: 0.732s\n",
      "2021-11-19 17:50:44,802-INFO: epoch:28  train step:80   loss:  1.0081 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:50:52,024-INFO: epoch:28  train step:90   loss:  1.0965 lr: 0.000995 elapse: 0.712s\n",
      "2021-11-19 17:50:59,191-INFO: epoch:28  train step:100  loss:  1.1006 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:51:06,291-INFO: epoch:28  train step:110  loss:  1.8832 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:51:13,394-INFO: epoch:28  train step:120  loss:  1.4181 lr: 0.000995 elapse: 0.701s\n",
      "2021-11-19 17:51:20,501-INFO: epoch:28  train step:130  loss:  0.9203 lr: 0.000995 elapse: 0.771s\n",
      "2021-11-19 17:51:27,605-INFO: epoch:28  train step:140  loss:  1.0327 lr: 0.000995 elapse: 0.703s\n",
      "2021-11-19 17:51:34,703-INFO: epoch:28  train step:150  loss:  0.8758 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:51:41,819-INFO: epoch:28  train step:160  loss:  2.3571 lr: 0.000995 elapse: 0.715s\n",
      "2021-11-19 17:51:49,302-INFO: epoch:28  train step:170  loss:  1.6107 lr: 0.000995 elapse: 0.722s\n",
      "2021-11-19 17:51:56,522-INFO: epoch:28  train step:180  loss:  1.0380 lr: 0.000995 elapse: 0.716s\n",
      "2021-11-19 17:52:03,503-INFO: epoch:28  train step:190  loss:  0.8930 lr: 0.000995 elapse: 0.700s\n",
      "2021-11-19 17:52:10,500-INFO: epoch:28  train step:200  loss:  1.4715 lr: 0.000995 elapse: 0.666s\n",
      "2021-11-19 17:52:17,169-INFO: epoch:28  train step:210  loss:  0.9139 lr: 0.000995 elapse: 0.666s\n",
      "2021-11-19 17:52:24,074-INFO: epoch:28  train step:220  loss:  2.4333 lr: 0.000995 elapse: 0.666s\n",
      "2021-11-19 17:52:30,970-INFO: epoch:28  train step:230  loss:  0.9665 lr: 0.000995 elapse: 0.673s\n",
      "2021-11-19 17:52:37,043-INFO: END epoch:28  train loss_avg:  1.3609  elapse_sum: 178.994s\n",
      "2021-11-19 17:52:45,292-INFO: epoch:29  train step:0    loss:  2.8210 lr: 0.000995 elapse: 8.247s\n",
      "2021-11-19 17:52:54,280-INFO: epoch:29  train step:10   loss:  2.0364 lr: 0.000995 elapse: 0.696s\n",
      "2021-11-19 17:53:01,503-INFO: epoch:29  train step:20   loss:  0.7886 lr: 0.000995 elapse: 0.700s\n",
      "2021-11-19 17:53:08,602-INFO: epoch:29  train step:30   loss:  1.8999 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:53:15,705-INFO: epoch:29  train step:40   loss:  1.0033 lr: 0.000995 elapse: 0.702s\n",
      "2021-11-19 17:53:22,802-INFO: epoch:29  train step:50   loss:  1.0677 lr: 0.000995 elapse: 0.710s\n",
      "2021-11-19 17:53:29,891-INFO: epoch:29  train step:60   loss:  1.9089 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:53:37,003-INFO: epoch:29  train step:70   loss:  1.6639 lr: 0.000995 elapse: 0.778s\n",
      "2021-11-19 17:53:44,001-INFO: epoch:29  train step:80   loss:  0.8370 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:53:51,102-INFO: epoch:29  train step:90   loss:  0.9976 lr: 0.000995 elapse: 0.740s\n",
      "2021-11-19 17:53:58,592-INFO: epoch:29  train step:100  loss:  1.0838 lr: 0.000995 elapse: 0.700s\n",
      "2021-11-19 17:54:05,703-INFO: epoch:29  train step:110  loss:  1.3667 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:54:12,724-INFO: epoch:29  train step:120  loss:  2.3178 lr: 0.000995 elapse: 0.721s\n",
      "2021-11-19 17:54:19,802-INFO: epoch:29  train step:130  loss:  0.9794 lr: 0.000995 elapse: 0.698s\n",
      "2021-11-19 17:54:26,997-INFO: epoch:29  train step:140  loss:  0.8211 lr: 0.000995 elapse: 0.706s\n",
      "2021-11-19 17:54:34,206-INFO: epoch:29  train step:150  loss:  0.9612 lr: 0.000995 elapse: 0.685s\n",
      "2021-11-19 17:54:41,303-INFO: epoch:29  train step:160  loss:  1.0192 lr: 0.000995 elapse: 0.699s\n",
      "2021-11-19 17:54:48,391-INFO: epoch:29  train step:170  loss:  0.9497 lr: 0.000994 elapse: 0.767s\n",
      "2021-11-19 17:54:55,503-INFO: epoch:29  train step:180  loss:  0.9073 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:55:02,802-INFO: epoch:29  train step:190  loss:  0.9028 lr: 0.000994 elapse: 0.694s\n",
      "2021-11-19 17:55:09,838-INFO: epoch:29  train step:200  loss:  1.9904 lr: 0.000994 elapse: 0.679s\n",
      "2021-11-19 17:55:16,535-INFO: epoch:29  train step:210  loss:  1.8989 lr: 0.000994 elapse: 0.665s\n",
      "2021-11-19 17:55:23,471-INFO: epoch:29  train step:220  loss:  0.8763 lr: 0.000994 elapse: 0.669s\n",
      "2021-11-19 17:55:30,416-INFO: epoch:29  train step:230  loss:  1.4786 lr: 0.000994 elapse: 0.666s\n",
      "2021-11-19 17:55:36,439-INFO: END epoch:29  train loss_avg:  1.3529  elapse_sum: 179.393s\n",
      "2021-11-19 17:55:44,593-INFO: epoch:30  train step:0    loss:  0.9681 lr: 0.000994 elapse: 8.152s\n",
      "2021-11-19 17:55:53,315-INFO: epoch:30  train step:10   loss:  1.4997 lr: 0.000994 elapse: 0.710s\n",
      "2021-11-19 17:56:00,421-INFO: epoch:30  train step:20   loss:  2.1780 lr: 0.000994 elapse: 0.715s\n",
      "2021-11-19 17:56:07,791-INFO: epoch:30  train step:30   loss:  1.9027 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:56:14,891-INFO: epoch:30  train step:40   loss:  2.5650 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:56:21,904-INFO: epoch:30  train step:50   loss:  1.6230 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:56:28,991-INFO: epoch:30  train step:60   loss:  2.4160 lr: 0.000994 elapse: 0.767s\n",
      "2021-11-19 17:56:36,105-INFO: epoch:30  train step:70   loss:  0.9680 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:56:43,191-INFO: epoch:30  train step:80   loss:  0.9971 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:56:50,308-INFO: epoch:30  train step:90   loss:  1.8182 lr: 0.000994 elapse: 0.704s\n",
      "2021-11-19 17:56:57,402-INFO: epoch:30  train step:100  loss:  1.8462 lr: 0.000994 elapse: 0.686s\n",
      "2021-11-19 17:57:04,403-INFO: epoch:30  train step:110  loss:  0.9454 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 17:57:11,704-INFO: epoch:30  train step:120  loss:  0.9569 lr: 0.000994 elapse: 0.696s\n",
      "2021-11-19 17:57:18,891-INFO: epoch:30  train step:130  loss:  1.0627 lr: 0.000994 elapse: 0.764s\n",
      "2021-11-19 17:57:25,902-INFO: epoch:30  train step:140  loss:  2.2811 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:57:33,093-INFO: epoch:30  train step:150  loss:  2.0053 lr: 0.000994 elapse: 0.747s\n",
      "2021-11-19 17:57:40,191-INFO: epoch:30  train step:160  loss:  2.0709 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:57:47,291-INFO: epoch:30  train step:170  loss:  0.8643 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 17:57:54,403-INFO: epoch:30  train step:180  loss:  1.7596 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 17:58:01,404-INFO: epoch:30  train step:190  loss:  1.8936 lr: 0.000994 elapse: 0.697s\n",
      "2021-11-19 17:58:08,446-INFO: epoch:30  train step:200  loss:  1.4481 lr: 0.000994 elapse: 0.676s\n",
      "2021-11-19 17:58:15,206-INFO: epoch:30  train step:210  loss:  1.0461 lr: 0.000994 elapse: 0.668s\n",
      "2021-11-19 17:58:22,151-INFO: epoch:30  train step:220  loss:  0.9678 lr: 0.000994 elapse: 0.677s\n",
      "2021-11-19 17:58:29,134-INFO: epoch:30  train step:230  loss:  1.9113 lr: 0.000994 elapse: 0.678s\n",
      "2021-11-19 17:58:35,210-INFO: END epoch:30  train loss_avg:  1.3578  elapse_sum: 178.768s\n",
      "2021-11-19 17:58:39,917-INFO: epoch:30  valid step:0    loss:  1.2928 top1: 0.8125 top5: 1.0000 elapse: 4.705s\n",
      "2021-11-19 17:58:45,063-INFO: epoch:30  valid step:10   loss:  1.4819 top1: 0.7500 top5: 0.9375 elapse: 0.229s\n",
      "2021-11-19 17:58:47,390-INFO: epoch:30  valid step:20   loss:  1.5352 top1: 0.6875 top5: 0.8750 elapse: 0.230s\n",
      "2021-11-19 17:58:49,502-INFO: epoch:30  valid step:30   loss:  1.2196 top1: 0.8750 top5: 0.9375 elapse: 0.189s\n",
      "2021-11-19 17:58:51,391-INFO: epoch:30  valid step:40   loss:  1.0794 top1: 1.0000 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 17:58:53,286-INFO: epoch:30  valid step:50   loss:  1.5731 top1: 0.7500 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 17:58:55,207-INFO: epoch:30  valid step:60   loss:  1.6511 top1: 0.8125 top5: 0.8750 elapse: 0.193s\n",
      "2021-11-19 17:58:57,182-INFO: epoch:30  valid step:70   loss:  1.0994 top1: 0.8750 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 17:58:59,164-INFO: epoch:30  valid step:80   loss:  1.4371 top1: 0.7500 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 17:59:01,059-INFO: END epoch:30  valid loss_avg:  1.2622 top1_avg: 0.8451 top5_avg: 0.9632 elapse_sum: 25.847s\n",
      "2021-11-19 17:59:02,349-INFO: Already save model in ./output/EfficientNetB3/30\n",
      "2021-11-19 17:59:10,492-INFO: epoch:31  train step:0    loss:  2.3603 lr: 0.000994 elapse: 8.142s\n",
      "2021-11-19 17:59:19,403-INFO: epoch:31  train step:10   loss:  1.0160 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 17:59:26,602-INFO: epoch:31  train step:20   loss:  0.9229 lr: 0.000994 elapse: 0.700s\n",
      "2021-11-19 17:59:33,702-INFO: epoch:31  train step:30   loss:  1.4732 lr: 0.000994 elapse: 0.697s\n",
      "2021-11-19 17:59:40,891-INFO: epoch:31  train step:40   loss:  0.9484 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 17:59:48,001-INFO: epoch:31  train step:50   loss:  2.1246 lr: 0.000994 elapse: 0.709s\n",
      "2021-11-19 17:59:55,201-INFO: epoch:31  train step:60   loss:  1.2676 lr: 0.000994 elapse: 0.695s\n",
      "2021-11-19 18:00:02,302-INFO: epoch:31  train step:70   loss:  2.5282 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 18:00:09,391-INFO: epoch:31  train step:80   loss:  2.2531 lr: 0.000994 elapse: 0.695s\n",
      "2021-11-19 18:00:16,491-INFO: epoch:31  train step:90   loss:  0.9837 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 18:00:23,804-INFO: epoch:31  train step:100  loss:  0.8033 lr: 0.000994 elapse: 0.702s\n",
      "2021-11-19 18:00:30,891-INFO: epoch:31  train step:110  loss:  0.9485 lr: 0.000994 elapse: 0.766s\n",
      "2021-11-19 18:00:38,005-INFO: epoch:31  train step:120  loss:  2.1323 lr: 0.000994 elapse: 0.701s\n",
      "2021-11-19 18:00:45,037-INFO: epoch:31  train step:130  loss:  2.6552 lr: 0.000994 elapse: 0.733s\n",
      "2021-11-19 18:00:52,302-INFO: epoch:31  train step:140  loss:  2.4034 lr: 0.000994 elapse: 0.699s\n",
      "2021-11-19 18:00:59,428-INFO: epoch:31  train step:150  loss:  1.6003 lr: 0.000994 elapse: 0.726s\n",
      "2021-11-19 18:01:06,620-INFO: epoch:31  train step:160  loss:  1.6877 lr: 0.000994 elapse: 0.716s\n",
      "2021-11-19 18:01:13,704-INFO: epoch:31  train step:170  loss:  0.9289 lr: 0.000994 elapse: 0.698s\n",
      "2021-11-19 18:01:20,902-INFO: epoch:31  train step:180  loss:  0.8623 lr: 0.000993 elapse: 0.696s\n",
      "2021-11-19 18:01:28,321-INFO: epoch:31  train step:190  loss:  0.8809 lr: 0.000993 elapse: 0.717s\n",
      "2021-11-19 18:01:35,312-INFO: epoch:31  train step:200  loss:  1.0171 lr: 0.000993 elapse: 0.667s\n",
      "2021-11-19 18:01:41,993-INFO: epoch:31  train step:210  loss:  0.9029 lr: 0.000993 elapse: 0.667s\n",
      "2021-11-19 18:01:48,938-INFO: epoch:31  train step:220  loss:  0.7926 lr: 0.000993 elapse: 0.679s\n",
      "2021-11-19 18:01:55,875-INFO: epoch:31  train step:230  loss:  0.8825 lr: 0.000993 elapse: 0.667s\n",
      "2021-11-19 18:02:01,964-INFO: END epoch:31  train loss_avg:  1.3634  elapse_sum: 179.612s\n",
      "2021-11-19 18:02:10,092-INFO: epoch:32  train step:0    loss:  0.8433 lr: 0.000993 elapse: 8.127s\n",
      "2021-11-19 18:02:18,602-INFO: epoch:32  train step:10   loss:  0.8671 lr: 0.000993 elapse: 0.697s\n",
      "2021-11-19 18:02:25,735-INFO: epoch:32  train step:20   loss:  1.0559 lr: 0.000993 elapse: 0.733s\n",
      "2021-11-19 18:02:33,291-INFO: epoch:32  train step:30   loss:  1.6215 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:02:40,503-INFO: epoch:32  train step:40   loss:  2.3331 lr: 0.000993 elapse: 0.700s\n",
      "2021-11-19 18:02:47,703-INFO: epoch:32  train step:50   loss:  1.6795 lr: 0.000993 elapse: 0.711s\n",
      "2021-11-19 18:02:54,804-INFO: epoch:32  train step:60   loss:  0.8551 lr: 0.000993 elapse: 0.700s\n",
      "2021-11-19 18:03:01,903-INFO: epoch:32  train step:70   loss:  1.6058 lr: 0.000993 elapse: 0.700s\n",
      "2021-11-19 18:03:09,002-INFO: epoch:32  train step:80   loss:  1.7304 lr: 0.000993 elapse: 0.709s\n",
      "2021-11-19 18:03:16,002-INFO: epoch:32  train step:90   loss:  2.3688 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:03:23,102-INFO: epoch:32  train step:100  loss:  0.8787 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:03:30,202-INFO: epoch:32  train step:110  loss:  0.8482 lr: 0.000993 elapse: 0.709s\n",
      "2021-11-19 18:03:37,594-INFO: epoch:32  train step:120  loss:  2.0566 lr: 0.000993 elapse: 0.693s\n",
      "2021-11-19 18:03:44,602-INFO: epoch:32  train step:130  loss:  2.3575 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:03:51,721-INFO: epoch:32  train step:140  loss:  1.0900 lr: 0.000993 elapse: 0.718s\n",
      "2021-11-19 18:03:58,820-INFO: epoch:32  train step:150  loss:  1.2099 lr: 0.000993 elapse: 0.717s\n",
      "2021-11-19 18:04:06,002-INFO: epoch:32  train step:160  loss:  2.1423 lr: 0.000993 elapse: 0.698s\n",
      "2021-11-19 18:04:13,202-INFO: epoch:32  train step:170  loss:  1.0740 lr: 0.000993 elapse: 0.711s\n",
      "2021-11-19 18:04:20,391-INFO: epoch:32  train step:180  loss:  0.9160 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:04:27,407-INFO: epoch:32  train step:190  loss:  1.9376 lr: 0.000993 elapse: 0.702s\n",
      "2021-11-19 18:04:34,456-INFO: epoch:32  train step:200  loss:  2.4028 lr: 0.000993 elapse: 0.677s\n",
      "2021-11-19 18:04:41,151-INFO: epoch:32  train step:210  loss:  2.1077 lr: 0.000993 elapse: 0.669s\n",
      "2021-11-19 18:04:47,958-INFO: epoch:32  train step:220  loss:  1.0517 lr: 0.000993 elapse: 0.668s\n",
      "2021-11-19 18:04:54,908-INFO: epoch:32  train step:230  loss:  2.6190 lr: 0.000993 elapse: 0.791s\n",
      "2021-11-19 18:05:01,027-INFO: END epoch:32  train loss_avg:  1.4563  elapse_sum: 179.060s\n",
      "2021-11-19 18:05:08,996-INFO: epoch:33  train step:0    loss:  0.8577 lr: 0.000993 elapse: 7.967s\n",
      "2021-11-19 18:05:17,923-INFO: epoch:33  train step:10   loss:  1.5363 lr: 0.000993 elapse: 0.719s\n",
      "2021-11-19 18:05:25,102-INFO: epoch:33  train step:20   loss:  2.5683 lr: 0.000993 elapse: 0.697s\n",
      "2021-11-19 18:05:32,291-INFO: epoch:33  train step:30   loss:  0.8751 lr: 0.000993 elapse: 0.770s\n",
      "2021-11-19 18:05:39,423-INFO: epoch:33  train step:40   loss:  0.8887 lr: 0.000993 elapse: 0.728s\n",
      "2021-11-19 18:05:46,791-INFO: epoch:33  train step:50   loss:  0.8983 lr: 0.000993 elapse: 0.697s\n",
      "2021-11-19 18:05:53,802-INFO: epoch:33  train step:60   loss:  1.0070 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:06:00,918-INFO: epoch:33  train step:70   loss:  0.8540 lr: 0.000993 elapse: 0.702s\n",
      "2021-11-19 18:06:08,003-INFO: epoch:33  train step:80   loss:  1.0614 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:06:15,199-INFO: epoch:33  train step:90   loss:  1.5310 lr: 0.000993 elapse: 0.717s\n",
      "2021-11-19 18:06:22,303-INFO: epoch:33  train step:100  loss:  2.4158 lr: 0.000993 elapse: 0.699s\n",
      "2021-11-19 18:06:29,403-INFO: epoch:33  train step:110  loss:  2.1380 lr: 0.000993 elapse: 0.700s\n",
      "2021-11-19 18:06:36,602-INFO: epoch:33  train step:120  loss:  2.4561 lr: 0.000993 elapse: 0.698s\n",
      "2021-11-19 18:06:43,752-INFO: epoch:33  train step:130  loss:  0.9555 lr: 0.000993 elapse: 0.747s\n",
      "2021-11-19 18:06:51,006-INFO: epoch:33  train step:140  loss:  2.1697 lr: 0.000993 elapse: 0.700s\n",
      "2021-11-19 18:06:58,191-INFO: epoch:33  train step:150  loss:  0.8538 lr: 0.000992 elapse: 0.698s\n",
      "2021-11-19 18:07:05,244-INFO: epoch:33  train step:160  loss:  2.6378 lr: 0.000992 elapse: 0.737s\n",
      "2021-11-19 18:07:12,506-INFO: epoch:33  train step:170  loss:  2.1602 lr: 0.000992 elapse: 0.704s\n",
      "2021-11-19 18:07:19,602-INFO: epoch:33  train step:180  loss:  0.8522 lr: 0.000992 elapse: 0.700s\n",
      "2021-11-19 18:07:26,691-INFO: epoch:33  train step:190  loss:  2.7540 lr: 0.000992 elapse: 0.700s\n",
      "2021-11-19 18:07:33,633-INFO: epoch:33  train step:200  loss:  1.9852 lr: 0.000992 elapse: 0.675s\n",
      "2021-11-19 18:07:40,314-INFO: epoch:33  train step:210  loss:  0.8978 lr: 0.000992 elapse: 0.667s\n",
      "2021-11-19 18:07:47,296-INFO: epoch:33  train step:220  loss:  0.8286 lr: 0.000992 elapse: 0.667s\n",
      "2021-11-19 18:07:54,391-INFO: epoch:33  train step:230  loss:  2.4739 lr: 0.000992 elapse: 0.748s\n",
      "2021-11-19 18:08:00,519-INFO: END epoch:33  train loss_avg:  1.4446  elapse_sum: 179.489s\n",
      "2021-11-19 18:08:07,993-INFO: epoch:34  train step:0    loss:  2.4279 lr: 0.000992 elapse: 7.471s\n",
      "2021-11-19 18:08:17,202-INFO: epoch:34  train step:10   loss:  0.9675 lr: 0.000992 elapse: 0.699s\n",
      "2021-11-19 18:08:24,403-INFO: epoch:34  train step:20   loss:  2.2225 lr: 0.000992 elapse: 0.707s\n",
      "2021-11-19 18:08:31,405-INFO: epoch:34  train step:30   loss:  1.2371 lr: 0.000992 elapse: 0.681s\n",
      "2021-11-19 18:08:38,405-INFO: epoch:34  train step:40   loss:  1.1219 lr: 0.000992 elapse: 0.702s\n",
      "2021-11-19 18:08:45,533-INFO: epoch:34  train step:50   loss:  0.9588 lr: 0.000992 elapse: 0.730s\n",
      "2021-11-19 18:08:52,907-INFO: epoch:34  train step:60   loss:  0.8342 lr: 0.000992 elapse: 0.789s\n",
      "2021-11-19 18:09:00,091-INFO: epoch:34  train step:70   loss:  1.4620 lr: 0.000992 elapse: 0.769s\n",
      "2021-11-19 18:09:07,101-INFO: epoch:34  train step:80   loss:  1.0801 lr: 0.000992 elapse: 0.697s\n",
      "2021-11-19 18:09:14,201-INFO: epoch:34  train step:90   loss:  0.9369 lr: 0.000992 elapse: 0.709s\n",
      "2021-11-19 18:09:21,302-INFO: epoch:34  train step:100  loss:  0.8371 lr: 0.000992 elapse: 0.710s\n",
      "2021-11-19 18:09:28,402-INFO: epoch:34  train step:110  loss:  1.5738 lr: 0.000992 elapse: 0.695s\n",
      "2021-11-19 18:09:35,501-INFO: epoch:34  train step:120  loss:  0.8677 lr: 0.000992 elapse: 0.694s\n",
      "2021-11-19 18:09:42,702-INFO: epoch:34  train step:130  loss:  2.7097 lr: 0.000992 elapse: 0.698s\n",
      "2021-11-19 18:09:49,802-INFO: epoch:34  train step:140  loss:  1.0681 lr: 0.000992 elapse: 0.697s\n",
      "2021-11-19 18:09:57,291-INFO: epoch:34  train step:150  loss:  1.0447 lr: 0.000992 elapse: 0.786s\n",
      "2021-11-19 18:10:04,503-INFO: epoch:34  train step:160  loss:  0.8753 lr: 0.000992 elapse: 0.707s\n",
      "2021-11-19 18:10:11,623-INFO: epoch:34  train step:170  loss:  1.8267 lr: 0.000992 elapse: 0.712s\n",
      "2021-11-19 18:10:18,691-INFO: epoch:34  train step:180  loss:  2.1217 lr: 0.000992 elapse: 0.763s\n",
      "2021-11-19 18:10:25,703-INFO: epoch:34  train step:190  loss:  1.1537 lr: 0.000992 elapse: 0.698s\n",
      "2021-11-19 18:10:32,637-INFO: epoch:34  train step:200  loss:  1.0475 lr: 0.000992 elapse: 0.678s\n",
      "2021-11-19 18:10:39,313-INFO: epoch:34  train step:210  loss:  0.8652 lr: 0.000992 elapse: 0.666s\n",
      "2021-11-19 18:10:46,191-INFO: epoch:34  train step:220  loss:  0.9554 lr: 0.000992 elapse: 0.675s\n",
      "2021-11-19 18:10:53,164-INFO: epoch:34  train step:230  loss:  1.1833 lr: 0.000992 elapse: 0.802s\n",
      "2021-11-19 18:10:59,268-INFO: END epoch:34  train loss_avg:  1.4082  elapse_sum: 178.746s\n",
      "2021-11-19 18:11:07,393-INFO: epoch:35  train step:0    loss:  0.8921 lr: 0.000992 elapse: 8.122s\n",
      "2021-11-19 18:11:16,302-INFO: epoch:35  train step:10   loss:  2.5052 lr: 0.000992 elapse: 0.700s\n",
      "2021-11-19 18:11:23,520-INFO: epoch:35  train step:20   loss:  0.9359 lr: 0.000992 elapse: 0.718s\n",
      "2021-11-19 18:11:30,701-INFO: epoch:35  train step:30   loss:  1.0783 lr: 0.000992 elapse: 0.695s\n",
      "2021-11-19 18:11:37,891-INFO: epoch:35  train step:40   loss:  2.4515 lr: 0.000992 elapse: 0.699s\n",
      "2021-11-19 18:11:44,993-INFO: epoch:35  train step:50   loss:  1.2461 lr: 0.000992 elapse: 0.701s\n",
      "2021-11-19 18:11:52,227-INFO: epoch:35  train step:60   loss:  2.1754 lr: 0.000992 elapse: 0.735s\n",
      "2021-11-19 18:11:59,402-INFO: epoch:35  train step:70   loss:  2.0066 lr: 0.000992 elapse: 0.710s\n",
      "2021-11-19 18:12:06,503-INFO: epoch:35  train step:80   loss:  0.9304 lr: 0.000992 elapse: 0.710s\n",
      "2021-11-19 18:12:13,919-INFO: epoch:35  train step:90   loss:  1.3974 lr: 0.000992 elapse: 0.716s\n",
      "2021-11-19 18:12:21,120-INFO: epoch:35  train step:100  loss:  1.0088 lr: 0.000991 elapse: 0.717s\n",
      "2021-11-19 18:12:28,292-INFO: epoch:35  train step:110  loss:  0.8471 lr: 0.000991 elapse: 0.768s\n",
      "2021-11-19 18:12:35,305-INFO: epoch:35  train step:120  loss:  0.9877 lr: 0.000991 elapse: 0.701s\n",
      "2021-11-19 18:12:42,402-INFO: epoch:35  train step:130  loss:  2.2121 lr: 0.000991 elapse: 0.681s\n",
      "2021-11-19 18:12:49,502-INFO: epoch:35  train step:140  loss:  1.7876 lr: 0.000991 elapse: 0.696s\n",
      "2021-11-19 18:12:56,602-INFO: epoch:35  train step:150  loss:  0.8377 lr: 0.000991 elapse: 0.695s\n",
      "2021-11-19 18:13:03,606-INFO: epoch:35  train step:160  loss:  1.2716 lr: 0.000991 elapse: 0.702s\n",
      "2021-11-19 18:13:10,724-INFO: epoch:35  train step:170  loss:  0.9864 lr: 0.000991 elapse: 0.718s\n",
      "2021-11-19 18:13:18,137-INFO: epoch:35  train step:180  loss:  0.9067 lr: 0.000991 elapse: 0.720s\n",
      "2021-11-19 18:13:25,202-INFO: epoch:35  train step:190  loss:  1.0178 lr: 0.000991 elapse: 0.699s\n",
      "2021-11-19 18:13:32,141-INFO: epoch:35  train step:200  loss:  0.8612 lr: 0.000991 elapse: 0.679s\n",
      "2021-11-19 18:13:38,814-INFO: epoch:35  train step:210  loss:  0.8948 lr: 0.000991 elapse: 0.667s\n",
      "2021-11-19 18:13:45,775-INFO: epoch:35  train step:220  loss:  1.2168 lr: 0.000991 elapse: 0.666s\n",
      "2021-11-19 18:13:52,742-INFO: epoch:35  train step:230  loss:  0.9803 lr: 0.000991 elapse: 0.732s\n",
      "2021-11-19 18:13:58,840-INFO: END epoch:35  train loss_avg:  1.3056  elapse_sum: 179.569s\n",
      "2021-11-19 18:14:06,792-INFO: epoch:36  train step:0    loss:  0.8950 lr: 0.000991 elapse: 7.951s\n",
      "2021-11-19 18:14:15,591-INFO: epoch:36  train step:10   loss:  1.1866 lr: 0.000991 elapse: 0.787s\n",
      "2021-11-19 18:14:22,891-INFO: epoch:36  train step:20   loss:  0.9135 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:14:29,901-INFO: epoch:36  train step:30   loss:  1.2699 lr: 0.000991 elapse: 0.696s\n",
      "2021-11-19 18:14:36,904-INFO: epoch:36  train step:40   loss:  0.8871 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:14:43,996-INFO: epoch:36  train step:50   loss:  0.9827 lr: 0.000991 elapse: 0.750s\n",
      "2021-11-19 18:14:51,092-INFO: epoch:36  train step:60   loss:  1.2750 lr: 0.000991 elapse: 0.786s\n",
      "2021-11-19 18:14:58,302-INFO: epoch:36  train step:70   loss:  1.2931 lr: 0.000991 elapse: 0.700s\n",
      "2021-11-19 18:15:05,402-INFO: epoch:36  train step:80   loss:  1.6630 lr: 0.000991 elapse: 0.755s\n",
      "2021-11-19 18:15:12,503-INFO: epoch:36  train step:90   loss:  2.0601 lr: 0.000991 elapse: 0.696s\n",
      "2021-11-19 18:15:19,691-INFO: epoch:36  train step:100  loss:  0.9229 lr: 0.000991 elapse: 0.779s\n",
      "2021-11-19 18:15:27,001-INFO: epoch:36  train step:110  loss:  0.9602 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:15:34,103-INFO: epoch:36  train step:120  loss:  2.0882 lr: 0.000991 elapse: 0.700s\n",
      "2021-11-19 18:15:41,205-INFO: epoch:36  train step:130  loss:  1.9509 lr: 0.000991 elapse: 0.701s\n",
      "2021-11-19 18:15:48,203-INFO: epoch:36  train step:140  loss:  0.9283 lr: 0.000991 elapse: 0.699s\n",
      "2021-11-19 18:15:55,602-INFO: epoch:36  train step:150  loss:  0.9851 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:16:02,803-INFO: epoch:36  train step:160  loss:  2.2966 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:16:09,920-INFO: epoch:36  train step:170  loss:  1.7273 lr: 0.000991 elapse: 0.714s\n",
      "2021-11-19 18:16:17,002-INFO: epoch:36  train step:180  loss:  1.1785 lr: 0.000991 elapse: 0.699s\n",
      "2021-11-19 18:16:24,149-INFO: epoch:36  train step:190  loss:  2.4272 lr: 0.000991 elapse: 0.744s\n",
      "2021-11-19 18:16:31,438-INFO: epoch:36  train step:200  loss:  0.8376 lr: 0.000991 elapse: 0.677s\n",
      "2021-11-19 18:16:38,180-INFO: epoch:36  train step:210  loss:  1.0003 lr: 0.000991 elapse: 0.667s\n",
      "2021-11-19 18:16:45,126-INFO: epoch:36  train step:220  loss:  2.2579 lr: 0.000991 elapse: 0.667s\n",
      "2021-11-19 18:16:52,171-INFO: epoch:36  train step:230  loss:  0.9103 lr: 0.000991 elapse: 0.733s\n",
      "2021-11-19 18:16:58,306-INFO: END epoch:36  train loss_avg:  1.3659  elapse_sum: 179.463s\n",
      "2021-11-19 18:17:06,019-INFO: epoch:37  train step:0    loss:  0.8849 lr: 0.000991 elapse: 7.711s\n",
      "2021-11-19 18:17:15,002-INFO: epoch:37  train step:10   loss:  0.8212 lr: 0.000991 elapse: 0.698s\n",
      "2021-11-19 18:17:22,105-INFO: epoch:37  train step:20   loss:  2.5877 lr: 0.000990 elapse: 0.702s\n",
      "2021-11-19 18:17:29,237-INFO: epoch:37  train step:30   loss:  0.9794 lr: 0.000990 elapse: 0.746s\n",
      "2021-11-19 18:17:36,703-INFO: epoch:37  train step:40   loss:  0.9423 lr: 0.000990 elapse: 0.701s\n",
      "2021-11-19 18:17:43,891-INFO: epoch:37  train step:50   loss:  1.0424 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:17:50,938-INFO: epoch:37  train step:60   loss:  0.9408 lr: 0.000990 elapse: 0.743s\n",
      "2021-11-19 18:17:57,991-INFO: epoch:37  train step:70   loss:  0.8052 lr: 0.000990 elapse: 0.688s\n",
      "2021-11-19 18:18:05,191-INFO: epoch:37  train step:80   loss:  1.1408 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:18:12,202-INFO: epoch:37  train step:90   loss:  1.4321 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:18:19,302-INFO: epoch:37  train step:100  loss:  1.3025 lr: 0.000990 elapse: 0.694s\n",
      "2021-11-19 18:18:26,402-INFO: epoch:37  train step:110  loss:  1.5184 lr: 0.000990 elapse: 0.697s\n",
      "2021-11-19 18:18:33,561-INFO: epoch:37  train step:120  loss:  2.1040 lr: 0.000990 elapse: 0.756s\n",
      "2021-11-19 18:18:40,906-INFO: epoch:37  train step:130  loss:  2.1020 lr: 0.000990 elapse: 0.703s\n",
      "2021-11-19 18:18:47,902-INFO: epoch:37  train step:140  loss:  0.9566 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:18:55,103-INFO: epoch:37  train step:150  loss:  0.8603 lr: 0.000990 elapse: 0.711s\n",
      "2021-11-19 18:19:02,144-INFO: epoch:37  train step:160  loss:  1.4499 lr: 0.000990 elapse: 0.740s\n",
      "2021-11-19 18:19:09,301-INFO: epoch:37  train step:170  loss:  0.9858 lr: 0.000990 elapse: 0.697s\n",
      "2021-11-19 18:19:16,393-INFO: epoch:37  train step:180  loss:  1.3127 lr: 0.000990 elapse: 0.701s\n",
      "2021-11-19 18:19:23,491-INFO: epoch:37  train step:190  loss:  0.8620 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:19:30,402-INFO: epoch:37  train step:200  loss:  2.3794 lr: 0.000990 elapse: 0.667s\n",
      "2021-11-19 18:19:37,097-INFO: epoch:37  train step:210  loss:  1.3138 lr: 0.000990 elapse: 0.666s\n",
      "2021-11-19 18:19:44,108-INFO: epoch:37  train step:220  loss:  1.1358 lr: 0.000990 elapse: 0.666s\n",
      "2021-11-19 18:19:51,162-INFO: epoch:37  train step:230  loss:  2.2117 lr: 0.000990 elapse: 0.721s\n",
      "2021-11-19 18:19:57,328-INFO: END epoch:37  train loss_avg:  1.3144  elapse_sum: 179.019s\n",
      "2021-11-19 18:20:05,492-INFO: epoch:38  train step:0    loss:  1.7622 lr: 0.000990 elapse: 8.162s\n",
      "2021-11-19 18:20:14,402-INFO: epoch:38  train step:10   loss:  2.4580 lr: 0.000990 elapse: 0.710s\n",
      "2021-11-19 18:20:21,491-INFO: epoch:38  train step:20   loss:  1.1167 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:20:28,502-INFO: epoch:38  train step:30   loss:  0.9774 lr: 0.000990 elapse: 0.698s\n",
      "2021-11-19 18:20:35,692-INFO: epoch:38  train step:40   loss:  1.1626 lr: 0.000990 elapse: 0.700s\n",
      "2021-11-19 18:20:42,845-INFO: epoch:38  train step:50   loss:  2.3018 lr: 0.000990 elapse: 0.743s\n",
      "2021-11-19 18:20:50,003-INFO: epoch:38  train step:60   loss:  1.2451 lr: 0.000990 elapse: 0.698s\n",
      "2021-11-19 18:20:57,202-INFO: epoch:38  train step:70   loss:  1.0497 lr: 0.000990 elapse: 0.695s\n",
      "2021-11-19 18:21:04,293-INFO: epoch:38  train step:80   loss:  1.2762 lr: 0.000990 elapse: 0.701s\n",
      "2021-11-19 18:21:11,323-INFO: epoch:38  train step:90   loss:  0.8343 lr: 0.000990 elapse: 0.718s\n",
      "2021-11-19 18:21:18,503-INFO: epoch:38  train step:100  loss:  0.9647 lr: 0.000990 elapse: 0.711s\n",
      "2021-11-19 18:21:25,607-INFO: epoch:38  train step:110  loss:  1.6556 lr: 0.000990 elapse: 0.703s\n",
      "2021-11-19 18:21:32,802-INFO: epoch:38  train step:120  loss:  1.0510 lr: 0.000990 elapse: 0.695s\n",
      "2021-11-19 18:21:39,991-INFO: epoch:38  train step:130  loss:  1.6560 lr: 0.000990 elapse: 0.699s\n",
      "2021-11-19 18:21:47,146-INFO: epoch:38  train step:140  loss:  0.9641 lr: 0.000990 elapse: 0.743s\n",
      "2021-11-19 18:21:54,601-INFO: epoch:38  train step:150  loss:  0.9033 lr: 0.000990 elapse: 0.762s\n",
      "2021-11-19 18:22:01,691-INFO: epoch:38  train step:160  loss:  0.8683 lr: 0.000989 elapse: 0.699s\n",
      "2021-11-19 18:22:08,703-INFO: epoch:38  train step:170  loss:  2.4508 lr: 0.000989 elapse: 0.699s\n",
      "2021-11-19 18:22:15,702-INFO: epoch:38  train step:180  loss:  0.8722 lr: 0.000989 elapse: 0.699s\n",
      "2021-11-19 18:22:22,803-INFO: epoch:38  train step:190  loss:  0.9305 lr: 0.000989 elapse: 0.698s\n",
      "2021-11-19 18:22:29,839-INFO: epoch:38  train step:200  loss:  2.5716 lr: 0.000989 elapse: 0.678s\n",
      "2021-11-19 18:22:36,538-INFO: epoch:38  train step:210  loss:  0.9927 lr: 0.000989 elapse: 0.666s\n",
      "2021-11-19 18:22:43,416-INFO: epoch:38  train step:220  loss:  2.1929 lr: 0.000989 elapse: 0.676s\n",
      "2021-11-19 18:22:50,386-INFO: epoch:38  train step:230  loss:  0.8810 lr: 0.000989 elapse: 0.684s\n",
      "2021-11-19 18:22:56,633-INFO: END epoch:38  train loss_avg:  1.3275  elapse_sum: 179.302s\n",
      "2021-11-19 18:23:04,493-INFO: epoch:39  train step:0    loss:  1.8657 lr: 0.000989 elapse: 7.858s\n",
      "2021-11-19 18:23:13,391-INFO: epoch:39  train step:10   loss:  0.8215 lr: 0.000989 elapse: 0.767s\n",
      "2021-11-19 18:23:20,402-INFO: epoch:39  train step:20   loss:  2.3317 lr: 0.000989 elapse: 0.698s\n",
      "2021-11-19 18:23:27,502-INFO: epoch:39  train step:30   loss:  0.9263 lr: 0.000989 elapse: 0.697s\n",
      "2021-11-19 18:23:34,602-INFO: epoch:39  train step:40   loss:  0.9135 lr: 0.000989 elapse: 0.696s\n",
      "2021-11-19 18:23:41,695-INFO: epoch:39  train step:50   loss:  1.2552 lr: 0.000989 elapse: 0.752s\n",
      "2021-11-19 18:23:48,760-INFO: epoch:39  train step:60   loss:  1.8142 lr: 0.000989 elapse: 0.736s\n",
      "2021-11-19 18:23:56,148-INFO: epoch:39  train step:70   loss:  1.5425 lr: 0.000989 elapse: 0.744s\n",
      "2021-11-19 18:24:03,391-INFO: epoch:39  train step:80   loss:  2.0984 lr: 0.000989 elapse: 0.697s\n",
      "2021-11-19 18:24:10,527-INFO: epoch:39  train step:90   loss:  0.9317 lr: 0.000989 elapse: 0.725s\n",
      "2021-11-19 18:24:17,624-INFO: epoch:39  train step:100  loss:  2.5152 lr: 0.000989 elapse: 0.717s\n",
      "2021-11-19 18:24:24,791-INFO: epoch:39  train step:110  loss:  0.8602 lr: 0.000989 elapse: 0.765s\n",
      "2021-11-19 18:24:31,803-INFO: epoch:39  train step:120  loss:  1.0193 lr: 0.000989 elapse: 0.695s\n",
      "2021-11-19 18:24:38,901-INFO: epoch:39  train step:130  loss:  2.3952 lr: 0.000989 elapse: 0.698s\n",
      "2021-11-19 18:24:46,003-INFO: epoch:39  train step:140  loss:  1.4610 lr: 0.000989 elapse: 0.687s\n",
      "2021-11-19 18:24:53,312-INFO: epoch:39  train step:150  loss:  0.9888 lr: 0.000989 elapse: 0.708s\n",
      "2021-11-19 18:25:00,795-INFO: epoch:39  train step:160  loss:  1.1062 lr: 0.000989 elapse: 0.775s\n",
      "2021-11-19 18:25:07,902-INFO: epoch:39  train step:170  loss:  1.0254 lr: 0.000989 elapse: 0.698s\n",
      "2021-11-19 18:25:15,106-INFO: epoch:39  train step:180  loss:  1.9494 lr: 0.000989 elapse: 0.703s\n",
      "2021-11-19 18:25:22,191-INFO: epoch:39  train step:190  loss:  1.1054 lr: 0.000989 elapse: 0.699s\n",
      "2021-11-19 18:25:29,210-INFO: epoch:39  train step:200  loss:  0.8822 lr: 0.000989 elapse: 0.667s\n",
      "2021-11-19 18:25:35,936-INFO: epoch:39  train step:210  loss:  0.8721 lr: 0.000989 elapse: 0.667s\n",
      "2021-11-19 18:25:42,818-INFO: epoch:39  train step:220  loss:  2.0486 lr: 0.000989 elapse: 0.675s\n",
      "2021-11-19 18:25:49,782-INFO: epoch:39  train step:230  loss:  2.5097 lr: 0.000989 elapse: 0.682s\n",
      "2021-11-19 18:25:55,893-INFO: END epoch:39  train loss_avg:  1.3570  elapse_sum: 179.257s\n",
      "2021-11-19 18:26:03,892-INFO: epoch:40  train step:0    loss:  0.8757 lr: 0.000989 elapse: 7.998s\n",
      "2021-11-19 18:26:13,123-INFO: epoch:40  train step:10   loss:  0.8317 lr: 0.000989 elapse: 0.719s\n",
      "2021-11-19 18:26:20,403-INFO: epoch:40  train step:20   loss:  1.0567 lr: 0.000989 elapse: 0.711s\n",
      "2021-11-19 18:26:27,593-INFO: epoch:40  train step:30   loss:  1.9869 lr: 0.000989 elapse: 0.772s\n",
      "2021-11-19 18:26:34,619-INFO: epoch:40  train step:40   loss:  0.8728 lr: 0.000988 elapse: 0.713s\n",
      "2021-11-19 18:26:41,702-INFO: epoch:40  train step:50   loss:  0.9088 lr: 0.000988 elapse: 0.697s\n",
      "2021-11-19 18:26:48,802-INFO: epoch:40  train step:60   loss:  0.9971 lr: 0.000988 elapse: 0.700s\n",
      "2021-11-19 18:26:55,991-INFO: epoch:40  train step:70   loss:  2.1655 lr: 0.000988 elapse: 0.697s\n",
      "2021-11-19 18:27:03,014-INFO: epoch:40  train step:80   loss:  1.0316 lr: 0.000988 elapse: 0.711s\n",
      "2021-11-19 18:27:10,291-INFO: epoch:40  train step:90   loss:  0.9430 lr: 0.000988 elapse: 0.787s\n",
      "2021-11-19 18:27:17,605-INFO: epoch:40  train step:100  loss:  2.3552 lr: 0.000988 elapse: 0.701s\n",
      "2021-11-19 18:27:24,691-INFO: epoch:40  train step:110  loss:  1.9034 lr: 0.000988 elapse: 0.698s\n",
      "2021-11-19 18:27:31,791-INFO: epoch:40  train step:120  loss:  1.0813 lr: 0.000988 elapse: 0.698s\n",
      "2021-11-19 18:27:38,893-INFO: epoch:40  train step:130  loss:  1.4042 lr: 0.000988 elapse: 0.767s\n",
      "2021-11-19 18:27:45,991-INFO: epoch:40  train step:140  loss:  0.8773 lr: 0.000988 elapse: 0.693s\n",
      "2021-11-19 18:27:53,102-INFO: epoch:40  train step:150  loss:  0.9392 lr: 0.000988 elapse: 0.699s\n",
      "2021-11-19 18:28:00,102-INFO: epoch:40  train step:160  loss:  1.0176 lr: 0.000988 elapse: 0.699s\n",
      "2021-11-19 18:28:07,202-INFO: epoch:40  train step:170  loss:  1.5359 lr: 0.000988 elapse: 0.699s\n",
      "2021-11-19 18:28:14,345-INFO: epoch:40  train step:180  loss:  0.8056 lr: 0.000988 elapse: 0.740s\n",
      "2021-11-19 18:28:21,791-INFO: epoch:40  train step:190  loss:  1.6121 lr: 0.000988 elapse: 0.764s\n",
      "2021-11-19 18:28:28,744-INFO: epoch:40  train step:200  loss:  0.9400 lr: 0.000988 elapse: 0.676s\n",
      "2021-11-19 18:28:35,450-INFO: epoch:40  train step:210  loss:  0.8722 lr: 0.000988 elapse: 0.668s\n",
      "2021-11-19 18:28:42,443-INFO: epoch:40  train step:220  loss:  1.0369 lr: 0.000988 elapse: 0.666s\n",
      "2021-11-19 18:28:49,408-INFO: epoch:40  train step:230  loss:  1.0959 lr: 0.000988 elapse: 0.668s\n",
      "2021-11-19 18:28:55,488-INFO: END epoch:40  train loss_avg:  1.3077  elapse_sum: 179.592s\n",
      "2021-11-19 18:29:00,123-INFO: epoch:40  valid step:0    loss:  1.1193 top1: 0.8750 top5: 1.0000 elapse: 4.634s\n",
      "2021-11-19 18:29:05,283-INFO: epoch:40  valid step:10   loss:  1.7086 top1: 0.7500 top5: 0.8125 elapse: 0.203s\n",
      "2021-11-19 18:29:07,555-INFO: epoch:40  valid step:20   loss:  1.2531 top1: 0.8125 top5: 1.0000 elapse: 0.232s\n",
      "2021-11-19 18:29:09,666-INFO: epoch:40  valid step:30   loss:  1.4972 top1: 0.6250 top5: 0.9375 elapse: 0.190s\n",
      "2021-11-19 18:29:11,570-INFO: epoch:40  valid step:40   loss:  1.3335 top1: 0.8125 top5: 0.9375 elapse: 0.188s\n",
      "2021-11-19 18:29:13,461-INFO: epoch:40  valid step:50   loss:  1.2207 top1: 0.8750 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 18:29:15,448-INFO: epoch:40  valid step:60   loss:  1.2689 top1: 0.9375 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 18:29:17,434-INFO: epoch:40  valid step:70   loss:  1.2253 top1: 0.8125 top5: 1.0000 elapse: 0.188s\n",
      "2021-11-19 18:29:19,441-INFO: epoch:40  valid step:80   loss:  1.3722 top1: 0.7500 top5: 0.9375 elapse: 0.190s\n",
      "2021-11-19 18:29:21,419-INFO: END epoch:40  valid loss_avg:  1.2301 top1_avg: 0.8590 top5_avg: 0.9604 elapse_sum: 25.930s\n",
      "2021-11-19 18:29:22,479-INFO: Already save model in ./output/EfficientNetB3/40\n",
      "2021-11-19 18:29:30,495-INFO: epoch:41  train step:0    loss:  0.9302 lr: 0.000988 elapse: 8.015s\n",
      "2021-11-19 18:29:39,191-INFO: epoch:41  train step:10   loss:  0.9067 lr: 0.000988 elapse: 0.779s\n",
      "2021-11-19 18:29:46,303-INFO: epoch:41  train step:20   loss:  0.9402 lr: 0.000988 elapse: 0.700s\n",
      "2021-11-19 18:29:53,491-INFO: epoch:41  train step:30   loss:  0.9560 lr: 0.000988 elapse: 0.767s\n",
      "2021-11-19 18:30:00,523-INFO: epoch:41  train step:40   loss:  2.0002 lr: 0.000988 elapse: 0.717s\n",
      "2021-11-19 18:30:07,726-INFO: epoch:41  train step:50   loss:  2.3472 lr: 0.000988 elapse: 0.734s\n",
      "2021-11-19 18:30:14,702-INFO: epoch:41  train step:60   loss:  1.0299 lr: 0.000988 elapse: 0.697s\n",
      "2021-11-19 18:30:21,891-INFO: epoch:41  train step:70   loss:  2.6149 lr: 0.000988 elapse: 0.786s\n",
      "2021-11-19 18:30:29,303-INFO: epoch:41  train step:80   loss:  0.8664 lr: 0.000988 elapse: 0.709s\n",
      "2021-11-19 18:30:36,491-INFO: epoch:41  train step:90   loss:  0.9209 lr: 0.000988 elapse: 0.699s\n",
      "2021-11-19 18:30:43,502-INFO: epoch:41  train step:100  loss:  0.9504 lr: 0.000988 elapse: 0.698s\n",
      "2021-11-19 18:30:50,522-INFO: epoch:41  train step:110  loss:  1.4239 lr: 0.000988 elapse: 0.720s\n",
      "2021-11-19 18:30:57,702-INFO: epoch:41  train step:120  loss:  0.8221 lr: 0.000988 elapse: 0.698s\n",
      "2021-11-19 18:31:04,803-INFO: epoch:41  train step:130  loss:  0.9106 lr: 0.000988 elapse: 0.699s\n",
      "2021-11-19 18:31:11,902-INFO: epoch:41  train step:140  loss:  1.0370 lr: 0.000988 elapse: 0.709s\n",
      "2021-11-19 18:31:18,904-INFO: epoch:41  train step:150  loss:  0.8416 lr: 0.000987 elapse: 0.698s\n",
      "2021-11-19 18:31:26,119-INFO: epoch:41  train step:160  loss:  0.8632 lr: 0.000987 elapse: 0.714s\n",
      "2021-11-19 18:31:33,591-INFO: epoch:41  train step:170  loss:  2.1797 lr: 0.000987 elapse: 0.697s\n",
      "2021-11-19 18:31:40,601-INFO: epoch:41  train step:180  loss:  2.0614 lr: 0.000987 elapse: 0.698s\n",
      "2021-11-19 18:31:47,803-INFO: epoch:41  train step:190  loss:  0.9140 lr: 0.000987 elapse: 0.699s\n",
      "2021-11-19 18:31:54,950-INFO: epoch:41  train step:200  loss:  0.8583 lr: 0.000987 elapse: 0.675s\n",
      "2021-11-19 18:32:01,675-INFO: epoch:41  train step:210  loss:  2.5332 lr: 0.000987 elapse: 0.668s\n",
      "2021-11-19 18:32:08,603-INFO: epoch:41  train step:220  loss:  1.1071 lr: 0.000987 elapse: 0.684s\n",
      "2021-11-19 18:32:15,512-INFO: epoch:41  train step:230  loss:  1.7007 lr: 0.000987 elapse: 0.787s\n",
      "2021-11-19 18:32:21,590-INFO: END epoch:41  train loss_avg:  1.2733  elapse_sum: 179.109s\n",
      "2021-11-19 18:32:29,692-INFO: epoch:42  train step:0    loss:  0.8576 lr: 0.000987 elapse: 8.100s\n",
      "2021-11-19 18:32:38,608-INFO: epoch:42  train step:10   loss:  2.3534 lr: 0.000987 elapse: 0.702s\n",
      "2021-11-19 18:32:45,725-INFO: epoch:42  train step:20   loss:  0.9649 lr: 0.000987 elapse: 0.721s\n",
      "2021-11-19 18:32:53,002-INFO: epoch:42  train step:30   loss:  2.5731 lr: 0.000987 elapse: 0.699s\n",
      "2021-11-19 18:33:00,202-INFO: epoch:42  train step:40   loss:  0.8964 lr: 0.000987 elapse: 0.700s\n",
      "2021-11-19 18:33:07,305-INFO: epoch:42  train step:50   loss:  0.8885 lr: 0.000987 elapse: 0.702s\n",
      "2021-11-19 18:33:14,417-INFO: epoch:42  train step:60   loss:  1.3985 lr: 0.000987 elapse: 0.711s\n",
      "2021-11-19 18:33:21,503-INFO: epoch:42  train step:70   loss:  0.8265 lr: 0.000987 elapse: 0.684s\n",
      "2021-11-19 18:33:28,502-INFO: epoch:42  train step:80   loss:  0.7947 lr: 0.000987 elapse: 0.700s\n",
      "2021-11-19 18:33:35,605-INFO: epoch:42  train step:90   loss:  0.9115 lr: 0.000987 elapse: 0.713s\n",
      "2021-11-19 18:33:42,902-INFO: epoch:42  train step:100  loss:  1.2455 lr: 0.000987 elapse: 0.698s\n",
      "2021-11-19 18:33:50,004-INFO: epoch:42  train step:110  loss:  0.9863 lr: 0.000987 elapse: 0.709s\n",
      "2021-11-19 18:33:57,102-INFO: epoch:42  train step:120  loss:  2.3937 lr: 0.000987 elapse: 0.699s\n",
      "2021-11-19 18:34:04,105-INFO: epoch:42  train step:130  loss:  0.9271 lr: 0.000987 elapse: 0.701s\n",
      "2021-11-19 18:34:11,201-INFO: epoch:42  train step:140  loss:  0.8904 lr: 0.000987 elapse: 0.699s\n",
      "2021-11-19 18:34:18,391-INFO: epoch:42  train step:150  loss:  1.0133 lr: 0.000987 elapse: 0.698s\n",
      "2021-11-19 18:34:25,493-INFO: epoch:42  train step:160  loss:  1.0546 lr: 0.000987 elapse: 0.702s\n",
      "2021-11-19 18:34:32,491-INFO: epoch:42  train step:170  loss:  1.0573 lr: 0.000987 elapse: 0.698s\n",
      "2021-11-19 18:34:39,504-INFO: epoch:42  train step:180  loss:  1.0562 lr: 0.000987 elapse: 0.700s\n",
      "2021-11-19 18:34:46,907-INFO: epoch:42  train step:190  loss:  2.6046 lr: 0.000987 elapse: 0.704s\n",
      "2021-11-19 18:34:53,915-INFO: epoch:42  train step:200  loss:  0.8766 lr: 0.000987 elapse: 0.676s\n",
      "2021-11-19 18:35:00,603-INFO: epoch:42  train step:210  loss:  2.1417 lr: 0.000987 elapse: 0.668s\n",
      "2021-11-19 18:35:07,551-INFO: epoch:42  train step:220  loss:  0.9540 lr: 0.000987 elapse: 0.675s\n",
      "2021-11-19 18:35:14,536-INFO: epoch:42  train step:230  loss:  1.0751 lr: 0.000987 elapse: 0.678s\n",
      "2021-11-19 18:35:20,585-INFO: END epoch:42  train loss_avg:  1.3562  elapse_sum: 178.992s\n",
      "2021-11-19 18:35:28,493-INFO: epoch:43  train step:0    loss:  0.8899 lr: 0.000986 elapse: 7.906s\n",
      "2021-11-19 18:35:37,302-INFO: epoch:43  train step:10   loss:  0.9095 lr: 0.000986 elapse: 0.699s\n",
      "2021-11-19 18:35:44,435-INFO: epoch:43  train step:20   loss:  2.2468 lr: 0.000986 elapse: 0.731s\n",
      "2021-11-19 18:35:51,901-INFO: epoch:43  train step:30   loss:  0.9016 lr: 0.000986 elapse: 0.755s\n",
      "2021-11-19 18:35:58,991-INFO: epoch:43  train step:40   loss:  2.3735 lr: 0.000986 elapse: 0.698s\n",
      "2021-11-19 18:36:06,104-INFO: epoch:43  train step:50   loss:  0.9548 lr: 0.000986 elapse: 0.714s\n",
      "2021-11-19 18:36:13,202-INFO: epoch:43  train step:60   loss:  0.9682 lr: 0.000986 elapse: 0.697s\n",
      "2021-11-19 18:36:20,336-INFO: epoch:43  train step:70   loss:  1.8443 lr: 0.000986 elapse: 0.729s\n",
      "2021-11-19 18:36:27,580-INFO: epoch:43  train step:80   loss:  1.0318 lr: 0.000986 elapse: 0.699s\n",
      "2021-11-19 18:36:34,791-INFO: epoch:43  train step:90   loss:  2.5114 lr: 0.000986 elapse: 0.757s\n",
      "2021-11-19 18:36:41,919-INFO: epoch:43  train step:100  loss:  0.9578 lr: 0.000986 elapse: 0.712s\n",
      "2021-11-19 18:36:49,102-INFO: epoch:43  train step:110  loss:  0.8838 lr: 0.000986 elapse: 0.783s\n",
      "2021-11-19 18:36:56,603-INFO: epoch:43  train step:120  loss:  1.5938 lr: 0.000986 elapse: 0.701s\n",
      "2021-11-19 18:37:03,705-INFO: epoch:43  train step:130  loss:  1.2274 lr: 0.000986 elapse: 0.702s\n",
      "2021-11-19 18:37:10,802-INFO: epoch:43  train step:140  loss:  2.1282 lr: 0.000986 elapse: 0.697s\n",
      "2021-11-19 18:37:17,891-INFO: epoch:43  train step:150  loss:  0.8569 lr: 0.000986 elapse: 0.700s\n",
      "2021-11-19 18:37:25,002-INFO: epoch:43  train step:160  loss:  0.9036 lr: 0.000986 elapse: 0.700s\n",
      "2021-11-19 18:37:32,091-INFO: epoch:43  train step:170  loss:  1.5023 lr: 0.000986 elapse: 0.698s\n",
      "2021-11-19 18:37:39,191-INFO: epoch:43  train step:180  loss:  0.8241 lr: 0.000986 elapse: 0.699s\n",
      "2021-11-19 18:37:46,291-INFO: epoch:43  train step:190  loss:  1.0592 lr: 0.000986 elapse: 0.772s\n",
      "2021-11-19 18:37:53,413-INFO: epoch:43  train step:200  loss:  2.2551 lr: 0.000986 elapse: 0.677s\n",
      "2021-11-19 18:38:00,166-INFO: epoch:43  train step:210  loss:  0.9581 lr: 0.000986 elapse: 0.679s\n",
      "2021-11-19 18:38:07,165-INFO: epoch:43  train step:220  loss:  0.8587 lr: 0.000986 elapse: 0.675s\n",
      "2021-11-19 18:38:14,117-INFO: epoch:43  train step:230  loss:  0.8441 lr: 0.000986 elapse: 0.821s\n",
      "2021-11-19 18:38:20,203-INFO: END epoch:43  train loss_avg:  1.3137  elapse_sum: 179.615s\n",
      "2021-11-19 18:38:28,115-INFO: epoch:44  train step:0    loss:  1.1271 lr: 0.000986 elapse: 7.909s\n",
      "2021-11-19 18:38:36,901-INFO: epoch:44  train step:10   loss:  0.9859 lr: 0.000986 elapse: 0.709s\n",
      "2021-11-19 18:38:43,991-INFO: epoch:44  train step:20   loss:  0.9978 lr: 0.000986 elapse: 0.697s\n",
      "2021-11-19 18:38:51,223-INFO: epoch:44  train step:30   loss:  0.8192 lr: 0.000986 elapse: 0.759s\n",
      "2021-11-19 18:38:58,437-INFO: epoch:44  train step:40   loss:  1.1549 lr: 0.000986 elapse: 0.745s\n",
      "2021-11-19 18:39:05,792-INFO: epoch:44  train step:50   loss:  1.6237 lr: 0.000986 elapse: 0.695s\n",
      "2021-11-19 18:39:12,803-INFO: epoch:44  train step:60   loss:  1.1430 lr: 0.000986 elapse: 0.699s\n",
      "2021-11-19 18:39:19,908-INFO: epoch:44  train step:70   loss:  1.0161 lr: 0.000986 elapse: 0.704s\n",
      "2021-11-19 18:39:26,902-INFO: epoch:44  train step:80   loss:  1.2763 lr: 0.000986 elapse: 0.684s\n",
      "2021-11-19 18:39:34,002-INFO: epoch:44  train step:90   loss:  0.8129 lr: 0.000985 elapse: 0.698s\n",
      "2021-11-19 18:39:41,003-INFO: epoch:44  train step:100  loss:  2.3861 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:39:48,216-INFO: epoch:44  train step:110  loss:  1.4151 lr: 0.000985 elapse: 0.697s\n",
      "2021-11-19 18:39:55,503-INFO: epoch:44  train step:120  loss:  2.6294 lr: 0.000985 elapse: 0.711s\n",
      "2021-11-19 18:40:02,748-INFO: epoch:44  train step:130  loss:  1.0499 lr: 0.000985 elapse: 0.789s\n",
      "2021-11-19 18:40:10,191-INFO: epoch:44  train step:140  loss:  1.5164 lr: 0.000985 elapse: 0.765s\n",
      "2021-11-19 18:40:17,305-INFO: epoch:44  train step:150  loss:  1.0713 lr: 0.000985 elapse: 0.701s\n",
      "2021-11-19 18:40:24,530-INFO: epoch:44  train step:160  loss:  1.5054 lr: 0.000985 elapse: 0.724s\n",
      "2021-11-19 18:40:31,602-INFO: epoch:44  train step:170  loss:  1.0844 lr: 0.000985 elapse: 0.698s\n",
      "2021-11-19 18:40:38,603-INFO: epoch:44  train step:180  loss:  0.9884 lr: 0.000985 elapse: 0.697s\n",
      "2021-11-19 18:40:45,603-INFO: epoch:44  train step:190  loss:  2.1846 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:40:52,661-INFO: epoch:44  train step:200  loss:  0.8436 lr: 0.000985 elapse: 0.676s\n",
      "2021-11-19 18:40:59,390-INFO: epoch:44  train step:210  loss:  0.8470 lr: 0.000985 elapse: 0.667s\n",
      "2021-11-19 18:41:06,321-INFO: epoch:44  train step:220  loss:  0.9391 lr: 0.000985 elapse: 0.669s\n",
      "2021-11-19 18:41:13,216-INFO: epoch:44  train step:230  loss:  2.4866 lr: 0.000985 elapse: 0.764s\n",
      "2021-11-19 18:41:19,336-INFO: END epoch:44  train loss_avg:  1.3300  elapse_sum: 179.130s\n",
      "2021-11-19 18:41:27,392-INFO: epoch:45  train step:0    loss:  1.0750 lr: 0.000985 elapse: 8.055s\n",
      "2021-11-19 18:41:36,102-INFO: epoch:45  train step:10   loss:  2.3484 lr: 0.000985 elapse: 0.709s\n",
      "2021-11-19 18:41:43,202-INFO: epoch:45  train step:20   loss:  0.8628 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:41:50,308-INFO: epoch:45  train step:30   loss:  0.9429 lr: 0.000985 elapse: 0.706s\n",
      "2021-11-19 18:41:57,403-INFO: epoch:45  train step:40   loss:  2.4200 lr: 0.000985 elapse: 0.700s\n",
      "2021-11-19 18:42:04,492-INFO: epoch:45  train step:50   loss:  1.2558 lr: 0.000985 elapse: 0.700s\n",
      "2021-11-19 18:42:11,703-INFO: epoch:45  train step:60   loss:  1.9043 lr: 0.000985 elapse: 0.764s\n",
      "2021-11-19 18:42:18,902-INFO: epoch:45  train step:70   loss:  0.9059 lr: 0.000985 elapse: 0.698s\n",
      "2021-11-19 18:42:26,191-INFO: epoch:45  train step:80   loss:  1.1921 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:42:33,291-INFO: epoch:45  train step:90   loss:  2.2479 lr: 0.000985 elapse: 0.774s\n",
      "2021-11-19 18:42:40,306-INFO: epoch:45  train step:100  loss:  1.5495 lr: 0.000985 elapse: 0.702s\n",
      "2021-11-19 18:42:47,303-INFO: epoch:45  train step:110  loss:  1.0003 lr: 0.000985 elapse: 0.700s\n",
      "2021-11-19 18:42:54,404-INFO: epoch:45  train step:120  loss:  2.0954 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:43:01,491-INFO: epoch:45  train step:130  loss:  2.0352 lr: 0.000985 elapse: 0.730s\n",
      "2021-11-19 18:43:08,602-INFO: epoch:45  train step:140  loss:  1.3006 lr: 0.000985 elapse: 0.699s\n",
      "2021-11-19 18:43:15,902-INFO: epoch:45  train step:150  loss:  0.9630 lr: 0.000985 elapse: 0.776s\n",
      "2021-11-19 18:43:23,203-INFO: epoch:45  train step:160  loss:  1.3071 lr: 0.000984 elapse: 0.700s\n",
      "2021-11-19 18:43:30,235-INFO: epoch:45  train step:170  loss:  0.8353 lr: 0.000984 elapse: 0.732s\n",
      "2021-11-19 18:43:37,304-INFO: epoch:45  train step:180  loss:  1.1343 lr: 0.000984 elapse: 0.700s\n",
      "2021-11-19 18:43:44,391-INFO: epoch:45  train step:190  loss:  1.0177 lr: 0.000984 elapse: 0.699s\n",
      "2021-11-19 18:43:51,427-INFO: epoch:45  train step:200  loss:  1.0226 lr: 0.000984 elapse: 0.677s\n",
      "2021-11-19 18:43:58,137-INFO: epoch:45  train step:210  loss:  0.8770 lr: 0.000984 elapse: 0.667s\n",
      "2021-11-19 18:44:05,079-INFO: epoch:45  train step:220  loss:  0.9192 lr: 0.000984 elapse: 0.668s\n",
      "2021-11-19 18:44:12,044-INFO: epoch:45  train step:230  loss:  2.3731 lr: 0.000984 elapse: 0.667s\n",
      "2021-11-19 18:44:18,136-INFO: END epoch:45  train loss_avg:  1.3644  elapse_sum: 178.797s\n",
      "2021-11-19 18:44:26,510-INFO: epoch:46  train step:0    loss:  1.0701 lr: 0.000984 elapse: 8.373s\n",
      "2021-11-19 18:44:35,401-INFO: epoch:46  train step:10   loss:  0.8947 lr: 0.000984 elapse: 0.709s\n",
      "2021-11-19 18:44:42,502-INFO: epoch:46  train step:20   loss:  0.8541 lr: 0.000984 elapse: 0.700s\n",
      "2021-11-19 18:44:49,691-INFO: epoch:46  train step:30   loss:  1.3066 lr: 0.000984 elapse: 0.774s\n",
      "2021-11-19 18:44:56,906-INFO: epoch:46  train step:40   loss:  0.9263 lr: 0.000984 elapse: 0.694s\n",
      "2021-11-19 18:45:04,091-INFO: epoch:46  train step:50   loss:  1.4296 lr: 0.000984 elapse: 0.770s\n",
      "2021-11-19 18:45:11,105-INFO: epoch:46  train step:60   loss:  0.9468 lr: 0.000984 elapse: 0.702s\n",
      "2021-11-19 18:45:18,215-INFO: epoch:46  train step:70   loss:  1.0012 lr: 0.000984 elapse: 0.713s\n",
      "2021-11-19 18:45:25,303-INFO: epoch:46  train step:80   loss:  1.1538 lr: 0.000984 elapse: 0.710s\n",
      "2021-11-19 18:45:32,606-INFO: epoch:46  train step:90   loss:  0.9646 lr: 0.000984 elapse: 0.794s\n",
      "2021-11-19 18:45:39,703-INFO: epoch:46  train step:100  loss:  0.8975 lr: 0.000984 elapse: 0.695s\n",
      "2021-11-19 18:45:46,802-INFO: epoch:46  train step:110  loss:  0.9624 lr: 0.000984 elapse: 0.700s\n",
      "2021-11-19 18:45:53,991-INFO: epoch:46  train step:120  loss:  1.0353 lr: 0.000984 elapse: 0.697s\n",
      "2021-11-19 18:46:01,124-INFO: epoch:46  train step:130  loss:  2.4788 lr: 0.000984 elapse: 0.706s\n",
      "2021-11-19 18:46:08,203-INFO: epoch:46  train step:140  loss:  0.9999 lr: 0.000984 elapse: 0.682s\n",
      "2021-11-19 18:46:15,302-INFO: epoch:46  train step:150  loss:  1.0014 lr: 0.000984 elapse: 0.699s\n",
      "2021-11-19 18:46:22,412-INFO: epoch:46  train step:160  loss:  1.0318 lr: 0.000984 elapse: 0.707s\n",
      "2021-11-19 18:46:29,491-INFO: epoch:46  train step:170  loss:  2.7885 lr: 0.000984 elapse: 0.699s\n",
      "2021-11-19 18:46:36,903-INFO: epoch:46  train step:180  loss:  1.2215 lr: 0.000984 elapse: 0.778s\n",
      "2021-11-19 18:46:44,116-INFO: epoch:46  train step:190  loss:  0.8064 lr: 0.000984 elapse: 0.713s\n",
      "2021-11-19 18:46:51,037-INFO: epoch:46  train step:200  loss:  1.8402 lr: 0.000984 elapse: 0.679s\n",
      "2021-11-19 18:46:57,773-INFO: epoch:46  train step:210  loss:  1.8001 lr: 0.000984 elapse: 0.666s\n",
      "2021-11-19 18:47:04,723-INFO: epoch:46  train step:220  loss:  2.2844 lr: 0.000983 elapse: 0.667s\n",
      "2021-11-19 18:47:11,660-INFO: epoch:46  train step:230  loss:  2.3364 lr: 0.000983 elapse: 0.666s\n",
      "2021-11-19 18:47:17,686-INFO: END epoch:46  train loss_avg:  1.3195  elapse_sum: 179.547s\n",
      "2021-11-19 18:47:25,192-INFO: epoch:47  train step:0    loss:  1.3990 lr: 0.000983 elapse: 7.505s\n",
      "2021-11-19 18:47:34,291-INFO: epoch:47  train step:10   loss:  1.0712 lr: 0.000983 elapse: 0.696s\n",
      "2021-11-19 18:47:41,591-INFO: epoch:47  train step:20   loss:  2.0445 lr: 0.000983 elapse: 0.838s\n",
      "2021-11-19 18:47:48,803-INFO: epoch:47  train step:30   loss:  0.8199 lr: 0.000983 elapse: 0.689s\n",
      "2021-11-19 18:47:55,801-INFO: epoch:47  train step:40   loss:  1.8131 lr: 0.000983 elapse: 0.698s\n",
      "2021-11-19 18:48:02,902-INFO: epoch:47  train step:50   loss:  0.8968 lr: 0.000983 elapse: 0.777s\n",
      "2021-11-19 18:48:09,891-INFO: epoch:47  train step:60   loss:  2.6332 lr: 0.000983 elapse: 0.698s\n",
      "2021-11-19 18:48:16,993-INFO: epoch:47  train step:70   loss:  0.8898 lr: 0.000983 elapse: 0.690s\n",
      "2021-11-19 18:48:24,015-INFO: epoch:47  train step:80   loss:  2.4130 lr: 0.000983 elapse: 0.711s\n",
      "2021-11-19 18:48:31,144-INFO: epoch:47  train step:90   loss:  0.8595 lr: 0.000983 elapse: 0.723s\n",
      "2021-11-19 18:48:38,291-INFO: epoch:47  train step:100  loss:  1.6926 lr: 0.000983 elapse: 0.697s\n",
      "2021-11-19 18:48:45,516-INFO: epoch:47  train step:110  loss:  0.8226 lr: 0.000983 elapse: 0.766s\n",
      "2021-11-19 18:48:52,903-INFO: epoch:47  train step:120  loss:  0.8924 lr: 0.000983 elapse: 0.700s\n",
      "2021-11-19 18:48:59,991-INFO: epoch:47  train step:130  loss:  1.3866 lr: 0.000983 elapse: 0.699s\n",
      "2021-11-19 18:49:07,101-INFO: epoch:47  train step:140  loss:  0.9411 lr: 0.000983 elapse: 0.698s\n",
      "2021-11-19 18:49:14,101-INFO: epoch:47  train step:150  loss:  2.4099 lr: 0.000983 elapse: 0.697s\n",
      "2021-11-19 18:49:21,224-INFO: epoch:47  train step:160  loss:  0.8564 lr: 0.000983 elapse: 0.715s\n",
      "2021-11-19 18:49:28,302-INFO: epoch:47  train step:170  loss:  0.8602 lr: 0.000983 elapse: 0.699s\n",
      "2021-11-19 18:49:35,408-INFO: epoch:47  train step:180  loss:  0.8716 lr: 0.000983 elapse: 0.703s\n",
      "2021-11-19 18:49:42,421-INFO: epoch:47  train step:190  loss:  0.9615 lr: 0.000983 elapse: 0.705s\n",
      "2021-11-19 18:49:49,471-INFO: epoch:47  train step:200  loss:  0.8627 lr: 0.000983 elapse: 0.678s\n",
      "2021-11-19 18:49:56,210-INFO: epoch:47  train step:210  loss:  1.0229 lr: 0.000983 elapse: 0.673s\n",
      "2021-11-19 18:50:03,051-INFO: epoch:47  train step:220  loss:  1.8329 lr: 0.000983 elapse: 0.669s\n",
      "2021-11-19 18:50:09,979-INFO: epoch:47  train step:230  loss:  1.2695 lr: 0.000983 elapse: 0.676s\n",
      "2021-11-19 18:50:16,263-INFO: END epoch:47  train loss_avg:  1.3823  elapse_sum: 178.574s\n",
      "2021-11-19 18:50:24,022-INFO: epoch:48  train step:0    loss:  1.0436 lr: 0.000983 elapse: 7.757s\n",
      "2021-11-19 18:50:32,906-INFO: epoch:48  train step:10   loss:  0.8054 lr: 0.000983 elapse: 0.699s\n",
      "2021-11-19 18:50:40,002-INFO: epoch:48  train step:20   loss:  0.8676 lr: 0.000983 elapse: 0.708s\n",
      "2021-11-19 18:50:47,102-INFO: epoch:48  train step:30   loss:  2.2597 lr: 0.000982 elapse: 0.709s\n",
      "2021-11-19 18:50:54,348-INFO: epoch:48  train step:40   loss:  0.9695 lr: 0.000982 elapse: 0.745s\n",
      "2021-11-19 18:51:01,604-INFO: epoch:48  train step:50   loss:  1.1571 lr: 0.000982 elapse: 0.701s\n",
      "2021-11-19 18:51:08,791-INFO: epoch:48  train step:60   loss:  0.8775 lr: 0.000982 elapse: 0.766s\n",
      "2021-11-19 18:51:15,935-INFO: epoch:48  train step:70   loss:  2.3703 lr: 0.000982 elapse: 0.716s\n",
      "2021-11-19 18:51:23,224-INFO: epoch:48  train step:80   loss:  1.8267 lr: 0.000982 elapse: 0.721s\n",
      "2021-11-19 18:51:30,391-INFO: epoch:48  train step:90   loss:  2.4106 lr: 0.000982 elapse: 0.783s\n",
      "2021-11-19 18:51:37,602-INFO: epoch:48  train step:100  loss:  0.9752 lr: 0.000982 elapse: 0.698s\n",
      "2021-11-19 18:51:44,802-INFO: epoch:48  train step:110  loss:  1.6441 lr: 0.000982 elapse: 0.698s\n",
      "2021-11-19 18:51:52,194-INFO: epoch:48  train step:120  loss:  1.2244 lr: 0.000982 elapse: 0.769s\n",
      "2021-11-19 18:51:59,606-INFO: epoch:48  train step:130  loss:  0.8336 lr: 0.000982 elapse: 0.780s\n",
      "2021-11-19 18:52:06,710-INFO: epoch:48  train step:140  loss:  0.8831 lr: 0.000982 elapse: 0.783s\n",
      "2021-11-19 18:52:13,791-INFO: epoch:48  train step:150  loss:  2.1302 lr: 0.000982 elapse: 0.698s\n",
      "2021-11-19 18:52:20,826-INFO: epoch:48  train step:160  loss:  1.7361 lr: 0.000982 elapse: 0.721s\n",
      "2021-11-19 18:52:27,991-INFO: epoch:48  train step:170  loss:  0.8558 lr: 0.000982 elapse: 0.767s\n",
      "2021-11-19 18:52:35,003-INFO: epoch:48  train step:180  loss:  0.8842 lr: 0.000982 elapse: 0.699s\n",
      "2021-11-19 18:52:42,191-INFO: epoch:48  train step:190  loss:  0.8255 lr: 0.000982 elapse: 0.699s\n",
      "2021-11-19 18:52:49,144-INFO: epoch:48  train step:200  loss:  2.2038 lr: 0.000982 elapse: 0.676s\n",
      "2021-11-19 18:52:55,822-INFO: epoch:48  train step:210  loss:  1.2835 lr: 0.000982 elapse: 0.669s\n",
      "2021-11-19 18:53:02,761-INFO: epoch:48  train step:220  loss:  0.9796 lr: 0.000982 elapse: 0.811s\n",
      "2021-11-19 18:53:09,723-INFO: epoch:48  train step:230  loss:  1.8571 lr: 0.000982 elapse: 0.805s\n",
      "2021-11-19 18:53:15,832-INFO: END epoch:48  train loss_avg:  1.2823  elapse_sum: 179.566s\n",
      "2021-11-19 18:53:23,507-INFO: epoch:49  train step:0    loss:  1.4914 lr: 0.000982 elapse: 7.673s\n",
      "2021-11-19 18:53:32,504-INFO: epoch:49  train step:10   loss:  1.5270 lr: 0.000982 elapse: 0.698s\n",
      "2021-11-19 18:53:39,502-INFO: epoch:49  train step:20   loss:  0.8191 lr: 0.000982 elapse: 0.698s\n",
      "2021-11-19 18:53:46,592-INFO: epoch:49  train step:30   loss:  0.8670 lr: 0.000982 elapse: 0.700s\n",
      "2021-11-19 18:53:53,705-INFO: epoch:49  train step:40   loss:  1.7332 lr: 0.000982 elapse: 0.702s\n",
      "2021-11-19 18:54:00,705-INFO: epoch:49  train step:50   loss:  2.0234 lr: 0.000982 elapse: 0.701s\n",
      "2021-11-19 18:54:08,091-INFO: epoch:49  train step:60   loss:  0.9847 lr: 0.000982 elapse: 0.799s\n",
      "2021-11-19 18:54:15,191-INFO: epoch:49  train step:70   loss:  1.0229 lr: 0.000982 elapse: 0.699s\n",
      "2021-11-19 18:54:22,205-INFO: epoch:49  train step:80   loss:  1.0669 lr: 0.000981 elapse: 0.701s\n",
      "2021-11-19 18:54:29,291-INFO: epoch:49  train step:90   loss:  0.9782 lr: 0.000981 elapse: 0.699s\n",
      "2021-11-19 18:54:36,391-INFO: epoch:49  train step:100  loss:  1.3441 lr: 0.000981 elapse: 0.699s\n",
      "2021-11-19 18:54:43,404-INFO: epoch:49  train step:110  loss:  1.8513 lr: 0.000981 elapse: 0.700s\n",
      "2021-11-19 18:54:50,565-INFO: epoch:49  train step:120  loss:  0.8563 lr: 0.000981 elapse: 0.670s\n",
      "2021-11-19 18:54:58,102-INFO: epoch:49  train step:130  loss:  2.5745 lr: 0.000981 elapse: 0.699s\n",
      "2021-11-19 18:55:05,202-INFO: epoch:49  train step:140  loss:  0.9914 lr: 0.000981 elapse: 0.697s\n",
      "2021-11-19 18:55:12,552-INFO: epoch:49  train step:150  loss:  1.1577 lr: 0.000981 elapse: 0.747s\n",
      "2021-11-19 18:55:19,791-INFO: epoch:49  train step:160  loss:  1.1048 lr: 0.000981 elapse: 0.767s\n",
      "2021-11-19 18:55:26,791-INFO: epoch:49  train step:170  loss:  1.2536 lr: 0.000981 elapse: 0.699s\n",
      "2021-11-19 18:55:33,894-INFO: epoch:49  train step:180  loss:  2.0966 lr: 0.000981 elapse: 0.690s\n",
      "2021-11-19 18:55:41,004-INFO: epoch:49  train step:190  loss:  2.1891 lr: 0.000981 elapse: 0.701s\n",
      "2021-11-19 18:55:48,013-INFO: epoch:49  train step:200  loss:  0.9878 lr: 0.000981 elapse: 0.668s\n",
      "2021-11-19 18:55:54,697-INFO: epoch:49  train step:210  loss:  1.4166 lr: 0.000981 elapse: 0.668s\n",
      "2021-11-19 18:56:01,656-INFO: epoch:49  train step:220  loss:  1.2574 lr: 0.000981 elapse: 0.668s\n",
      "2021-11-19 18:56:08,590-INFO: epoch:49  train step:230  loss:  1.0412 lr: 0.000981 elapse: 0.666s\n",
      "2021-11-19 18:56:14,596-INFO: END epoch:49  train loss_avg:  1.3420  elapse_sum: 178.761s\n",
      "2021-11-19 18:56:22,792-INFO: epoch:50  train step:0    loss:  1.2038 lr: 0.000981 elapse: 8.195s\n",
      "2021-11-19 18:56:31,491-INFO: epoch:50  train step:10   loss:  0.8350 lr: 0.000981 elapse: 0.697s\n",
      "2021-11-19 18:56:38,591-INFO: epoch:50  train step:20   loss:  2.2463 lr: 0.000981 elapse: 0.699s\n",
      "2021-11-19 18:56:45,601-INFO: epoch:50  train step:30   loss:  1.0356 lr: 0.000981 elapse: 0.698s\n",
      "2021-11-19 18:56:52,821-INFO: epoch:50  train step:40   loss:  1.0996 lr: 0.000981 elapse: 0.718s\n",
      "2021-11-19 18:57:00,003-INFO: epoch:50  train step:50   loss:  1.1033 lr: 0.000981 elapse: 0.777s\n",
      "2021-11-19 18:57:07,017-INFO: epoch:50  train step:60   loss:  0.8336 lr: 0.000981 elapse: 0.712s\n",
      "2021-11-19 18:57:14,203-INFO: epoch:50  train step:70   loss:  2.0466 lr: 0.000981 elapse: 0.711s\n",
      "2021-11-19 18:57:21,508-INFO: epoch:50  train step:80   loss:  0.8751 lr: 0.000981 elapse: 0.705s\n",
      "2021-11-19 18:57:28,604-INFO: epoch:50  train step:90   loss:  0.9099 lr: 0.000981 elapse: 0.701s\n",
      "2021-11-19 18:57:35,625-INFO: epoch:50  train step:100  loss:  0.9617 lr: 0.000981 elapse: 0.701s\n",
      "2021-11-19 18:57:42,700-INFO: epoch:50  train step:110  loss:  0.8380 lr: 0.000980 elapse: 0.706s\n",
      "2021-11-19 18:57:49,792-INFO: epoch:50  train step:120  loss:  0.9330 lr: 0.000980 elapse: 0.766s\n",
      "2021-11-19 18:57:56,903-INFO: epoch:50  train step:130  loss:  2.4283 lr: 0.000980 elapse: 0.699s\n",
      "2021-11-19 18:58:04,006-INFO: epoch:50  train step:140  loss:  0.9517 lr: 0.000980 elapse: 0.702s\n",
      "2021-11-19 18:58:11,101-INFO: epoch:50  train step:150  loss:  2.0697 lr: 0.000980 elapse: 0.710s\n",
      "2021-11-19 18:58:18,106-INFO: epoch:50  train step:160  loss:  1.0519 lr: 0.000980 elapse: 0.702s\n",
      "2021-11-19 18:58:25,422-INFO: epoch:50  train step:170  loss:  1.1466 lr: 0.000980 elapse: 0.719s\n",
      "2021-11-19 18:58:32,592-INFO: epoch:50  train step:180  loss:  0.8549 lr: 0.000980 elapse: 0.698s\n",
      "2021-11-19 18:58:39,603-INFO: epoch:50  train step:190  loss:  1.9459 lr: 0.000980 elapse: 0.699s\n",
      "2021-11-19 18:58:46,607-INFO: epoch:50  train step:200  loss:  1.1323 lr: 0.000980 elapse: 0.670s\n",
      "2021-11-19 18:58:53,288-INFO: epoch:50  train step:210  loss:  1.3809 lr: 0.000980 elapse: 0.667s\n",
      "2021-11-19 18:59:00,191-INFO: epoch:50  train step:220  loss:  1.2345 lr: 0.000980 elapse: 0.667s\n",
      "2021-11-19 18:59:07,110-INFO: epoch:50  train step:230  loss:  1.0285 lr: 0.000980 elapse: 0.666s\n",
      "2021-11-19 18:59:13,176-INFO: END epoch:50  train loss_avg:  1.2928  elapse_sum: 178.577s\n",
      "2021-11-19 18:59:17,790-INFO: epoch:50  valid step:0    loss:  1.3072 top1: 0.8125 top5: 1.0000 elapse: 4.545s\n",
      "2021-11-19 18:59:23,049-INFO: epoch:50  valid step:10   loss:  1.6361 top1: 0.7500 top5: 0.8750 elapse: 0.215s\n",
      "2021-11-19 18:59:25,562-INFO: epoch:50  valid step:20   loss:  1.3044 top1: 0.7500 top5: 0.9375 elapse: 0.289s\n",
      "2021-11-19 18:59:27,942-INFO: epoch:50  valid step:30   loss:  1.1332 top1: 0.8125 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 18:59:29,832-INFO: epoch:50  valid step:40   loss:  1.3590 top1: 0.8125 top5: 1.0000 elapse: 0.188s\n",
      "2021-11-19 18:59:31,707-INFO: epoch:50  valid step:50   loss:  1.2942 top1: 0.8750 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 18:59:33,594-INFO: epoch:50  valid step:60   loss:  1.3264 top1: 0.8750 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 18:59:35,572-INFO: epoch:50  valid step:70   loss:  1.1040 top1: 0.9375 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 18:59:37,542-INFO: epoch:50  valid step:80   loss:  1.6629 top1: 0.6250 top5: 0.8125 elapse: 0.187s\n",
      "2021-11-19 18:59:39,404-INFO: END epoch:50  valid loss_avg:  1.2352 top1_avg: 0.8521 top5_avg: 0.9590 elapse_sum: 26.227s\n",
      "2021-11-19 18:59:40,486-INFO: Already save model in ./output/EfficientNetB3/50\n",
      "2021-11-19 18:59:48,792-INFO: epoch:51  train step:0    loss:  1.1248 lr: 0.000980 elapse: 8.305s\n",
      "2021-11-19 18:59:57,702-INFO: epoch:51  train step:10   loss:  0.9296 lr: 0.000980 elapse: 0.697s\n",
      "2021-11-19 19:00:04,791-INFO: epoch:51  train step:20   loss:  0.9885 lr: 0.000980 elapse: 0.767s\n",
      "2021-11-19 19:00:11,802-INFO: epoch:51  train step:30   loss:  0.8196 lr: 0.000980 elapse: 0.698s\n",
      "2021-11-19 19:00:18,950-INFO: epoch:51  train step:40   loss:  0.8255 lr: 0.000980 elapse: 0.726s\n",
      "2021-11-19 19:00:26,101-INFO: epoch:51  train step:50   loss:  0.9484 lr: 0.000980 elapse: 0.698s\n",
      "2021-11-19 19:00:33,705-INFO: epoch:51  train step:60   loss:  0.8894 lr: 0.000980 elapse: 0.758s\n",
      "2021-11-19 19:00:41,001-INFO: epoch:51  train step:70   loss:  1.9993 lr: 0.000980 elapse: 0.755s\n",
      "2021-11-19 19:00:48,102-INFO: epoch:51  train step:80   loss:  1.7441 lr: 0.000980 elapse: 0.700s\n",
      "2021-11-19 19:00:55,203-INFO: epoch:51  train step:90   loss:  0.9073 lr: 0.000980 elapse: 0.711s\n",
      "2021-11-19 19:01:02,302-INFO: epoch:51  train step:100  loss:  0.8842 lr: 0.000980 elapse: 0.710s\n",
      "2021-11-19 19:01:09,303-INFO: epoch:51  train step:110  loss:  1.6308 lr: 0.000980 elapse: 0.700s\n",
      "2021-11-19 19:01:16,326-INFO: epoch:51  train step:120  loss:  0.9734 lr: 0.000980 elapse: 0.711s\n",
      "2021-11-19 19:01:23,502-INFO: epoch:51  train step:130  loss:  1.0795 lr: 0.000980 elapse: 0.699s\n",
      "2021-11-19 19:01:30,620-INFO: epoch:51  train step:140  loss:  2.0195 lr: 0.000979 elapse: 0.694s\n",
      "2021-11-19 19:01:38,024-INFO: epoch:51  train step:150  loss:  1.4268 lr: 0.000979 elapse: 0.717s\n",
      "2021-11-19 19:01:45,102-INFO: epoch:51  train step:160  loss:  0.8506 lr: 0.000979 elapse: 0.710s\n",
      "2021-11-19 19:01:52,324-INFO: epoch:51  train step:170  loss:  1.0796 lr: 0.000979 elapse: 0.720s\n",
      "2021-11-19 19:01:59,303-INFO: epoch:51  train step:180  loss:  2.0025 lr: 0.000979 elapse: 0.700s\n",
      "2021-11-19 19:02:06,405-INFO: epoch:51  train step:190  loss:  0.9440 lr: 0.000979 elapse: 0.701s\n",
      "2021-11-19 19:02:13,338-INFO: epoch:51  train step:200  loss:  0.8411 lr: 0.000979 elapse: 0.676s\n",
      "2021-11-19 19:02:20,013-INFO: epoch:51  train step:210  loss:  1.1404 lr: 0.000979 elapse: 0.666s\n",
      "2021-11-19 19:02:26,971-INFO: epoch:51  train step:220  loss:  1.2145 lr: 0.000979 elapse: 0.669s\n",
      "2021-11-19 19:02:33,938-INFO: epoch:51  train step:230  loss:  1.0844 lr: 0.000979 elapse: 0.669s\n",
      "2021-11-19 19:02:39,951-INFO: END epoch:51  train loss_avg:  1.2686  elapse_sum: 179.462s\n",
      "2021-11-19 19:02:47,993-INFO: epoch:52  train step:0    loss:  2.0920 lr: 0.000979 elapse: 8.041s\n",
      "2021-11-19 19:02:56,722-INFO: epoch:52  train step:10   loss:  0.9222 lr: 0.000979 elapse: 0.705s\n",
      "2021-11-19 19:03:03,803-INFO: epoch:52  train step:20   loss:  1.1150 lr: 0.000979 elapse: 0.698s\n",
      "2021-11-19 19:03:10,891-INFO: epoch:52  train step:30   loss:  2.3583 lr: 0.000979 elapse: 0.698s\n",
      "2021-11-19 19:03:18,103-INFO: epoch:52  train step:40   loss:  0.8061 lr: 0.000979 elapse: 0.712s\n",
      "2021-11-19 19:03:25,091-INFO: epoch:52  train step:50   loss:  1.9443 lr: 0.000979 elapse: 0.699s\n",
      "2021-11-19 19:03:32,302-INFO: epoch:52  train step:60   loss:  1.8768 lr: 0.000979 elapse: 0.699s\n",
      "2021-11-19 19:03:39,402-INFO: epoch:52  train step:70   loss:  1.9139 lr: 0.000979 elapse: 0.710s\n",
      "2021-11-19 19:03:46,801-INFO: epoch:52  train step:80   loss:  0.9924 lr: 0.000979 elapse: 0.738s\n",
      "2021-11-19 19:03:53,891-INFO: epoch:52  train step:90   loss:  0.8875 lr: 0.000979 elapse: 0.785s\n",
      "2021-11-19 19:04:00,992-INFO: epoch:52  train step:100  loss:  1.6091 lr: 0.000979 elapse: 0.700s\n",
      "2021-11-19 19:04:08,004-INFO: epoch:52  train step:110  loss:  1.3412 lr: 0.000979 elapse: 0.701s\n",
      "2021-11-19 19:04:15,191-INFO: epoch:52  train step:120  loss:  1.8047 lr: 0.000979 elapse: 0.699s\n",
      "2021-11-19 19:04:22,302-INFO: epoch:52  train step:130  loss:  0.9089 lr: 0.000979 elapse: 0.710s\n",
      "2021-11-19 19:04:29,303-INFO: epoch:52  train step:140  loss:  0.9375 lr: 0.000979 elapse: 0.711s\n",
      "2021-11-19 19:04:36,434-INFO: epoch:52  train step:150  loss:  0.8927 lr: 0.000979 elapse: 0.732s\n",
      "2021-11-19 19:04:43,506-INFO: epoch:52  train step:160  loss:  0.9629 lr: 0.000979 elapse: 0.700s\n",
      "2021-11-19 19:04:51,017-INFO: epoch:52  train step:170  loss:  1.7080 lr: 0.000978 elapse: 0.813s\n",
      "2021-11-19 19:04:58,191-INFO: epoch:52  train step:180  loss:  0.8984 lr: 0.000978 elapse: 0.699s\n",
      "2021-11-19 19:05:05,301-INFO: epoch:52  train step:190  loss:  0.8482 lr: 0.000978 elapse: 0.696s\n",
      "2021-11-19 19:05:12,331-INFO: epoch:52  train step:200  loss:  0.9753 lr: 0.000978 elapse: 0.670s\n",
      "2021-11-19 19:05:19,024-INFO: epoch:52  train step:210  loss:  1.0277 lr: 0.000978 elapse: 0.671s\n",
      "2021-11-19 19:05:25,995-INFO: epoch:52  train step:220  loss:  0.9690 lr: 0.000978 elapse: 0.668s\n",
      "2021-11-19 19:05:33,108-INFO: epoch:52  train step:230  loss:  1.1129 lr: 0.000978 elapse: 0.793s\n",
      "2021-11-19 19:05:39,284-INFO: END epoch:52  train loss_avg:  1.3519  elapse_sum: 179.330s\n",
      "2021-11-19 19:05:47,505-INFO: epoch:53  train step:0    loss:  0.9701 lr: 0.000978 elapse: 8.219s\n",
      "2021-11-19 19:05:56,432-INFO: epoch:53  train step:10   loss:  1.5805 lr: 0.000978 elapse: 0.729s\n",
      "2021-11-19 19:06:03,504-INFO: epoch:53  train step:20   loss:  0.8700 lr: 0.000978 elapse: 0.701s\n",
      "2021-11-19 19:06:10,603-INFO: epoch:53  train step:30   loss:  1.0038 lr: 0.000978 elapse: 0.700s\n",
      "2021-11-19 19:06:17,702-INFO: epoch:53  train step:40   loss:  2.0726 lr: 0.000978 elapse: 0.709s\n",
      "2021-11-19 19:06:24,703-INFO: epoch:53  train step:50   loss:  0.8799 lr: 0.000978 elapse: 0.699s\n",
      "2021-11-19 19:06:31,703-INFO: epoch:53  train step:60   loss:  2.1720 lr: 0.000978 elapse: 0.699s\n",
      "2021-11-19 19:06:38,791-INFO: epoch:53  train step:70   loss:  1.0684 lr: 0.000978 elapse: 0.699s\n",
      "2021-11-19 19:06:45,992-INFO: epoch:53  train step:80   loss:  1.0137 lr: 0.000978 elapse: 0.768s\n",
      "2021-11-19 19:06:53,102-INFO: epoch:53  train step:90   loss:  1.0043 lr: 0.000978 elapse: 0.700s\n",
      "2021-11-19 19:07:00,504-INFO: epoch:53  train step:100  loss:  1.5595 lr: 0.000978 elapse: 0.711s\n",
      "2021-11-19 19:07:07,702-INFO: epoch:53  train step:110  loss:  1.1435 lr: 0.000978 elapse: 0.777s\n",
      "2021-11-19 19:07:14,703-INFO: epoch:53  train step:120  loss:  1.0762 lr: 0.000978 elapse: 0.700s\n",
      "2021-11-19 19:07:21,903-INFO: epoch:53  train step:130  loss:  0.9418 lr: 0.000978 elapse: 0.767s\n",
      "2021-11-19 19:07:28,994-INFO: epoch:53  train step:140  loss:  0.8056 lr: 0.000978 elapse: 0.699s\n",
      "2021-11-19 19:07:36,003-INFO: epoch:53  train step:150  loss:  0.9063 lr: 0.000978 elapse: 0.700s\n",
      "2021-11-19 19:07:43,124-INFO: epoch:53  train step:160  loss:  2.0672 lr: 0.000978 elapse: 0.719s\n",
      "2021-11-19 19:07:50,291-INFO: epoch:53  train step:170  loss:  1.3282 lr: 0.000978 elapse: 0.766s\n",
      "2021-11-19 19:07:57,302-INFO: epoch:53  train step:180  loss:  0.8300 lr: 0.000978 elapse: 0.695s\n",
      "2021-11-19 19:08:04,791-INFO: epoch:53  train step:190  loss:  0.9475 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:08:11,729-INFO: epoch:53  train step:200  loss:  1.0496 lr: 0.000977 elapse: 0.669s\n",
      "2021-11-19 19:08:18,406-INFO: epoch:53  train step:210  loss:  0.9537 lr: 0.000977 elapse: 0.666s\n",
      "2021-11-19 19:08:25,350-INFO: epoch:53  train step:220  loss:  1.6347 lr: 0.000977 elapse: 0.666s\n",
      "2021-11-19 19:08:32,282-INFO: epoch:53  train step:230  loss:  0.9254 lr: 0.000977 elapse: 0.668s\n",
      "2021-11-19 19:08:38,262-INFO: END epoch:53  train loss_avg:  1.3301  elapse_sum: 178.975s\n",
      "2021-11-19 19:08:45,693-INFO: epoch:54  train step:0    loss:  1.1061 lr: 0.000977 elapse: 7.429s\n",
      "2021-11-19 19:08:55,003-INFO: epoch:54  train step:10   loss:  1.0023 lr: 0.000977 elapse: 0.701s\n",
      "2021-11-19 19:09:02,102-INFO: epoch:54  train step:20   loss:  1.9880 lr: 0.000977 elapse: 0.698s\n",
      "2021-11-19 19:09:09,491-INFO: epoch:54  train step:30   loss:  1.4406 lr: 0.000977 elapse: 0.697s\n",
      "2021-11-19 19:09:16,502-INFO: epoch:54  train step:40   loss:  0.8476 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:09:23,591-INFO: epoch:54  train step:50   loss:  0.9279 lr: 0.000977 elapse: 0.766s\n",
      "2021-11-19 19:09:30,602-INFO: epoch:54  train step:60   loss:  1.1246 lr: 0.000977 elapse: 0.698s\n",
      "2021-11-19 19:09:37,702-INFO: epoch:54  train step:70   loss:  1.5651 lr: 0.000977 elapse: 0.697s\n",
      "2021-11-19 19:09:44,801-INFO: epoch:54  train step:80   loss:  1.2264 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:09:51,902-INFO: epoch:54  train step:90   loss:  0.8043 lr: 0.000977 elapse: 0.754s\n",
      "2021-11-19 19:09:59,002-INFO: epoch:54  train step:100  loss:  1.0720 lr: 0.000977 elapse: 0.698s\n",
      "2021-11-19 19:10:06,191-INFO: epoch:54  train step:110  loss:  0.8623 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:10:13,538-INFO: epoch:54  train step:120  loss:  2.0302 lr: 0.000977 elapse: 0.730s\n",
      "2021-11-19 19:10:20,591-INFO: epoch:54  train step:130  loss:  0.8244 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:10:27,692-INFO: epoch:54  train step:140  loss:  0.8477 lr: 0.000977 elapse: 0.700s\n",
      "2021-11-19 19:10:34,891-INFO: epoch:54  train step:150  loss:  1.0441 lr: 0.000977 elapse: 0.698s\n",
      "2021-11-19 19:10:42,059-INFO: epoch:54  train step:160  loss:  0.9316 lr: 0.000977 elapse: 0.752s\n",
      "2021-11-19 19:10:49,403-INFO: epoch:54  train step:170  loss:  0.8848 lr: 0.000977 elapse: 0.778s\n",
      "2021-11-19 19:10:56,702-INFO: epoch:54  train step:180  loss:  1.2907 lr: 0.000977 elapse: 0.698s\n",
      "2021-11-19 19:11:03,791-INFO: epoch:54  train step:190  loss:  2.3815 lr: 0.000977 elapse: 0.699s\n",
      "2021-11-19 19:11:10,757-INFO: epoch:54  train step:200  loss:  0.8672 lr: 0.000976 elapse: 0.678s\n",
      "2021-11-19 19:11:17,495-INFO: epoch:54  train step:210  loss:  1.7574 lr: 0.000976 elapse: 0.668s\n",
      "2021-11-19 19:11:24,446-INFO: epoch:54  train step:220  loss:  0.9134 lr: 0.000976 elapse: 0.677s\n",
      "2021-11-19 19:11:31,402-INFO: epoch:54  train step:230  loss:  1.1177 lr: 0.000976 elapse: 0.681s\n",
      "2021-11-19 19:11:37,485-INFO: END epoch:54  train loss_avg:  1.3542  elapse_sum: 179.220s\n",
      "2021-11-19 19:11:45,193-INFO: epoch:55  train step:0    loss:  0.8249 lr: 0.000976 elapse: 7.705s\n",
      "2021-11-19 19:11:54,502-INFO: epoch:55  train step:10   loss:  1.0116 lr: 0.000976 elapse: 0.710s\n",
      "2021-11-19 19:12:01,506-INFO: epoch:55  train step:20   loss:  1.3348 lr: 0.000976 elapse: 0.699s\n",
      "2021-11-19 19:12:08,691-INFO: epoch:55  train step:30   loss:  0.9767 lr: 0.000976 elapse: 0.768s\n",
      "2021-11-19 19:12:15,791-INFO: epoch:55  train step:40   loss:  0.8646 lr: 0.000976 elapse: 0.782s\n",
      "2021-11-19 19:12:23,091-INFO: epoch:55  train step:50   loss:  0.8814 lr: 0.000976 elapse: 0.699s\n",
      "2021-11-19 19:12:30,191-INFO: epoch:55  train step:60   loss:  0.9606 lr: 0.000976 elapse: 0.697s\n",
      "2021-11-19 19:12:37,191-INFO: epoch:55  train step:70   loss:  0.9344 lr: 0.000976 elapse: 0.699s\n",
      "2021-11-19 19:12:44,192-INFO: epoch:55  train step:80   loss:  0.8564 lr: 0.000976 elapse: 0.700s\n",
      "2021-11-19 19:12:51,237-INFO: epoch:55  train step:90   loss:  2.2378 lr: 0.000976 elapse: 0.733s\n",
      "2021-11-19 19:12:58,491-INFO: epoch:55  train step:100  loss:  0.8984 lr: 0.000976 elapse: 0.754s\n",
      "2021-11-19 19:13:05,591-INFO: epoch:55  train step:110  loss:  1.2123 lr: 0.000976 elapse: 0.698s\n",
      "2021-11-19 19:13:12,691-INFO: epoch:55  train step:120  loss:  0.8627 lr: 0.000976 elapse: 0.699s\n",
      "2021-11-19 19:13:19,852-INFO: epoch:55  train step:130  loss:  1.0152 lr: 0.000976 elapse: 0.748s\n",
      "2021-11-19 19:13:27,207-INFO: epoch:55  train step:140  loss:  0.9396 lr: 0.000976 elapse: 0.703s\n",
      "2021-11-19 19:13:34,305-INFO: epoch:55  train step:150  loss:  1.3350 lr: 0.000976 elapse: 0.700s\n",
      "2021-11-19 19:13:41,492-INFO: epoch:55  train step:160  loss:  1.3681 lr: 0.000976 elapse: 0.763s\n",
      "2021-11-19 19:13:48,595-INFO: epoch:55  train step:170  loss:  1.4980 lr: 0.000976 elapse: 0.692s\n",
      "2021-11-19 19:13:55,691-INFO: epoch:55  train step:180  loss:  0.8316 lr: 0.000976 elapse: 0.765s\n",
      "2021-11-19 19:14:02,802-INFO: epoch:55  train step:190  loss:  2.8242 lr: 0.000976 elapse: 0.698s\n",
      "2021-11-19 19:14:09,737-INFO: epoch:55  train step:200  loss:  1.0017 lr: 0.000976 elapse: 0.678s\n",
      "2021-11-19 19:14:16,442-INFO: epoch:55  train step:210  loss:  0.8459 lr: 0.000975 elapse: 0.666s\n",
      "2021-11-19 19:14:23,355-INFO: epoch:55  train step:220  loss:  0.8324 lr: 0.000975 elapse: 0.666s\n",
      "2021-11-19 19:14:30,347-INFO: epoch:55  train step:230  loss:  0.9314 lr: 0.000975 elapse: 0.670s\n",
      "2021-11-19 19:14:36,343-INFO: END epoch:55  train loss_avg:  1.2952  elapse_sum: 178.855s\n",
      "2021-11-19 19:14:44,392-INFO: epoch:56  train step:0    loss:  1.3270 lr: 0.000975 elapse: 8.048s\n",
      "2021-11-19 19:14:53,191-INFO: epoch:56  train step:10   loss:  0.8909 lr: 0.000975 elapse: 0.766s\n",
      "2021-11-19 19:15:00,291-INFO: epoch:56  train step:20   loss:  1.1272 lr: 0.000975 elapse: 0.699s\n",
      "2021-11-19 19:15:07,291-INFO: epoch:56  train step:30   loss:  1.5988 lr: 0.000975 elapse: 0.699s\n",
      "2021-11-19 19:15:14,303-INFO: epoch:56  train step:40   loss:  1.1608 lr: 0.000975 elapse: 0.700s\n",
      "2021-11-19 19:15:21,391-INFO: epoch:56  train step:50   loss:  0.8610 lr: 0.000975 elapse: 0.785s\n",
      "2021-11-19 19:15:28,505-INFO: epoch:56  train step:60   loss:  1.0557 lr: 0.000975 elapse: 0.702s\n",
      "2021-11-19 19:15:35,891-INFO: epoch:56  train step:70   loss:  0.8421 lr: 0.000975 elapse: 0.698s\n",
      "2021-11-19 19:15:42,991-INFO: epoch:56  train step:80   loss:  0.9431 lr: 0.000975 elapse: 0.752s\n",
      "2021-11-19 19:15:50,102-INFO: epoch:56  train step:90   loss:  1.3118 lr: 0.000975 elapse: 0.764s\n",
      "2021-11-19 19:15:57,305-INFO: epoch:56  train step:100  loss:  1.2393 lr: 0.000975 elapse: 0.724s\n",
      "2021-11-19 19:16:04,301-INFO: epoch:56  train step:110  loss:  1.0806 lr: 0.000975 elapse: 0.698s\n",
      "2021-11-19 19:16:11,391-INFO: epoch:56  train step:120  loss:  2.1735 lr: 0.000975 elapse: 0.698s\n",
      "2021-11-19 19:16:18,405-INFO: epoch:56  train step:130  loss:  1.1709 lr: 0.000975 elapse: 0.701s\n",
      "2021-11-19 19:16:25,504-INFO: epoch:56  train step:140  loss:  0.8704 lr: 0.000975 elapse: 0.702s\n",
      "2021-11-19 19:16:32,691-INFO: epoch:56  train step:150  loss:  0.8436 lr: 0.000975 elapse: 0.698s\n",
      "2021-11-19 19:16:40,091-INFO: epoch:56  train step:160  loss:  1.1156 lr: 0.000975 elapse: 0.699s\n",
      "2021-11-19 19:16:47,191-INFO: epoch:56  train step:170  loss:  1.7531 lr: 0.000975 elapse: 0.765s\n",
      "2021-11-19 19:16:54,316-INFO: epoch:56  train step:180  loss:  1.7924 lr: 0.000975 elapse: 0.713s\n",
      "2021-11-19 19:17:01,391-INFO: epoch:56  train step:190  loss:  0.9245 lr: 0.000975 elapse: 0.698s\n",
      "2021-11-19 19:17:08,350-INFO: epoch:56  train step:200  loss:  2.3220 lr: 0.000975 elapse: 0.668s\n",
      "2021-11-19 19:17:15,043-INFO: epoch:56  train step:210  loss:  1.6024 lr: 0.000974 elapse: 0.667s\n",
      "2021-11-19 19:17:21,973-INFO: epoch:56  train step:220  loss:  0.9025 lr: 0.000974 elapse: 0.666s\n",
      "2021-11-19 19:17:28,933-INFO: epoch:56  train step:230  loss:  1.5784 lr: 0.000974 elapse: 0.665s\n",
      "2021-11-19 19:17:34,935-INFO: END epoch:56  train loss_avg:  1.3232  elapse_sum: 178.589s\n",
      "2021-11-19 19:17:42,907-INFO: epoch:57  train step:0    loss:  2.0277 lr: 0.000974 elapse: 7.970s\n",
      "2021-11-19 19:17:52,302-INFO: epoch:57  train step:10   loss:  0.8182 lr: 0.000974 elapse: 0.700s\n",
      "2021-11-19 19:17:59,391-INFO: epoch:57  train step:20   loss:  1.6257 lr: 0.000974 elapse: 0.699s\n",
      "2021-11-19 19:18:06,391-INFO: epoch:57  train step:30   loss:  0.8758 lr: 0.000974 elapse: 0.699s\n",
      "2021-11-19 19:18:13,401-INFO: epoch:57  train step:40   loss:  1.3560 lr: 0.000974 elapse: 0.697s\n",
      "2021-11-19 19:18:20,493-INFO: epoch:57  train step:50   loss:  1.9277 lr: 0.000974 elapse: 0.701s\n",
      "2021-11-19 19:18:27,601-INFO: epoch:57  train step:60   loss:  0.8515 lr: 0.000974 elapse: 0.711s\n",
      "2021-11-19 19:18:34,604-INFO: epoch:57  train step:70   loss:  0.9422 lr: 0.000974 elapse: 0.699s\n",
      "2021-11-19 19:18:41,617-INFO: epoch:57  train step:80   loss:  0.9953 lr: 0.000974 elapse: 0.709s\n",
      "2021-11-19 19:18:49,024-INFO: epoch:57  train step:90   loss:  0.8842 lr: 0.000974 elapse: 0.732s\n",
      "2021-11-19 19:18:56,403-INFO: epoch:57  train step:100  loss:  0.9091 lr: 0.000974 elapse: 0.697s\n",
      "2021-11-19 19:19:03,492-INFO: epoch:57  train step:110  loss:  0.9013 lr: 0.000974 elapse: 0.700s\n",
      "2021-11-19 19:19:10,625-INFO: epoch:57  train step:120  loss:  2.7000 lr: 0.000974 elapse: 0.722s\n",
      "2021-11-19 19:19:17,693-INFO: epoch:57  train step:130  loss:  0.8148 lr: 0.000974 elapse: 0.701s\n",
      "2021-11-19 19:19:24,793-INFO: epoch:57  train step:140  loss:  0.9256 lr: 0.000974 elapse: 0.774s\n",
      "2021-11-19 19:19:31,901-INFO: epoch:57  train step:150  loss:  0.8183 lr: 0.000974 elapse: 0.698s\n",
      "2021-11-19 19:19:38,924-INFO: epoch:57  train step:160  loss:  2.1304 lr: 0.000974 elapse: 0.721s\n",
      "2021-11-19 19:19:46,003-INFO: epoch:57  train step:170  loss:  1.5551 lr: 0.000974 elapse: 0.700s\n",
      "2021-11-19 19:19:53,346-INFO: epoch:57  train step:180  loss:  1.9867 lr: 0.000974 elapse: 0.792s\n",
      "2021-11-19 19:20:00,503-INFO: epoch:57  train step:190  loss:  0.8759 lr: 0.000974 elapse: 0.701s\n",
      "2021-11-19 19:20:07,601-INFO: epoch:57  train step:200  loss:  2.0347 lr: 0.000974 elapse: 0.667s\n",
      "2021-11-19 19:20:14,272-INFO: epoch:57  train step:210  loss:  0.8214 lr: 0.000973 elapse: 0.667s\n",
      "2021-11-19 19:20:21,201-INFO: epoch:57  train step:220  loss:  1.0391 lr: 0.000973 elapse: 0.666s\n",
      "2021-11-19 19:20:28,184-INFO: epoch:57  train step:230  loss:  2.2499 lr: 0.000973 elapse: 0.668s\n",
      "2021-11-19 19:20:34,172-INFO: END epoch:57  train loss_avg:  1.3279  elapse_sum: 179.234s\n",
      "2021-11-19 19:20:41,992-INFO: epoch:58  train step:0    loss:  0.8348 lr: 0.000973 elapse: 7.819s\n",
      "2021-11-19 19:20:50,904-INFO: epoch:58  train step:10   loss:  2.1563 lr: 0.000973 elapse: 0.777s\n",
      "2021-11-19 19:20:58,104-INFO: epoch:58  train step:20   loss:  0.8652 lr: 0.000973 elapse: 0.755s\n",
      "2021-11-19 19:21:05,401-INFO: epoch:58  train step:30   loss:  0.8974 lr: 0.000973 elapse: 0.697s\n",
      "2021-11-19 19:21:12,424-INFO: epoch:58  train step:40   loss:  0.8944 lr: 0.000973 elapse: 0.720s\n",
      "2021-11-19 19:21:19,491-INFO: epoch:58  train step:50   loss:  1.0799 lr: 0.000973 elapse: 0.699s\n",
      "2021-11-19 19:21:26,591-INFO: epoch:58  train step:60   loss:  1.0061 lr: 0.000973 elapse: 0.699s\n",
      "2021-11-19 19:21:33,602-INFO: epoch:58  train step:70   loss:  0.9550 lr: 0.000973 elapse: 0.682s\n",
      "2021-11-19 19:21:40,725-INFO: epoch:58  train step:80   loss:  2.0575 lr: 0.000973 elapse: 0.722s\n",
      "2021-11-19 19:21:47,916-INFO: epoch:58  train step:90   loss:  1.9051 lr: 0.000973 elapse: 0.713s\n",
      "2021-11-19 19:21:55,002-INFO: epoch:58  train step:100  loss:  0.9385 lr: 0.000973 elapse: 0.699s\n",
      "2021-11-19 19:22:02,246-INFO: epoch:58  train step:110  loss:  0.8872 lr: 0.000973 elapse: 0.754s\n",
      "2021-11-19 19:22:09,527-INFO: epoch:58  train step:120  loss:  2.0403 lr: 0.000973 elapse: 0.724s\n",
      "2021-11-19 19:22:16,701-INFO: epoch:58  train step:130  loss:  1.6901 lr: 0.000973 elapse: 0.698s\n",
      "2021-11-19 19:22:23,911-INFO: epoch:58  train step:140  loss:  0.8432 lr: 0.000973 elapse: 0.707s\n",
      "2021-11-19 19:22:31,004-INFO: epoch:58  train step:150  loss:  0.9301 lr: 0.000973 elapse: 0.700s\n",
      "2021-11-19 19:22:38,291-INFO: epoch:58  train step:160  loss:  1.4124 lr: 0.000973 elapse: 0.765s\n",
      "2021-11-19 19:22:45,401-INFO: epoch:58  train step:170  loss:  1.7649 lr: 0.000973 elapse: 0.680s\n",
      "2021-11-19 19:22:52,504-INFO: epoch:58  train step:180  loss:  2.1963 lr: 0.000973 elapse: 0.702s\n",
      "2021-11-19 19:22:59,703-INFO: epoch:58  train step:190  loss:  1.0926 lr: 0.000973 elapse: 0.711s\n",
      "2021-11-19 19:23:06,660-INFO: epoch:58  train step:200  loss:  0.8470 lr: 0.000972 elapse: 0.668s\n",
      "2021-11-19 19:23:13,341-INFO: epoch:58  train step:210  loss:  1.6006 lr: 0.000972 elapse: 0.670s\n",
      "2021-11-19 19:23:20,240-INFO: epoch:58  train step:220  loss:  2.2957 lr: 0.000972 elapse: 0.788s\n",
      "2021-11-19 19:23:27,192-INFO: epoch:58  train step:230  loss:  1.3880 lr: 0.000972 elapse: 0.796s\n",
      "2021-11-19 19:23:33,285-INFO: END epoch:58  train loss_avg:  1.2941  elapse_sum: 179.110s\n",
      "2021-11-19 19:23:41,392-INFO: epoch:59  train step:0    loss:  2.2310 lr: 0.000972 elapse: 8.105s\n",
      "2021-11-19 19:23:50,015-INFO: epoch:59  train step:10   loss:  1.3333 lr: 0.000972 elapse: 0.712s\n",
      "2021-11-19 19:23:57,302-INFO: epoch:59  train step:20   loss:  0.8574 lr: 0.000972 elapse: 0.700s\n",
      "2021-11-19 19:24:04,402-INFO: epoch:59  train step:30   loss:  1.0903 lr: 0.000972 elapse: 0.696s\n",
      "2021-11-19 19:24:11,691-INFO: epoch:59  train step:40   loss:  2.2152 lr: 0.000972 elapse: 0.788s\n",
      "2021-11-19 19:24:18,901-INFO: epoch:59  train step:50   loss:  0.9288 lr: 0.000972 elapse: 0.695s\n",
      "2021-11-19 19:24:25,991-INFO: epoch:59  train step:60   loss:  1.1383 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:24:33,002-INFO: epoch:59  train step:70   loss:  1.6672 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:24:40,093-INFO: epoch:59  train step:80   loss:  0.9968 lr: 0.000972 elapse: 0.701s\n",
      "2021-11-19 19:24:47,205-INFO: epoch:59  train step:90   loss:  1.3371 lr: 0.000972 elapse: 0.702s\n",
      "2021-11-19 19:24:54,505-INFO: epoch:59  train step:100  loss:  0.8628 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:25:01,707-INFO: epoch:59  train step:110  loss:  0.9013 lr: 0.000972 elapse: 0.704s\n",
      "2021-11-19 19:25:08,781-INFO: epoch:59  train step:120  loss:  0.8581 lr: 0.000972 elapse: 0.689s\n",
      "2021-11-19 19:25:16,239-INFO: epoch:59  train step:130  loss:  1.0240 lr: 0.000972 elapse: 0.801s\n",
      "2021-11-19 19:25:23,503-INFO: epoch:59  train step:140  loss:  0.9364 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:25:30,602-INFO: epoch:59  train step:150  loss:  1.0798 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:25:37,803-INFO: epoch:59  train step:160  loss:  2.2783 lr: 0.000972 elapse: 0.701s\n",
      "2021-11-19 19:25:44,802-INFO: epoch:59  train step:170  loss:  0.8303 lr: 0.000972 elapse: 0.699s\n",
      "2021-11-19 19:25:51,902-INFO: epoch:59  train step:180  loss:  1.7947 lr: 0.000972 elapse: 0.700s\n",
      "2021-11-19 19:25:58,994-INFO: epoch:59  train step:190  loss:  1.5802 lr: 0.000971 elapse: 0.703s\n",
      "2021-11-19 19:26:05,984-INFO: epoch:59  train step:200  loss:  1.1577 lr: 0.000971 elapse: 0.677s\n",
      "2021-11-19 19:26:12,688-INFO: epoch:59  train step:210  loss:  0.8259 lr: 0.000971 elapse: 0.667s\n",
      "2021-11-19 19:26:19,659-INFO: epoch:59  train step:220  loss:  0.8702 lr: 0.000971 elapse: 0.667s\n",
      "2021-11-19 19:26:26,581-INFO: epoch:59  train step:230  loss:  2.0004 lr: 0.000971 elapse: 0.675s\n",
      "2021-11-19 19:26:32,672-INFO: END epoch:59  train loss_avg:  1.2978  elapse_sum: 179.384s\n",
      "2021-11-19 19:26:40,294-INFO: epoch:60  train step:0    loss:  0.8604 lr: 0.000971 elapse: 7.620s\n",
      "2021-11-19 19:26:49,491-INFO: epoch:60  train step:10   loss:  0.8280 lr: 0.000971 elapse: 0.699s\n",
      "2021-11-19 19:26:56,503-INFO: epoch:60  train step:20   loss:  0.9567 lr: 0.000971 elapse: 0.698s\n",
      "2021-11-19 19:27:03,719-INFO: epoch:60  train step:30   loss:  1.2562 lr: 0.000971 elapse: 0.712s\n",
      "2021-11-19 19:27:10,801-INFO: epoch:60  train step:40   loss:  1.0725 lr: 0.000971 elapse: 0.694s\n",
      "2021-11-19 19:27:17,891-INFO: epoch:60  train step:50   loss:  1.4731 lr: 0.000971 elapse: 0.771s\n",
      "2021-11-19 19:27:25,202-INFO: epoch:60  train step:60   loss:  1.7846 lr: 0.000971 elapse: 0.789s\n",
      "2021-11-19 19:27:32,391-INFO: epoch:60  train step:70   loss:  0.9033 lr: 0.000971 elapse: 0.699s\n",
      "2021-11-19 19:27:39,403-INFO: epoch:60  train step:80   loss:  1.0681 lr: 0.000971 elapse: 0.686s\n",
      "2021-11-19 19:27:46,503-INFO: epoch:60  train step:90   loss:  1.7964 lr: 0.000971 elapse: 0.700s\n",
      "2021-11-19 19:27:53,606-INFO: epoch:60  train step:100  loss:  1.0581 lr: 0.000971 elapse: 0.688s\n",
      "2021-11-19 19:28:00,601-INFO: epoch:60  train step:110  loss:  0.9325 lr: 0.000971 elapse: 0.698s\n",
      "2021-11-19 19:28:07,791-INFO: epoch:60  train step:120  loss:  2.3966 lr: 0.000971 elapse: 0.699s\n",
      "2021-11-19 19:28:14,827-INFO: epoch:60  train step:130  loss:  0.8280 lr: 0.000971 elapse: 0.721s\n",
      "2021-11-19 19:28:21,892-INFO: epoch:60  train step:140  loss:  0.8293 lr: 0.000971 elapse: 0.691s\n",
      "2021-11-19 19:28:29,113-INFO: epoch:60  train step:150  loss:  0.8116 lr: 0.000971 elapse: 0.774s\n",
      "2021-11-19 19:28:36,302-INFO: epoch:60  train step:160  loss:  0.9705 lr: 0.000971 elapse: 0.700s\n",
      "2021-11-19 19:28:43,428-INFO: epoch:60  train step:170  loss:  0.8498 lr: 0.000971 elapse: 0.725s\n",
      "2021-11-19 19:28:50,505-INFO: epoch:60  train step:180  loss:  0.9968 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 19:28:57,791-INFO: epoch:60  train step:190  loss:  1.3777 lr: 0.000970 elapse: 0.788s\n",
      "2021-11-19 19:29:04,814-INFO: epoch:60  train step:200  loss:  0.8118 lr: 0.000970 elapse: 0.668s\n",
      "2021-11-19 19:29:11,495-INFO: epoch:60  train step:210  loss:  1.5448 lr: 0.000970 elapse: 0.670s\n",
      "2021-11-19 19:29:18,431-INFO: epoch:60  train step:220  loss:  0.9520 lr: 0.000970 elapse: 0.667s\n",
      "2021-11-19 19:29:25,422-INFO: epoch:60  train step:230  loss:  1.0950 lr: 0.000970 elapse: 0.673s\n",
      "2021-11-19 19:29:31,761-INFO: END epoch:60  train loss_avg:  1.3702  elapse_sum: 179.086s\n",
      "2021-11-19 19:29:36,697-INFO: epoch:60  valid step:0    loss:  1.2080 top1: 0.8125 top5: 1.0000 elapse: 4.934s\n",
      "2021-11-19 19:29:41,824-INFO: epoch:60  valid step:10   loss:  1.4525 top1: 0.7500 top5: 0.9375 elapse: 0.205s\n",
      "2021-11-19 19:29:44,163-INFO: epoch:60  valid step:20   loss:  1.1291 top1: 0.8750 top5: 1.0000 elapse: 0.215s\n",
      "2021-11-19 19:29:46,288-INFO: epoch:60  valid step:30   loss:  1.3265 top1: 0.8750 top5: 0.9375 elapse: 0.189s\n",
      "2021-11-19 19:29:48,190-INFO: epoch:60  valid step:40   loss:  1.1754 top1: 0.8750 top5: 1.0000 elapse: 0.187s\n",
      "2021-11-19 19:29:50,186-INFO: epoch:60  valid step:50   loss:  1.4967 top1: 0.7500 top5: 0.8750 elapse: 0.188s\n",
      "2021-11-19 19:29:52,162-INFO: epoch:60  valid step:60   loss:  1.7037 top1: 0.8125 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 19:29:54,055-INFO: epoch:60  valid step:70   loss:  1.5394 top1: 0.7500 top5: 0.9375 elapse: 0.187s\n",
      "2021-11-19 19:29:56,039-INFO: epoch:60  valid step:80   loss:  1.5494 top1: 0.8125 top5: 0.8750 elapse: 0.188s\n",
      "2021-11-19 19:29:58,045-INFO: END epoch:60  valid loss_avg:  1.2394 top1_avg: 0.8528 top5_avg: 0.9632 elapse_sum: 26.282s\n",
      "2021-11-19 19:29:59,134-INFO: Already save model in ./output/EfficientNetB3/60\n",
      "2021-11-19 19:30:06,892-INFO: epoch:61  train step:0    loss:  0.8138 lr: 0.000970 elapse: 7.757s\n",
      "2021-11-19 19:30:16,294-INFO: epoch:61  train step:10   loss:  1.8551 lr: 0.000970 elapse: 0.702s\n",
      "2021-11-19 19:30:23,391-INFO: epoch:61  train step:20   loss:  1.5249 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 19:30:30,592-INFO: epoch:61  train step:30   loss:  0.8157 lr: 0.000970 elapse: 0.768s\n",
      "2021-11-19 19:30:37,605-INFO: epoch:61  train step:40   loss:  0.8453 lr: 0.000970 elapse: 0.707s\n",
      "2021-11-19 19:30:44,893-INFO: epoch:61  train step:50   loss:  0.8807 lr: 0.000970 elapse: 0.700s\n",
      "2021-11-19 19:30:51,905-INFO: epoch:61  train step:60   loss:  0.9435 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 19:30:59,004-INFO: epoch:61  train step:70   loss:  1.0407 lr: 0.000970 elapse: 0.769s\n",
      "2021-11-19 19:31:06,091-INFO: epoch:61  train step:80   loss:  1.1141 lr: 0.000970 elapse: 0.697s\n",
      "2021-11-19 19:31:13,191-INFO: epoch:61  train step:90   loss:  1.0122 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 19:31:20,325-INFO: epoch:61  train step:100  loss:  2.0560 lr: 0.000970 elapse: 0.720s\n",
      "2021-11-19 19:31:27,491-INFO: epoch:61  train step:110  loss:  1.4145 lr: 0.000970 elapse: 0.699s\n",
      "2021-11-19 19:31:34,504-INFO: epoch:61  train step:120  loss:  0.9833 lr: 0.000970 elapse: 0.701s\n",
      "2021-11-19 19:31:41,503-INFO: epoch:61  train step:130  loss:  1.4526 lr: 0.000970 elapse: 0.700s\n",
      "2021-11-19 19:31:48,803-INFO: epoch:61  train step:140  loss:  1.8159 lr: 0.000970 elapse: 0.700s\n",
      "2021-11-19 19:31:55,991-INFO: epoch:61  train step:150  loss:  0.8231 lr: 0.000970 elapse: 0.764s\n",
      "2021-11-19 19:32:03,105-INFO: epoch:61  train step:160  loss:  2.4284 lr: 0.000969 elapse: 0.699s\n",
      "2021-11-19 19:32:10,102-INFO: epoch:61  train step:170  loss:  0.8143 lr: 0.000969 elapse: 0.698s\n",
      "2021-11-19 19:32:17,102-INFO: epoch:61  train step:180  loss:  1.9629 lr: 0.000969 elapse: 0.700s\n",
      "2021-11-19 19:32:24,202-INFO: epoch:61  train step:190  loss:  1.2923 lr: 0.000969 elapse: 0.699s\n",
      "2021-11-19 19:32:31,222-INFO: epoch:61  train step:200  loss:  0.8080 lr: 0.000969 elapse: 0.680s\n",
      "2021-11-19 19:32:37,903-INFO: epoch:61  train step:210  loss:  2.6950 lr: 0.000969 elapse: 0.665s\n",
      "2021-11-19 19:32:44,710-INFO: epoch:61  train step:220  loss:  2.0857 lr: 0.000969 elapse: 0.665s\n",
      "2021-11-19 19:32:51,601-INFO: epoch:61  train step:230  loss:  1.5434 lr: 0.000969 elapse: 0.677s\n",
      "2021-11-19 19:32:57,682-INFO: END epoch:61  train loss_avg:  1.3194  elapse_sum: 178.545s\n",
      "2021-11-19 19:33:05,595-INFO: epoch:62  train step:0    loss:  0.9161 lr: 0.000969 elapse: 7.911s\n",
      "2021-11-19 19:33:14,703-INFO: epoch:62  train step:10   loss:  2.5672 lr: 0.000969 elapse: 0.711s\n",
      "2021-11-19 19:33:21,905-INFO: epoch:62  train step:20   loss:  1.0387 lr: 0.000969 elapse: 0.702s\n",
      "2021-11-19 19:33:29,117-INFO: epoch:62  train step:30   loss:  1.7533 lr: 0.000969 elapse: 0.715s\n",
      "2021-11-19 19:33:36,403-INFO: epoch:62  train step:40   loss:  1.3001 lr: 0.000969 elapse: 0.697s\n",
      "2021-11-19 19:33:43,703-INFO: epoch:62  train step:50   loss:  1.1271 lr: 0.000969 elapse: 0.711s\n",
      "2021-11-19 19:33:51,134-INFO: epoch:62  train step:60   loss:  0.8904 lr: 0.000969 elapse: 0.838s\n",
      "2021-11-19 19:33:58,517-INFO: epoch:62  train step:70   loss:  0.8907 lr: 0.000969 elapse: 0.711s\n",
      "2021-11-19 19:34:05,824-INFO: epoch:62  train step:80   loss:  0.9709 lr: 0.000969 elapse: 0.719s\n",
      "2021-11-19 19:34:12,903-INFO: epoch:62  train step:90   loss:  2.0424 lr: 0.000969 elapse: 0.723s\n",
      "2021-11-19 19:34:20,091-INFO: epoch:62  train step:100  loss:  1.8635 lr: 0.000969 elapse: 0.699s\n",
      "2021-11-19 19:34:27,302-INFO: epoch:62  train step:110  loss:  1.0783 lr: 0.000969 elapse: 0.709s\n",
      "2021-11-19 19:34:34,391-INFO: epoch:62  train step:120  loss:  1.6824 lr: 0.000969 elapse: 0.699s\n",
      "2021-11-19 19:34:41,402-INFO: epoch:62  train step:130  loss:  1.0319 lr: 0.000969 elapse: 0.710s\n",
      "2021-11-19 19:34:48,527-INFO: epoch:62  train step:140  loss:  0.9374 lr: 0.000968 elapse: 0.723s\n",
      "2021-11-19 19:34:55,817-INFO: epoch:62  train step:150  loss:  0.8233 lr: 0.000968 elapse: 0.775s\n",
      "2021-11-19 19:35:03,102-INFO: epoch:62  train step:160  loss:  1.1112 lr: 0.000968 elapse: 0.697s\n",
      "2021-11-19 19:35:10,203-INFO: epoch:62  train step:170  loss:  2.3408 lr: 0.000968 elapse: 0.700s\n",
      "2021-11-19 19:35:17,291-INFO: epoch:62  train step:180  loss:  0.9059 lr: 0.000968 elapse: 0.699s\n",
      "2021-11-19 19:35:24,305-INFO: epoch:62  train step:190  loss:  0.8028 lr: 0.000968 elapse: 0.701s\n",
      "2021-11-19 19:35:31,337-INFO: epoch:62  train step:200  loss:  2.1043 lr: 0.000968 elapse: 0.677s\n",
      "2021-11-19 19:35:38,033-INFO: epoch:62  train step:210  loss:  1.4078 lr: 0.000968 elapse: 0.667s\n",
      "2021-11-19 19:35:44,837-INFO: epoch:62  train step:220  loss:  0.7839 lr: 0.000968 elapse: 0.665s\n",
      "2021-11-19 19:35:51,822-INFO: epoch:62  train step:230  loss:  2.0799 lr: 0.000968 elapse: 0.667s\n",
      "2021-11-19 19:35:57,841-INFO: END epoch:62  train loss_avg:  1.3163  elapse_sum: 180.156s\n",
      "2021-11-19 19:36:06,392-INFO: epoch:63  train step:0    loss:  0.8977 lr: 0.000968 elapse: 8.550s\n",
      "2021-11-19 19:36:15,391-INFO: epoch:63  train step:10   loss:  0.8554 lr: 0.000968 elapse: 0.697s\n",
      "2021-11-19 19:36:22,402-INFO: epoch:63  train step:20   loss:  2.5107 lr: 0.000968 elapse: 0.699s\n",
      "2021-11-19 19:36:29,491-INFO: epoch:63  train step:30   loss:  1.6980 lr: 0.000968 elapse: 0.698s\n",
      "2021-11-19 19:36:36,491-INFO: epoch:63  train step:40   loss:  0.8981 lr: 0.000968 elapse: 0.699s\n",
      "2021-11-19 19:36:43,602-INFO: epoch:63  train step:50   loss:  1.8686 lr: 0.000968 elapse: 0.700s\n",
      "2021-11-19 19:36:50,639-INFO: epoch:63  train step:60   loss:  1.6262 lr: 0.000968 elapse: 0.721s\n",
      "2021-11-19 19:36:57,703-INFO: epoch:63  train step:70   loss:  2.1866 lr: 0.000968 elapse: 0.701s\n",
      "2021-11-19 19:37:04,702-INFO: epoch:63  train step:80   loss:  0.8972 lr: 0.000968 elapse: 0.698s\n",
      "2021-11-19 19:37:11,903-INFO: epoch:63  train step:90   loss:  2.2620 lr: 0.000968 elapse: 0.766s\n",
      "2021-11-19 19:37:19,093-INFO: epoch:63  train step:100  loss:  0.8601 lr: 0.000968 elapse: 0.691s\n",
      "2021-11-19 19:37:26,116-INFO: epoch:63  train step:110  loss:  2.0479 lr: 0.000967 elapse: 0.710s\n",
      "2021-11-19 19:37:33,200-INFO: epoch:63  train step:120  loss:  1.7976 lr: 0.000967 elapse: 0.698s\n",
      "2021-11-19 19:37:40,302-INFO: epoch:63  train step:130  loss:  1.0213 lr: 0.000967 elapse: 0.699s\n",
      "2021-11-19 19:37:47,391-INFO: epoch:63  train step:140  loss:  1.0348 lr: 0.000967 elapse: 0.699s\n",
      "2021-11-19 19:37:54,601-INFO: epoch:63  train step:150  loss:  1.3758 lr: 0.000967 elapse: 0.709s\n",
      "2021-11-19 19:38:01,802-INFO: epoch:63  train step:160  loss:  2.1430 lr: 0.000967 elapse: 0.783s\n",
      "2021-11-19 19:38:08,903-INFO: epoch:63  train step:170  loss:  1.1314 lr: 0.000967 elapse: 0.700s\n",
      "2021-11-19 19:38:16,191-INFO: epoch:63  train step:180  loss:  0.8105 lr: 0.000967 elapse: 0.751s\n",
      "2021-11-19 19:38:23,324-INFO: epoch:63  train step:190  loss:  1.0646 lr: 0.000967 elapse: 0.702s\n",
      "2021-11-19 19:38:30,335-INFO: epoch:63  train step:200  loss:  0.8569 lr: 0.000967 elapse: 0.678s\n",
      "2021-11-19 19:38:37,001-INFO: epoch:63  train step:210  loss:  0.8714 lr: 0.000967 elapse: 0.666s\n",
      "2021-11-19 19:38:43,866-INFO: epoch:63  train step:220  loss:  1.2186 lr: 0.000967 elapse: 0.666s\n",
      "2021-11-19 19:38:50,792-INFO: epoch:63  train step:230  loss:  1.1236 lr: 0.000967 elapse: 0.670s\n",
      "2021-11-19 19:38:56,779-INFO: END epoch:63  train loss_avg:  1.3573  elapse_sum: 178.935s\n",
      "2021-11-19 19:39:04,792-INFO: epoch:64  train step:0    loss:  1.4712 lr: 0.000967 elapse: 8.012s\n",
      "2021-11-19 19:39:13,792-INFO: epoch:64  train step:10   loss:  2.5457 lr: 0.000967 elapse: 0.700s\n",
      "2021-11-19 19:39:21,002-INFO: epoch:64  train step:20   loss:  2.2230 lr: 0.000967 elapse: 0.749s\n",
      "2021-11-19 19:39:28,202-INFO: epoch:64  train step:30   loss:  1.0625 lr: 0.000967 elapse: 0.676s\n",
      "2021-11-19 19:39:35,301-INFO: epoch:64  train step:40   loss:  2.0157 lr: 0.000967 elapse: 0.697s\n",
      "2021-11-19 19:39:42,391-INFO: epoch:64  train step:50   loss:  0.9181 lr: 0.000967 elapse: 0.765s\n",
      "2021-11-19 19:39:49,425-INFO: epoch:64  train step:60   loss:  0.9003 lr: 0.000967 elapse: 0.707s\n",
      "2021-11-19 19:39:56,691-INFO: epoch:64  train step:70   loss:  1.7909 lr: 0.000967 elapse: 0.699s\n",
      "2021-11-19 19:40:03,808-INFO: epoch:64  train step:80   loss:  2.5300 lr: 0.000966 elapse: 0.716s\n",
      "2021-11-19 19:40:10,891-INFO: epoch:64  train step:90   loss:  0.9652 lr: 0.000966 elapse: 0.766s\n",
      "2021-11-19 19:40:17,902-INFO: epoch:64  train step:100  loss:  0.8690 lr: 0.000966 elapse: 0.698s\n",
      "2021-11-19 19:40:25,041-INFO: epoch:64  train step:110  loss:  2.2565 lr: 0.000966 elapse: 0.793s\n",
      "2021-11-19 19:40:32,292-INFO: epoch:64  train step:120  loss:  1.1769 lr: 0.000966 elapse: 0.699s\n",
      "2021-11-19 19:40:39,315-INFO: epoch:64  train step:130  loss:  2.4343 lr: 0.000966 elapse: 0.712s\n",
      "2021-11-19 19:40:46,406-INFO: epoch:64  train step:140  loss:  1.0669 lr: 0.000966 elapse: 0.682s\n",
      "2021-11-19 19:40:53,604-INFO: epoch:64  train step:150  loss:  2.6469 lr: 0.000966 elapse: 0.700s\n",
      "2021-11-19 19:41:00,620-INFO: epoch:64  train step:160  loss:  0.8069 lr: 0.000966 elapse: 0.716s\n",
      "2021-11-19 19:41:07,801-INFO: epoch:64  train step:170  loss:  0.9308 lr: 0.000966 elapse: 0.694s\n",
      "2021-11-19 19:41:14,817-INFO: epoch:64  train step:180  loss:  1.0884 lr: 0.000966 elapse: 0.713s\n",
      "2021-11-19 19:41:21,991-INFO: epoch:64  train step:190  loss:  0.8550 lr: 0.000966 elapse: 0.768s\n",
      "2021-11-19 19:41:28,935-INFO: epoch:64  train step:200  loss:  1.7805 lr: 0.000966 elapse: 0.677s\n",
      "2021-11-19 19:41:35,647-INFO: epoch:64  train step:210  loss:  1.9407 lr: 0.000966 elapse: 0.667s\n",
      "2021-11-19 19:41:42,484-INFO: epoch:64  train step:220  loss:  1.8848 lr: 0.000966 elapse: 0.670s\n",
      "2021-11-19 19:41:49,452-INFO: epoch:64  train step:230  loss:  1.1285 lr: 0.000966 elapse: 0.666s\n",
      "2021-11-19 19:41:55,447-INFO: END epoch:64  train loss_avg:  1.3740  elapse_sum: 178.665s\n",
      "2021-11-19 19:42:03,392-INFO: epoch:65  train step:0    loss:  1.9656 lr: 0.000966 elapse: 7.944s\n",
      "2021-11-19 19:42:12,591-INFO: epoch:65  train step:10   loss:  2.1877 lr: 0.000966 elapse: 0.764s\n",
      "2021-11-19 19:42:19,602-INFO: epoch:65  train step:20   loss:  1.0332 lr: 0.000966 elapse: 0.698s\n",
      "2021-11-19 19:42:26,605-INFO: epoch:65  train step:30   loss:  0.8253 lr: 0.000966 elapse: 0.702s\n",
      "2021-11-19 19:42:33,891-INFO: epoch:65  train step:40   loss:  0.8296 lr: 0.000966 elapse: 0.838s\n",
      "2021-11-19 19:42:41,104-INFO: epoch:65  train step:50   loss:  1.7170 lr: 0.000965 elapse: 0.710s\n",
      "2021-11-19 19:42:48,191-INFO: epoch:65  train step:60   loss:  1.2629 lr: 0.000965 elapse: 0.699s\n",
      "2021-11-19 19:42:55,301-INFO: epoch:65  train step:70   loss:  0.9976 lr: 0.000965 elapse: 0.698s\n",
      "2021-11-19 19:43:02,302-INFO: epoch:65  train step:80   loss:  0.8473 lr: 0.000965 elapse: 0.695s\n",
      "2021-11-19 19:43:09,402-INFO: epoch:65  train step:90   loss:  0.8509 lr: 0.000965 elapse: 0.697s\n",
      "2021-11-19 19:43:16,504-INFO: epoch:65  train step:100  loss:  2.3011 lr: 0.000965 elapse: 0.701s\n",
      "2021-11-19 19:43:23,604-INFO: epoch:65  train step:110  loss:  1.0676 lr: 0.000965 elapse: 0.701s\n",
      "2021-11-19 19:43:30,791-INFO: epoch:65  train step:120  loss:  0.9235 lr: 0.000965 elapse: 0.768s\n",
      "2021-11-19 19:43:38,103-INFO: epoch:65  train step:130  loss:  1.0479 lr: 0.000965 elapse: 0.711s\n",
      "2021-11-19 19:43:45,401-INFO: epoch:65  train step:140  loss:  1.6581 lr: 0.000965 elapse: 0.706s\n",
      "2021-11-19 19:43:52,502-INFO: epoch:65  train step:150  loss:  0.8373 lr: 0.000965 elapse: 0.699s\n",
      "2021-11-19 19:43:59,583-INFO: epoch:65  train step:160  loss:  1.8197 lr: 0.000965 elapse: 0.702s\n",
      "2021-11-19 19:44:06,701-INFO: epoch:65  train step:170  loss:  0.9814 lr: 0.000965 elapse: 0.698s\n",
      "2021-11-19 19:44:13,815-INFO: epoch:65  train step:180  loss:  0.9658 lr: 0.000965 elapse: 0.711s\n",
      "2021-11-19 19:44:20,903-INFO: epoch:65  train step:190  loss:  0.9026 lr: 0.000965 elapse: 0.711s\n",
      "2021-11-19 19:44:27,903-INFO: epoch:65  train step:200  loss:  1.9964 lr: 0.000965 elapse: 0.671s\n",
      "2021-11-19 19:44:34,616-INFO: epoch:65  train step:210  loss:  1.2534 lr: 0.000965 elapse: 0.667s\n",
      "2021-11-19 19:44:41,419-INFO: epoch:65  train step:220  loss:  0.8912 lr: 0.000965 elapse: 0.676s\n",
      "2021-11-19 19:44:48,367-INFO: epoch:65  train step:230  loss:  0.7941 lr: 0.000965 elapse: 0.666s\n",
      "2021-11-19 19:44:54,359-INFO: END epoch:65  train loss_avg:  1.3036  elapse_sum: 178.909s\n",
      "2021-11-19 19:45:02,092-INFO: epoch:66  train step:0    loss:  0.8623 lr: 0.000965 elapse: 7.731s\n",
      "2021-11-19 19:45:11,205-INFO: epoch:66  train step:10   loss:  1.0093 lr: 0.000965 elapse: 0.703s\n",
      "2021-11-19 19:45:18,291-INFO: epoch:66  train step:20   loss:  1.0540 lr: 0.000964 elapse: 0.699s\n",
      "2021-11-19 19:45:25,305-INFO: epoch:66  train step:30   loss:  0.8774 lr: 0.000964 elapse: 0.702s\n",
      "2021-11-19 19:45:32,491-INFO: epoch:66  train step:40   loss:  2.2349 lr: 0.000964 elapse: 0.767s\n",
      "2021-11-19 19:45:39,501-INFO: epoch:66  train step:50   loss:  0.8129 lr: 0.000964 elapse: 0.693s\n",
      "2021-11-19 19:45:46,741-INFO: epoch:66  train step:60   loss:  1.6625 lr: 0.000964 elapse: 0.737s\n",
      "2021-11-19 19:45:53,991-INFO: epoch:66  train step:70   loss:  1.0180 lr: 0.000964 elapse: 0.697s\n",
      "2021-11-19 19:46:01,104-INFO: epoch:66  train step:80   loss:  1.4392 lr: 0.000964 elapse: 0.687s\n",
      "2021-11-19 19:46:08,191-INFO: epoch:66  train step:90   loss:  0.8675 lr: 0.000964 elapse: 0.698s\n",
      "2021-11-19 19:46:15,214-INFO: epoch:66  train step:100  loss:  0.9154 lr: 0.000964 elapse: 0.711s\n",
      "2021-11-19 19:46:22,302-INFO: epoch:66  train step:110  loss:  1.6814 lr: 0.000964 elapse: 0.700s\n",
      "2021-11-19 19:46:29,491-INFO: epoch:66  train step:120  loss:  2.4413 lr: 0.000964 elapse: 0.772s\n",
      "2021-11-19 19:46:36,627-INFO: epoch:66  train step:130  loss:  2.0657 lr: 0.000964 elapse: 0.725s\n",
      "2021-11-19 19:46:43,723-INFO: epoch:66  train step:140  loss:  1.4089 lr: 0.000964 elapse: 0.720s\n",
      "2021-11-19 19:46:51,014-INFO: epoch:66  train step:150  loss:  2.0153 lr: 0.000964 elapse: 0.783s\n",
      "2021-11-19 19:46:58,201-INFO: epoch:66  train step:160  loss:  2.0567 lr: 0.000964 elapse: 0.698s\n",
      "2021-11-19 19:47:05,315-INFO: epoch:66  train step:170  loss:  0.8217 lr: 0.000964 elapse: 0.712s\n",
      "2021-11-19 19:47:12,402-INFO: epoch:66  train step:180  loss:  1.9610 lr: 0.000964 elapse: 0.700s\n",
      "2021-11-19 19:47:19,409-INFO: epoch:66  train step:190  loss:  1.0189 lr: 0.000964 elapse: 0.705s\n",
      "2021-11-19 19:47:26,409-INFO: epoch:66  train step:200  loss:  0.8065 lr: 0.000964 elapse: 0.666s\n",
      "2021-11-19 19:47:33,104-INFO: epoch:66  train step:210  loss:  0.8423 lr: 0.000964 elapse: 0.668s\n",
      "2021-11-19 19:47:39,873-INFO: epoch:66  train step:220  loss:  1.0204 lr: 0.000963 elapse: 0.668s\n",
      "2021-11-19 19:47:46,687-INFO: epoch:66  train step:230  loss:  2.0703 lr: 0.000963 elapse: 0.671s\n",
      "2021-11-19 19:47:52,988-INFO: END epoch:66  train loss_avg:  1.2898  elapse_sum: 178.626s\n",
      "2021-11-19 19:48:01,394-INFO: epoch:67  train step:0    loss:  0.9730 lr: 0.000963 elapse: 8.404s\n",
      "2021-11-19 19:48:10,504-INFO: epoch:67  train step:10   loss:  0.7990 lr: 0.000963 elapse: 0.698s\n",
      "2021-11-19 19:48:17,602-INFO: epoch:67  train step:20   loss:  0.9411 lr: 0.000963 elapse: 0.696s\n",
      "2021-11-19 19:48:24,625-INFO: epoch:67  train step:30   loss:  0.8544 lr: 0.000963 elapse: 0.705s\n",
      "2021-11-19 19:48:31,703-INFO: epoch:67  train step:40   loss:  1.0739 lr: 0.000963 elapse: 0.710s\n",
      "2021-11-19 19:48:38,692-INFO: epoch:67  train step:50   loss:  0.8447 lr: 0.000963 elapse: 0.700s\n",
      "2021-11-19 19:48:45,702-INFO: epoch:67  train step:60   loss:  1.9051 lr: 0.000963 elapse: 0.697s\n",
      "2021-11-19 19:48:52,801-INFO: epoch:67  train step:70   loss:  2.6471 lr: 0.000963 elapse: 0.695s\n",
      "2021-11-19 19:48:59,901-INFO: epoch:67  train step:80   loss:  1.0610 lr: 0.000963 elapse: 0.697s\n",
      "2021-11-19 19:49:07,245-INFO: epoch:67  train step:90   loss:  0.9971 lr: 0.000963 elapse: 0.734s\n",
      "2021-11-19 19:49:14,491-INFO: epoch:67  train step:100  loss:  1.0727 lr: 0.000963 elapse: 0.699s\n",
      "2021-11-19 19:49:21,518-INFO: epoch:67  train step:110  loss:  2.0524 lr: 0.000963 elapse: 0.707s\n",
      "2021-11-19 19:49:28,723-INFO: epoch:67  train step:120  loss:  0.9809 lr: 0.000963 elapse: 0.721s\n",
      "2021-11-19 19:49:35,906-INFO: epoch:67  train step:130  loss:  2.1206 lr: 0.000963 elapse: 0.703s\n",
      "2021-11-19 19:49:43,191-INFO: epoch:67  train step:140  loss:  1.7063 lr: 0.000963 elapse: 0.697s\n",
      "2021-11-19 19:49:50,202-INFO: epoch:67  train step:150  loss:  1.2549 lr: 0.000963 elapse: 0.700s\n",
      "2021-11-19 19:49:57,304-INFO: epoch:67  train step:160  loss:  1.5238 lr: 0.000963 elapse: 0.686s\n",
      "2021-11-19 19:50:04,402-INFO: epoch:67  train step:170  loss:  2.3506 lr: 0.000963 elapse: 0.687s\n",
      "2021-11-19 19:50:11,760-INFO: epoch:67  train step:180  loss:  1.8348 lr: 0.000962 elapse: 0.756s\n",
      "2021-11-19 19:50:19,001-INFO: epoch:67  train step:190  loss:  1.6092 lr: 0.000962 elapse: 0.696s\n",
      "2021-11-19 19:50:26,015-INFO: epoch:67  train step:200  loss:  0.8790 lr: 0.000962 elapse: 0.675s\n",
      "2021-11-19 19:50:32,728-INFO: epoch:67  train step:210  loss:  1.0392 lr: 0.000962 elapse: 0.676s\n",
      "2021-11-19 19:50:39,576-INFO: epoch:67  train step:220  loss:  0.8904 lr: 0.000962 elapse: 0.667s\n",
      "2021-11-19 19:50:46,566-INFO: epoch:67  train step:230  loss:  1.1278 lr: 0.000962 elapse: 0.682s\n",
      "2021-11-19 19:50:52,741-INFO: END epoch:67  train loss_avg:  1.4067  elapse_sum: 179.750s\n",
      "2021-11-19 19:51:00,593-INFO: epoch:68  train step:0    loss:  1.9105 lr: 0.000962 elapse: 7.850s\n",
      "2021-11-19 19:51:09,702-INFO: epoch:68  train step:10   loss:  1.0559 lr: 0.000962 elapse: 0.699s\n",
      "2021-11-19 19:51:17,044-INFO: epoch:68  train step:20   loss:  0.8191 lr: 0.000962 elapse: 0.741s\n",
      "2021-11-19 19:51:24,291-INFO: epoch:68  train step:30   loss:  2.4665 lr: 0.000962 elapse: 0.699s\n",
      "2021-11-19 19:51:31,305-INFO: epoch:68  train step:40   loss:  0.7914 lr: 0.000962 elapse: 0.703s\n",
      "2021-11-19 19:51:38,426-INFO: epoch:68  train step:50   loss:  0.8044 lr: 0.000962 elapse: 0.720s\n",
      "2021-11-19 19:51:45,602-INFO: epoch:68  train step:60   loss:  0.8740 lr: 0.000962 elapse: 0.709s\n",
      "2021-11-19 19:51:52,703-INFO: epoch:68  train step:70   loss:  1.8141 lr: 0.000962 elapse: 0.700s\n",
      "2021-11-19 19:51:59,791-INFO: epoch:68  train step:80   loss:  0.8113 lr: 0.000962 elapse: 0.698s\n",
      "2021-11-19 19:52:06,902-INFO: epoch:68  train step:90   loss:  0.8540 lr: 0.000962 elapse: 0.700s\n",
      "2021-11-19 19:52:14,002-INFO: epoch:68  train step:100  loss:  1.5200 lr: 0.000962 elapse: 0.698s\n",
      "2021-11-19 19:52:21,251-INFO: epoch:68  train step:110  loss:  0.8796 lr: 0.000962 elapse: 0.759s\n",
      "2021-11-19 19:52:28,491-INFO: epoch:68  train step:120  loss:  0.8586 lr: 0.000962 elapse: 0.699s\n",
      "2021-11-19 19:52:35,591-INFO: epoch:68  train step:130  loss:  1.1635 lr: 0.000962 elapse: 0.698s\n",
      "2021-11-19 19:52:42,603-INFO: epoch:68  train step:140  loss:  2.4437 lr: 0.000961 elapse: 0.711s\n",
      "2021-11-19 19:52:49,694-INFO: epoch:68  train step:150  loss:  0.8796 lr: 0.000961 elapse: 0.701s\n",
      "2021-11-19 19:52:56,891-INFO: epoch:68  train step:160  loss:  1.6486 lr: 0.000961 elapse: 0.698s\n",
      "2021-11-19 19:53:04,001-INFO: epoch:68  train step:170  loss:  1.1827 lr: 0.000961 elapse: 0.698s\n",
      "2021-11-19 19:53:11,004-INFO: epoch:68  train step:180  loss:  1.9677 lr: 0.000961 elapse: 0.699s\n",
      "2021-11-19 19:53:18,003-INFO: epoch:68  train step:190  loss:  1.1282 lr: 0.000961 elapse: 0.700s\n",
      "2021-11-19 19:53:25,065-INFO: epoch:68  train step:200  loss:  1.4401 lr: 0.000961 elapse: 0.684s\n",
      "2021-11-19 19:53:31,755-INFO: epoch:68  train step:210  loss:  2.5106 lr: 0.000961 elapse: 0.675s\n",
      "2021-11-19 19:53:38,534-INFO: epoch:68  train step:220  loss:  2.2927 lr: 0.000961 elapse: 0.676s\n",
      "2021-11-19 19:53:45,502-INFO: epoch:68  train step:230  loss:  2.1554 lr: 0.000961 elapse: 0.700s\n",
      "2021-11-19 19:53:51,572-INFO: END epoch:68  train loss_avg:  1.3486  elapse_sum: 178.828s\n",
      "2021-11-19 19:53:59,392-INFO: epoch:69  train step:0    loss:  0.9228 lr: 0.000961 elapse: 7.819s\n",
      "2021-11-19 19:54:08,594-INFO: epoch:69  train step:10   loss:  0.9200 lr: 0.000961 elapse: 0.702s\n",
      "2021-11-19 19:54:15,604-INFO: epoch:69  train step:20   loss:  0.9982 lr: 0.000961 elapse: 0.692s\n",
      "2021-11-19 19:54:22,693-INFO: epoch:69  train step:30   loss:  1.5201 lr: 0.000961 elapse: 0.701s\n",
      "2021-11-19 19:54:29,948-INFO: epoch:69  train step:40   loss:  2.0521 lr: 0.000961 elapse: 0.756s\n",
      "2021-11-19 19:54:37,104-INFO: epoch:69  train step:50   loss:  1.5606 lr: 0.000961 elapse: 0.700s\n",
      "2021-11-19 19:54:44,102-INFO: epoch:69  train step:60   loss:  1.0679 lr: 0.000961 elapse: 0.697s\n",
      "2021-11-19 19:54:51,205-INFO: epoch:69  train step:70   loss:  1.3216 lr: 0.000961 elapse: 0.697s\n",
      "2021-11-19 19:54:58,311-INFO: epoch:69  train step:80   loss:  0.8561 lr: 0.000961 elapse: 0.707s\n",
      "2021-11-19 19:55:05,503-INFO: epoch:69  train step:90   loss:  1.5464 lr: 0.000960 elapse: 0.701s\n",
      "2021-11-19 19:55:12,705-INFO: epoch:69  train step:100  loss:  0.7963 lr: 0.000960 elapse: 0.702s\n",
      "2021-11-19 19:55:19,807-INFO: epoch:69  train step:110  loss:  1.9576 lr: 0.000960 elapse: 0.715s\n",
      "2021-11-19 19:55:26,891-INFO: epoch:69  train step:120  loss:  0.8860 lr: 0.000960 elapse: 0.697s\n",
      "2021-11-19 19:55:34,133-INFO: epoch:69  train step:130  loss:  0.8533 lr: 0.000960 elapse: 0.729s\n",
      "2021-11-19 19:55:41,401-INFO: epoch:69  train step:140  loss:  1.4900 lr: 0.000960 elapse: 0.697s\n",
      "2021-11-19 19:55:48,491-INFO: epoch:69  train step:150  loss:  2.6807 lr: 0.000960 elapse: 0.696s\n",
      "2021-11-19 19:55:55,606-INFO: epoch:69  train step:160  loss:  0.8844 lr: 0.000960 elapse: 0.702s\n",
      "2021-11-19 19:56:02,703-INFO: epoch:69  train step:170  loss:  0.9265 lr: 0.000960 elapse: 0.700s\n",
      "2021-11-19 19:56:09,802-INFO: epoch:69  train step:180  loss:  1.6258 lr: 0.000960 elapse: 0.699s\n",
      "2021-11-19 19:56:16,903-INFO: epoch:69  train step:190  loss:  0.8554 lr: 0.000960 elapse: 0.700s\n",
      "2021-11-19 19:56:23,909-INFO: epoch:69  train step:200  loss:  0.9604 lr: 0.000960 elapse: 0.672s\n",
      "2021-11-19 19:56:30,617-INFO: epoch:69  train step:210  loss:  0.7981 lr: 0.000960 elapse: 0.666s\n",
      "2021-11-19 19:56:37,422-INFO: epoch:69  train step:220  loss:  0.8108 lr: 0.000960 elapse: 0.666s\n",
      "2021-11-19 19:56:44,360-INFO: epoch:69  train step:230  loss:  1.2786 lr: 0.000960 elapse: 0.668s\n",
      "2021-11-19 19:56:50,379-INFO: END epoch:69  train loss_avg:  1.3196  elapse_sum: 178.805s\n",
      "2021-11-19 19:56:58,005-INFO: epoch:70  train step:0    loss:  2.4575 lr: 0.000960 elapse: 7.624s\n",
      "2021-11-19 19:57:07,324-INFO: epoch:70  train step:10   loss:  1.0321 lr: 0.000960 elapse: 0.719s\n",
      "2021-11-19 19:57:14,525-INFO: epoch:70  train step:20   loss:  0.9752 lr: 0.000960 elapse: 0.720s\n",
      "2021-11-19 19:57:21,601-INFO: epoch:70  train step:30   loss:  1.8977 lr: 0.000960 elapse: 0.696s\n",
      "2021-11-19 19:57:28,691-INFO: epoch:70  train step:40   loss:  0.8686 lr: 0.000960 elapse: 0.699s\n",
      "2021-11-19 19:57:35,701-INFO: epoch:70  train step:50   loss:  1.1082 lr: 0.000959 elapse: 0.693s\n",
      "2021-11-19 19:57:43,036-INFO: epoch:70  train step:60   loss:  1.9186 lr: 0.000959 elapse: 0.732s\n",
      "2021-11-19 19:57:50,226-INFO: epoch:70  train step:70   loss:  0.9342 lr: 0.000959 elapse: 0.720s\n",
      "2021-11-19 19:57:57,391-INFO: epoch:70  train step:80   loss:  1.1424 lr: 0.000959 elapse: 0.699s\n",
      "2021-11-19 19:58:04,403-INFO: epoch:70  train step:90   loss:  0.8434 lr: 0.000959 elapse: 0.697s\n",
      "2021-11-19 19:58:11,402-INFO: epoch:70  train step:100  loss:  2.1663 lr: 0.000959 elapse: 0.696s\n",
      "2021-11-19 19:58:18,503-INFO: epoch:70  train step:110  loss:  0.9441 lr: 0.000959 elapse: 0.700s\n",
      "2021-11-19 19:58:25,602-INFO: epoch:70  train step:120  loss:  0.8788 lr: 0.000959 elapse: 0.698s\n",
      "2021-11-19 19:58:32,728-INFO: epoch:70  train step:130  loss:  2.2260 lr: 0.000959 elapse: 0.726s\n",
      "2021-11-19 19:58:39,902-INFO: epoch:70  train step:140  loss:  0.8672 lr: 0.000959 elapse: 0.687s\n",
      "2021-11-19 19:58:47,291-INFO: epoch:70  train step:150  loss:  0.9489 lr: 0.000959 elapse: 0.788s\n",
      "2021-11-19 19:58:54,502-INFO: epoch:70  train step:160  loss:  1.6313 lr: 0.000959 elapse: 0.699s\n",
      "2021-11-19 19:59:01,591-INFO: epoch:70  train step:170  loss:  1.1058 lr: 0.000959 elapse: 0.699s\n",
      "2021-11-19 19:59:08,618-INFO: epoch:70  train step:180  loss:  0.8326 lr: 0.000959 elapse: 0.715s\n",
      "2021-11-19 19:59:15,691-INFO: epoch:70  train step:190  loss:  1.5098 lr: 0.000959 elapse: 0.769s\n",
      "2021-11-19 19:59:22,636-INFO: epoch:70  train step:200  loss:  0.8242 lr: 0.000959 elapse: 0.676s\n",
      "2021-11-19 19:59:29,327-INFO: epoch:70  train step:210  loss:  2.4553 lr: 0.000959 elapse: 0.667s\n",
      "2021-11-19 19:59:36,129-INFO: epoch:70  train step:220  loss:  2.1557 lr: 0.000959 elapse: 0.667s\n",
      "2021-11-19 19:59:43,118-INFO: epoch:70  train step:230  loss:  1.1765 lr: 0.000959 elapse: 0.667s\n",
      "2021-11-19 19:59:49,144-INFO: END epoch:70  train loss_avg:  1.3259  elapse_sum: 178.762s\n",
      "2021-11-19 19:59:54,593-INFO: epoch:70  valid step:0    loss:  1.2600 top1: 0.8125 top5: 1.0000 elapse: 5.447s\n",
      "2021-11-19 19:59:59,649-INFO: epoch:70  valid step:10   loss:  1.5676 top1: 0.7500 top5: 0.8750 elapse: 0.229s\n",
      "2021-11-19 20:00:02,054-INFO: epoch:70  valid step:20   loss:  1.1268 top1: 0.8750 top5: 1.0000 elapse: 0.234s\n",
      "2021-11-19 20:00:04,169-INFO: epoch:70  valid step:30   loss:  1.4302 top1: 0.8125 top5: 0.9375 elapse: 0.188s\n",
      "2021-11-19 20:00:06,077-INFO: epoch:70  valid step:40   loss:  1.1876 top1: 0.8750 top5: 1.0000 elapse: 0.189s\n",
      "2021-11-19 20:00:07,969-INFO: epoch:70  valid step:50   loss:  1.2573 top1: 0.8750 top5: 0.9375 elapse: 0.188s\n",
      "2021-11-19 20:00:09,868-INFO: epoch:70  valid step:60   loss:  1.6476 top1: 0.8125 top5: 0.8750 elapse: 0.187s\n",
      "2021-11-19 20:00:11,835-INFO: epoch:70  valid step:70   loss:  1.3385 top1: 0.8125 top5: 0.9375 elapse: 0.280s\n",
      "2021-11-19 20:00:13,720-INFO: epoch:70  valid step:80   loss:  1.4904 top1: 0.7500 top5: 0.8125 elapse: 0.187s\n",
      "2021-11-19 20:00:15,674-INFO: END epoch:70  valid loss_avg:  1.2256 top1_avg: 0.8542 top5_avg: 0.9618 elapse_sum: 26.528s\n",
      "2021-11-19 20:00:16,683-INFO: Already save model in ./output/EfficientNetB3/70\n",
      "2021-11-19 20:00:24,692-INFO: epoch:71  train step:0    loss:  1.3662 lr: 0.000958 elapse: 8.008s\n",
      "2021-11-19 20:00:33,601-INFO: epoch:71  train step:10   loss:  2.2733 lr: 0.000958 elapse: 0.709s\n",
      "2021-11-19 20:00:40,691-INFO: epoch:71  train step:20   loss:  0.9164 lr: 0.000958 elapse: 0.698s\n",
      "2021-11-19 20:00:47,816-INFO: epoch:71  train step:30   loss:  1.0694 lr: 0.000958 elapse: 0.712s\n",
      "2021-11-19 20:00:54,992-INFO: epoch:71  train step:40   loss:  1.1679 lr: 0.000958 elapse: 0.697s\n",
      "2021-11-19 20:01:02,327-INFO: epoch:71  train step:50   loss:  2.3817 lr: 0.000958 elapse: 0.719s\n",
      "2021-11-19 20:01:09,401-INFO: epoch:71  train step:60   loss:  0.9002 lr: 0.000958 elapse: 0.698s\n",
      "2021-11-19 20:01:16,403-INFO: epoch:71  train step:70   loss:  2.0044 lr: 0.000958 elapse: 0.700s\n",
      "2021-11-19 20:01:23,591-INFO: epoch:71  train step:80   loss:  1.1838 lr: 0.000958 elapse: 0.698s\n",
      "2021-11-19 20:01:30,614-INFO: epoch:71  train step:90   loss:  0.9762 lr: 0.000958 elapse: 0.712s\n",
      "2021-11-19 20:01:37,604-INFO: epoch:71  train step:100  loss:  1.0559 lr: 0.000958 elapse: 0.702s\n",
      "2021-11-19 20:01:44,806-INFO: epoch:71  train step:110  loss:  1.1516 lr: 0.000958 elapse: 0.702s\n",
      "2021-11-19 20:01:52,102-INFO: epoch:71  train step:120  loss:  0.7852 lr: 0.000958 elapse: 0.710s\n",
      "2021-11-19 20:01:59,102-INFO: epoch:71  train step:130  loss:  1.3677 lr: 0.000958 elapse: 0.699s\n",
      "2021-11-19 20:02:06,592-INFO: epoch:71  train step:140  loss:  2.1365 lr: 0.000958 elapse: 0.746s\n",
      "2021-11-19 20:02:13,816-INFO: epoch:71  train step:150  loss:  2.2077 lr: 0.000958 elapse: 0.710s\n",
      "2021-11-19 20:02:20,802-INFO: epoch:71  train step:160  loss:  2.5776 lr: 0.000958 elapse: 0.698s\n",
      "2021-11-19 20:02:27,891-INFO: epoch:71  train step:170  loss:  0.9173 lr: 0.000958 elapse: 0.772s\n",
      "2021-11-19 20:02:34,917-INFO: epoch:71  train step:180  loss:  1.5950 lr: 0.000958 elapse: 0.710s\n",
      "2021-11-19 20:02:41,999-INFO: epoch:71  train step:190  loss:  2.3218 lr: 0.000957 elapse: 0.707s\n",
      "2021-11-19 20:02:48,954-INFO: epoch:71  train step:200  loss:  0.9744 lr: 0.000957 elapse: 0.676s\n",
      "2021-11-19 20:02:55,690-INFO: epoch:71  train step:210  loss:  0.9021 lr: 0.000957 elapse: 0.667s\n",
      "2021-11-19 20:03:02,658-INFO: epoch:71  train step:220  loss:  0.9786 lr: 0.000957 elapse: 0.666s\n",
      "2021-11-19 20:03:09,600-INFO: epoch:71  train step:230  loss:  2.3047 lr: 0.000957 elapse: 0.668s\n",
      "2021-11-19 20:03:15,642-INFO: END epoch:71  train loss_avg:  1.3593  elapse_sum: 178.955s\n",
      "2021-11-19 20:03:23,707-INFO: epoch:72  train step:0    loss:  0.8700 lr: 0.000957 elapse: 8.063s\n",
      "2021-11-19 20:03:32,501-INFO: epoch:72  train step:10   loss:  1.1769 lr: 0.000957 elapse: 0.696s\n",
      "2021-11-19 20:03:39,591-INFO: epoch:72  train step:20   loss:  1.6100 lr: 0.000957 elapse: 0.765s\n",
      "2021-11-19 20:03:46,591-INFO: epoch:72  train step:30   loss:  0.9409 lr: 0.000957 elapse: 0.689s\n",
      "2021-11-19 20:03:53,802-INFO: epoch:72  train step:40   loss:  0.8818 lr: 0.000957 elapse: 0.708s\n",
      "2021-11-19 20:04:00,791-INFO: epoch:72  train step:50   loss:  2.6384 lr: 0.000957 elapse: 0.690s\n",
      "2021-11-19 20:04:07,802-INFO: epoch:72  train step:60   loss:  2.1067 lr: 0.000957 elapse: 0.698s\n",
      "2021-11-19 20:04:15,203-INFO: epoch:72  train step:70   loss:  1.1277 lr: 0.000957 elapse: 0.699s\n",
      "2021-11-19 20:04:22,292-INFO: epoch:72  train step:80   loss:  1.6677 lr: 0.000957 elapse: 0.698s\n",
      "2021-11-19 20:04:29,302-INFO: epoch:72  train step:90   loss:  1.8942 lr: 0.000957 elapse: 0.699s\n",
      "2021-11-19 20:04:36,302-INFO: epoch:72  train step:100  loss:  0.8646 lr: 0.000957 elapse: 0.697s\n",
      "2021-11-19 20:04:43,324-INFO: epoch:72  train step:110  loss:  2.5004 lr: 0.000957 elapse: 0.718s\n",
      "2021-11-19 20:04:50,403-INFO: epoch:72  train step:120  loss:  2.0225 lr: 0.000957 elapse: 0.700s\n",
      "2021-11-19 20:04:57,515-INFO: epoch:72  train step:130  loss:  0.8259 lr: 0.000956 elapse: 0.711s\n",
      "2021-11-19 20:05:04,602-INFO: epoch:72  train step:140  loss:  1.1964 lr: 0.000956 elapse: 0.696s\n",
      "2021-11-19 20:05:11,703-INFO: epoch:72  train step:150  loss:  0.9831 lr: 0.000956 elapse: 0.699s\n",
      "2021-11-19 20:05:19,102-INFO: epoch:72  train step:160  loss:  0.8377 lr: 0.000956 elapse: 0.699s\n",
      "2021-11-19 20:05:26,201-INFO: epoch:72  train step:170  loss:  0.8166 lr: 0.000956 elapse: 0.780s\n",
      "2021-11-19 20:05:33,302-INFO: epoch:72  train step:180  loss:  0.8223 lr: 0.000956 elapse: 0.763s\n",
      "2021-11-19 20:05:40,402-INFO: epoch:72  train step:190  loss:  0.9083 lr: 0.000956 elapse: 0.698s\n",
      "2021-11-19 20:05:47,404-INFO: epoch:72  train step:200  loss:  2.1384 lr: 0.000956 elapse: 0.668s\n",
      "2021-11-19 20:05:54,090-INFO: epoch:72  train step:210  loss:  1.1357 lr: 0.000956 elapse: 0.667s\n",
      "2021-11-19 20:06:01,000-INFO: epoch:72  train step:220  loss:  1.7347 lr: 0.000956 elapse: 0.666s\n",
      "2021-11-19 20:06:07,975-INFO: epoch:72  train step:230  loss:  1.6350 lr: 0.000956 elapse: 0.666s\n",
      "2021-11-19 20:06:13,953-INFO: END epoch:72  train loss_avg:  1.3695  elapse_sum: 178.308s\n",
      "2021-11-19 20:06:22,792-INFO: epoch:73  train step:0    loss:  0.8368 lr: 0.000956 elapse: 8.838s\n",
      "2021-11-19 20:06:31,501-INFO: epoch:73  train step:10   loss:  0.8698 lr: 0.000956 elapse: 0.709s\n",
      "2021-11-19 20:06:38,523-INFO: epoch:73  train step:20   loss:  1.1323 lr: 0.000956 elapse: 0.720s\n",
      "2021-11-19 20:06:45,601-INFO: epoch:73  train step:30   loss:  0.9012 lr: 0.000956 elapse: 0.709s\n",
      "2021-11-19 20:06:52,734-INFO: epoch:73  train step:40   loss:  1.7917 lr: 0.000956 elapse: 0.716s\n",
      "2021-11-19 20:06:59,803-INFO: epoch:73  train step:50   loss:  1.8626 lr: 0.000956 elapse: 0.700s\n",
      "2021-11-19 20:07:06,826-INFO: epoch:73  train step:60   loss:  1.6993 lr: 0.000956 elapse: 0.723s\n",
      "2021-11-19 20:07:13,905-INFO: epoch:73  train step:70   loss:  1.0316 lr: 0.000956 elapse: 0.700s\n",
      "2021-11-19 20:07:21,001-INFO: epoch:73  train step:80   loss:  1.0110 lr: 0.000955 elapse: 0.696s\n",
      "2021-11-19 20:07:28,191-INFO: epoch:73  train step:90   loss:  1.8054 lr: 0.000955 elapse: 0.789s\n",
      "2021-11-19 20:07:35,405-INFO: epoch:73  train step:100  loss:  2.5527 lr: 0.000955 elapse: 0.701s\n",
      "2021-11-19 20:07:42,602-INFO: epoch:73  train step:110  loss:  1.0547 lr: 0.000955 elapse: 0.710s\n",
      "2021-11-19 20:07:49,604-INFO: epoch:73  train step:120  loss:  1.0620 lr: 0.000955 elapse: 0.701s\n",
      "2021-11-19 20:07:56,801-INFO: epoch:73  train step:130  loss:  1.2853 lr: 0.000955 elapse: 0.708s\n",
      "2021-11-19 20:08:03,905-INFO: epoch:73  train step:140  loss:  0.7921 lr: 0.000955 elapse: 0.703s\n",
      "2021-11-19 20:08:11,103-INFO: epoch:73  train step:150  loss:  0.9468 lr: 0.000955 elapse: 0.779s\n",
      "2021-11-19 20:08:18,202-INFO: epoch:73  train step:160  loss:  0.9303 lr: 0.000955 elapse: 0.698s\n",
      "2021-11-19 20:08:25,204-INFO: epoch:73  train step:170  loss:  1.0316 lr: 0.000955 elapse: 0.701s\n",
      "2021-11-19 20:08:32,302-INFO: epoch:73  train step:180  loss:  2.0474 lr: 0.000955 elapse: 0.697s\n",
      "2021-11-19 20:08:39,605-INFO: epoch:73  train step:190  loss:  1.6192 lr: 0.000955 elapse: 0.702s\n",
      "2021-11-19 20:08:46,542-INFO: epoch:73  train step:200  loss:  0.8706 lr: 0.000955 elapse: 0.676s\n",
      "2021-11-19 20:08:53,294-INFO: epoch:73  train step:210  loss:  1.2319 lr: 0.000955 elapse: 0.667s\n",
      "2021-11-19 20:09:00,223-INFO: epoch:73  train step:220  loss:  1.0514 lr: 0.000955 elapse: 0.669s\n",
      "2021-11-19 20:09:07,159-INFO: epoch:73  train step:230  loss:  2.0057 lr: 0.000955 elapse: 0.668s\n",
      "2021-11-19 20:09:13,327-INFO: END epoch:73  train loss_avg:  1.3061  elapse_sum: 179.371s\n",
      "2021-11-19 20:09:20,993-INFO: epoch:74  train step:0    loss:  0.8486 lr: 0.000955 elapse: 7.664s\n"
     ]
    }
   ],
   "source": [
    "#EfficientNetB3\n",
    "!python -m paddle.distributed.launch tools/train.py -c ./configs/EfficientNet/EfficientNetB3.yaml -o pretrained_model=./output/EfficientNetB3/best_model/ppcls    #基于已训练的部分继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-19T02:18:01.097480Z",
     "iopub.status.busy": "2021-11-19T02:18:01.096458Z",
     "iopub.status.idle": "2021-11-19T02:18:35.972800Z",
     "shell.execute_reply": "2021-11-19T02:18:35.971969Z",
     "shell.execute_reply.started": "2021-11-19T02:18:01.097435Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  Configuration Arguments -----------\n",
      "cluster_node_ips: 127.0.0.1\n",
      "log_dir: None\n",
      "log_level: 20\n",
      "node_ip: 127.0.0.1\n",
      "print_config: True\n",
      "selected_gpus: None\n",
      "started_port: None\n",
      "training_script: tools/eval.py\n",
      "training_script_args: ['-c', './configs/eval-EfficientNetB3.yaml', '-o', 'ARCHITECTURE.name=EfficientNetB3', '-o', 'pretrained_model=./output/EfficientNetB3/best_model/ppcls']\n",
      "use_paddlecloud: False\n",
      "------------------------------------------------\n",
      "INFO 2021-11-19 10:18:02,579 launch.py:210] get cluster from args:job_server:None pods:['rank:0 id:None addr:127.0.0.1 port:None visible_gpu:[] trainers:[\"gpu:[\\'0\\'] endpoint:127.0.0.1:44837 rank:0\"]'] job_stage_flag:None hdfs:None\n",
      "INFO 2021-11-19 10:18:02,580 utils.py:367] start trainer proc:['/opt/conda/envs/python35-paddle120-env/bin/python', '-u', 'tools/eval.py', '-c', './configs/eval-EfficientNetB3.yaml', '-o', 'ARCHITECTURE.name=EfficientNetB3', '-o', 'pretrained_model=./output/EfficientNetB3/best_model/ppcls'] env:{'FLAGS_selected_gpus': '0', 'PADDLE_TRAINER_ID': '0', 'PADDLE_CURRENT_ENDPOINT': '127.0.0.1:44837', 'PADDLE_TRAINERS_NUM': '1', 'PADDLE_TRAINER_ENDPOINTS': '127.0.0.1:44837'}\n",
      "2021-11-19 10:18:03,665-INFO: \n",
      "===========================================================\n",
      "==        PaddleClas is powered by PaddlePaddle !        ==\n",
      "===========================================================\n",
      "==                                                       ==\n",
      "==   For more info please go to the following website.   ==\n",
      "==                                                       ==\n",
      "==       https://github.com/PaddlePaddle/PaddleClas      ==\n",
      "===========================================================\n",
      "\n",
      "2021-11-19 10:18:03,665-INFO: ARCHITECTURE : \n",
      "2021-11-19 10:18:03,665-INFO:     name : EfficientNetB3\n",
      "2021-11-19 10:18:03,665-INFO: ------------------------------------------------------------\n",
      "2021-11-19 10:18:03,665-INFO: VALID : \n",
      "2021-11-19 10:18:03,665-INFO:     batch_size : 16\n",
      "2021-11-19 10:18:03,665-INFO:     data_dir : /home/aistudio/work/datasets/train/\n",
      "2021-11-19 10:18:03,665-INFO:     file_list : /home/aistudio/work/datasets/eval_clas.txt\n",
      "2021-11-19 10:18:03,665-INFO:     num_workers : 4\n",
      "2021-11-19 10:18:03,665-INFO:     shuffle_seed : 0\n",
      "2021-11-19 10:18:03,665-INFO:     transforms : \n",
      "2021-11-19 10:18:03,665-INFO:         DecodeImage : \n",
      "2021-11-19 10:18:03,665-INFO:             channel_first : False\n",
      "2021-11-19 10:18:03,666-INFO:             to_np : False\n",
      "2021-11-19 10:18:03,666-INFO:             to_rgb : True\n",
      "2021-11-19 10:18:03,666-INFO:         ResizeImage : \n",
      "2021-11-19 10:18:03,666-INFO:             resize_short : 660\n",
      "2021-11-19 10:18:03,666-INFO:         CropImage : \n",
      "2021-11-19 10:18:03,666-INFO:             size : 600\n",
      "2021-11-19 10:18:03,666-INFO:         NormalizeImage : \n",
      "2021-11-19 10:18:03,666-INFO:             mean : [0.485, 0.456, 0.406]\n",
      "2021-11-19 10:18:03,666-INFO:             order : \n",
      "2021-11-19 10:18:03,666-INFO:             scale : 1.0/255.0\n",
      "2021-11-19 10:18:03,666-INFO:             std : [0.229, 0.224, 0.225]\n",
      "2021-11-19 10:18:03,666-INFO:         ToCHWImage : None\n",
      "2021-11-19 10:18:03,666-INFO: ------------------------------------------------------------\n",
      "2021-11-19 10:18:03,666-INFO: classes_num : 49\n",
      "2021-11-19 10:18:03,666-INFO: image_shape : [3, 600, 600]\n",
      "2021-11-19 10:18:03,666-INFO: mode : valid\n",
      "2021-11-19 10:18:03,666-INFO: pretrained_model : ./output/EfficientNetB3/best_model/ppcls\n",
      "2021-11-19 10:18:03,666-INFO: topk : 5\n",
      "2021-11-19 10:18:03,666-INFO: total_images : 800\n",
      "W1119 10:18:04.526522 20121 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0\n",
      "W1119 10:18:04.531919 20121 device_context.cc:260] device: 0, cuDNN Version: 7.6.\n",
      "2021-11-19 10:18:07,753-INFO: Loading parameters from ./output/EfficientNetB3/best_model/ppcls...\n",
      "2021-11-19 10:18:08,282-INFO: Finish initing model from ['./output/EfficientNetB3/best_model/ppcls']\n",
      "2021-11-19 10:18:09,966-INFO: eval step:0    loss:  0.7959 top1: 0.8125 top5: 0.9375 elapse: 1.663s\n",
      "2021-11-19 10:18:13,949-INFO: eval step:10   loss:  1.0928 top1: 0.6875 top5: 0.8750 elapse: 0.334s\n",
      "2021-11-19 10:18:17,847-INFO: eval step:20   loss:  0.8922 top1: 0.7500 top5: 0.9375 elapse: 0.493s\n",
      "2021-11-19 10:18:21,063-INFO: eval step:30   loss:  0.6324 top1: 0.8750 top5: 1.0000 elapse: 0.192s\n",
      "2021-11-19 10:18:22,994-INFO: eval step:40   loss:  0.8731 top1: 0.8125 top5: 0.9375 elapse: 0.191s\n",
      "2021-11-19 10:18:24,977-INFO: eval step:50   loss:  1.1527 top1: 0.6875 top5: 0.8750 elapse: 0.238s\n",
      "2021-11-19 10:18:26,916-INFO: eval step:60   loss:  1.0959 top1: 0.7500 top5: 0.8750 elapse: 0.197s\n",
      "2021-11-19 10:18:28,834-INFO: eval step:70   loss:  0.6534 top1: 0.8750 top5: 1.0000 elapse: 0.191s\n",
      "2021-11-19 10:18:30,756-INFO: eval step:80   loss:  1.1337 top1: 0.6875 top5: 0.9375 elapse: 0.193s\n",
      "2021-11-19 10:18:32,491-INFO: END eval loss_avg:  0.7786 top1_avg: 0.8271 top5_avg: 0.9507 elapse_sum: 24.187ss\n",
      "INFO 2021-11-19 10:18:35,641 launch.py:223] Local procs complete, POD info:rank:0 id:None addr:127.0.0.1 port:None visible_gpu:[] trainers:[\"gpu:['0'] endpoint:127.0.0.1:44837 rank:0\"]\n"
     ]
    }
   ],
   "source": [
    "# 验证数据\n",
    "#1.EfficientNetB4\n",
    "# pretrained_model -- 训练好的模型（非导出的预测模型）\n",
    "# !python -m paddle.distributed.launch \\\n",
    "#     tools/eval.py \\\n",
    "#     -c ./configs/eval.yaml \\\n",
    "#     -o ARCHITECTURE.name=\"EfficientNetB4\" \\\n",
    "#     -o pretrained_model=./output/EfficientNetB4/best_model/ppcls\n",
    "#2.ResNeXt101_32x8d_wsl\n",
    "!python -m paddle.distributed.launch \\\n",
    "    tools/eval.py \\\n",
    "    -c ./configs/eval-EfficientNetB3.yaml \\\n",
    "    -o ARCHITECTURE.name=\"EfficientNetB3\" \\\n",
    "    -o pretrained_model=./output/EfficientNetB3/best_model/ppcls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、训练结束后，对模型文件的处理\n",
    "\n",
    "1. 首先在保存文件前，将best模型取出，再放回 -- 然后执行模型的保存\n",
    "2. 将保存的模型，从默认路径移动到指定目录下 -- 方便使用和保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T13:57:22.341661Z",
     "iopub.status.busy": "2021-11-18T13:57:22.340859Z",
     "iopub.status.idle": "2021-11-18T13:57:23.962805Z",
     "shell.execute_reply": "2021-11-18T13:57:23.961956Z",
     "shell.execute_reply.started": "2021-11-18T13:57:22.341625Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: 在'/home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl' 后缺少了要操作的目标文件\n",
      "Try 'mv --help' for more information.\n",
      "mkdir: 无法创建目录\"/home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl\": 文件已存在\n"
     ]
    }
   ],
   "source": [
    "# # 1.0将训练好的best模型移出 -- 删除原来的文件夹 -- 只需修改相应的模型路径即可\n",
    "# # /home/aistudio/PaddleClas/output/ResNeXt152_vd_32x4d/best_model  中的 ResNeXt152_vd_32x4d 换成自己的模型文件夹即可\n",
    "# !mv /home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl/best_model /home/aistudio/PaddleClas/output/best_model\n",
    "# !mv /home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl\n",
    "# !mkdir /home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl\n",
    "\n",
    "# # 1.1将训练好的best模型放回 -- 只需修改相应的模型路径即可\n",
    "# !mv /home/aistudio/PaddleClas/output/best_model /home/aistudio/PaddleClas/output/ResNeXt101_32x8d_wsl/best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T13:59:39.063622Z",
     "iopub.status.busy": "2021-11-18T13:59:39.062875Z",
     "iopub.status.idle": "2021-11-18T13:59:46.175763Z",
     "shell.execute_reply": "2021-11-18T13:59:46.174867Z",
     "shell.execute_reply.started": "2021-11-18T13:59:39.063589Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 模型保存 -- PaddleClas主目录里\n",
    "# ResNeXt101_32x8d_wsl -- 改成自己的模型文件夹\n",
    "!python tools/export_model.py \\\n",
    "    --model=ResNeXt101_32x8d_wsl \\\n",
    "    --pretrained_model=./output/ResNeXt101_32x8d_wsl/best_model/ppcls \\\n",
    "    --output_path=./models/ResNeXt101_32x8d_wsl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T13:52:04.229440Z",
     "iopub.status.busy": "2021-11-18T13:52:04.228953Z",
     "iopub.status.idle": "2021-11-18T13:52:05.463031Z",
     "shell.execute_reply": "2021-11-18T13:52:05.462305Z",
     "shell.execute_reply.started": "2021-11-18T13:52:04.229406Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: 无法获取'/home/aistudio/PaddleClas/model' 的文件状态(stat): 没有那个文件或目录\n",
      "mv: 无法获取'/home/aistudio/PaddleClas/params' 的文件状态(stat): 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "# # 2.0创建模型保存文件夹\n",
    "# !mkdir /home/aistudio/PaddleClas/models/ResNeXt101_32x8d_wsl\n",
    "# # 2.1模型文件转移 -- /home/aistudio/PaddleClas/model -- 为本次训练目标的不同预测模型的保存点\n",
    "# !mv /home/aistudio/PaddleClas/model /home/aistudio/PaddleClas/models/ResNeXt101_32x8d_wsl/model\n",
    "# !mv /home/aistudio/PaddleClas/params /home/aistudio/PaddleClas/models/ResNeXt101_32x8d_wsl/params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:01:21.102735Z",
     "iopub.status.busy": "2021-11-18T14:01:21.101756Z",
     "iopub.status.idle": "2021-11-18T14:01:24.655854Z",
     "shell.execute_reply": "2021-11-18T14:01:24.655084Z",
     "shell.execute_reply.started": "2021-11-18T14:01:21.102690Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:  0\n",
      "E1118 22:01:22.467455  9706 analysis_config.cc:231] To use TensorRT engine, please call EnableGpu() first\n",
      "2021-11-18 22:01:24,291-INFO: class: 12\n",
      "2021-11-18 22:01:24,291-INFO: score: 0.7409507036209106\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "!python tools/infer/predict.py \\\n",
    "    -m \"./models/ResNeXt101_32x8d_wsl/model\" \\\n",
    "    -p \"./models/ResNeXt101_32x8d_wsl/params\" \\\n",
    "    -i \"/home/aistudio/work/datasets/test/10.jpg\" \\\n",
    "    --use_gpu=0 \\\n",
    "    --use_tensorrt=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、开始搭建完整的预测部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:04:28.430315Z",
     "iopub.status.busy": "2021-11-18T14:04:28.429735Z",
     "iopub.status.idle": "2021-11-18T14:04:28.443467Z",
     "shell.execute_reply": "2021-11-18T14:04:28.442912Z",
     "shell.execute_reply.started": "2021-11-18T14:04:28.430281Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 图像数据处理方法  -- 来自util.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DecodeImage(object):\n",
    "    def __init__(self, to_rgb=True):\n",
    "        self.to_rgb = to_rgb\n",
    "\n",
    "    def __call__(self, img):\n",
    "        data = np.frombuffer(img, dtype='uint8')\n",
    "        img = cv2.imdecode(data, 1)\n",
    "        if self.to_rgb:\n",
    "            assert img.shape[2] == 3, 'invalid shape of image[%s]' % (\n",
    "                img.shape)\n",
    "            img = img[:, :, ::-1]\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class ResizeImage(object):\n",
    "    def __init__(self, resize_short=None):\n",
    "        self.resize_short = resize_short\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        percent = float(self.resize_short) / min(img_w, img_h)\n",
    "        w = int(round(img_w * percent))\n",
    "        h = int(round(img_h * percent))\n",
    "        return cv2.resize(img, (w, h))\n",
    "\n",
    "\n",
    "class CropImage(object):\n",
    "    def __init__(self, size):\n",
    "        if type(size) is int:\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = self.size\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        w_start = (img_w - w) // 2\n",
    "        h_start = (img_h - h) // 2\n",
    "\n",
    "        w_end = w_start + w\n",
    "        h_end = h_start + h\n",
    "        return img[h_start:h_end, w_start:w_end, :]\n",
    "\n",
    "\n",
    "class NormalizeImage(object):\n",
    "    def __init__(self, scale=None, mean=None, std=None):\n",
    "        self.scale = np.float32(scale if scale is not None else 1.0 / 255.0)\n",
    "        mean = mean if mean is not None else [0.485, 0.456, 0.406]\n",
    "        std = std if std is not None else [0.229, 0.224, 0.225]\n",
    "\n",
    "        shape = (1, 1, 3)\n",
    "        self.mean = np.array(mean).reshape(shape).astype('float32')\n",
    "        self.std = np.array(std).reshape(shape).astype('float32')\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return (img.astype('float32') * self.scale - self.mean) / self.std\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、批量预测引擎的部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:13:50.070860Z",
     "iopub.status.busy": "2021-11-18T14:13:50.070240Z",
     "iopub.status.idle": "2021-11-18T14:13:50.083955Z",
     "shell.execute_reply": "2021-11-18T14:13:50.083411Z",
     "shell.execute_reply.started": "2021-11-18T14:13:50.070826Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 预测引擎的构建 -- 借鉴predict.py\n",
    "from paddle.fluid.core import AnalysisConfig\n",
    "from paddle.fluid.core import create_paddle_predictor\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 1\n",
    "enable_benchmark = True\n",
    "use_gpu = True\n",
    "ir_optim = True\n",
    "use_tensorrt = False  # 不能加速，否则会报错--tensorrt没有注册点，解决方法还没查找到\n",
    "use_fp16 = True\n",
    "\n",
    "# 创建引擎\n",
    "def create_predictor(model_file, params_file, gpu_mem=8000):\n",
    "    config = AnalysisConfig(model_file, params_file)\n",
    "\n",
    "    if use_gpu:\n",
    "        config.enable_use_gpu(gpu_mem, 0)\n",
    "    else:\n",
    "        config.disable_gpu()\n",
    "    \n",
    "    gpu_id = config.gpu_device_id()\n",
    "    print('gpu: ',  gpu_id)\n",
    "\n",
    "    config.disable_glog_info()\n",
    "    config.switch_ir_optim(ir_optim)  # default true\n",
    "\n",
    "    if use_tensorrt:\n",
    "        config.enable_tensorrt_engine(\n",
    "            precision_mode=AnalysisConfig.Precision.Half\n",
    "            if use_fp16 else AnalysisConfig.Precision.Float32,\n",
    "            max_batch_size=batch_size\n",
    "            )  # use_calib_mode = True\n",
    "\n",
    "    config.enable_mkldnn()\n",
    "\n",
    "    config.enable_memory_optim()\n",
    "    # use zero copy\n",
    "    config.switch_use_feed_fetch_ops(False)\n",
    "    predictor = create_paddle_predictor(config)\n",
    "\n",
    "    return predictor\n",
    "\n",
    "\n",
    "# 创建图像处理迭代器\n",
    "def create_operators():\n",
    "    size = 224\n",
    "    img_mean = [0.485, 0.456, 0.406]\n",
    "    img_std = [0.229, 0.224, 0.225]\n",
    "    img_scale = 1.0 / 255.0\n",
    "\n",
    "    decode_op = DecodeImage()\n",
    "    resize_op = ResizeImage(resize_short=256)\n",
    "    crop_op = CropImage(size=(size, size))\n",
    "    normalize_op = NormalizeImage(\n",
    "        scale=img_scale, mean=img_mean, std=img_std)\n",
    "    totensor_op = ToTensor()\n",
    "\n",
    "    return [decode_op, resize_op, crop_op, normalize_op, totensor_op]\n",
    "\n",
    "\n",
    "# 传入图片文件、并通过迭代处理器进行图像处理\n",
    "def preprocess(fname, ops):\n",
    "    data = open(fname, 'rb').read()\n",
    "    for op in ops:\n",
    "        data = op(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# 预测引擎主函数\n",
    "def main(model_file, aparams_file):\n",
    "    \n",
    "    # 验证参数 -- 不用取消注释，不影响使用\n",
    "    # if not enable_benchmark:\n",
    "    #     assert batch_size == 1\n",
    "    #     assert use_fp16 is False\n",
    "    # else:\n",
    "    #     assert use_gpu is True\n",
    "    #     assert use_tensorrt is True\n",
    "    # # HALF precission predict only work when using tensorrt\n",
    "    # if use_fp16 is True:\n",
    "    #     assert use_tensorrt is True\n",
    "\n",
    "    # 预测结果保存\n",
    "    result = []\n",
    "\n",
    "    operators = create_operators()  # 处理迭代器\n",
    "    predictor = create_predictor(model_file, aparams_file)  # 预测引擎\n",
    "    \n",
    "    # 预测输入组件\n",
    "    input_names = predictor.get_input_names()\n",
    "    input_tensor = predictor.get_input_tensor(input_names[0])  # 输入参数的接口\n",
    "    \n",
    "    # 预测输出组件\n",
    "    output_names = predictor.get_output_names()\n",
    "    output_tensor = predictor.get_output_tensor(output_names[0])  # 输出参数的接口\n",
    "\n",
    "    # 数据传入进行预测  -- 在这里开始批量预测的循环设置 -- 这里改成自己合适的数据集传入即可 -- 注意batch_size为1\n",
    "    datasets_lens = 800    \n",
    "    show_predect = True  # 是否展示与预测结果\n",
    "    for i in range(datasets_lens):\n",
    "        image_file = '/home/aistudio/work/datasets/test/{0}.jpg'.format(i)  # 次数 正好对应 id\n",
    "\n",
    "        inputs = preprocess(image_file, operators)  # 数据处理\n",
    "\n",
    "        inputs = np.expand_dims(\n",
    "            inputs, axis=0).repeat(\n",
    "                batch_size, axis=0).copy()  # 数据批转化\n",
    "\n",
    "        input_tensor.copy_from_cpu(inputs)  # 复制数据到输入接口里\n",
    "\n",
    "        predictor.zero_copy_run()   # 运行预测引擎\n",
    "\n",
    "        output = output_tensor.copy_to_cpu()  # 获取输出接口的数据\n",
    "        output = output.flatten()  # 将数据展平 -- 由于batch_size为1， 所以，直接展品后进行argmax操作会更方便\n",
    "        cls = np.argmax(output)  # 得到类别\n",
    "        score = output[cls]     # 得到对应的得分\n",
    "        \n",
    "        result.append(cls)\n",
    "        # 可将每次的cls放入一个列表中返回，得到结果，然后保存或者其他操作\n",
    "\n",
    "        # print('cls: ', cls)\n",
    "        # print('score: ', score)\n",
    "    \n",
    "    print('Detection has done!')\n",
    "    return result  # 所有预测的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:13:52.942842Z",
     "iopub.status.busy": "2021-11-18T14:13:52.942277Z",
     "iopub.status.idle": "2021-11-18T14:14:11.682573Z",
     "shell.execute_reply": "2021-11-18T14:14:11.681936Z",
     "shell.execute_reply.started": "2021-11-18T14:13:52.942809Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1118 22:13:52.944423   174 paddle_pass_builder.cc:139] GPU not support MKLDNN yet\n",
      "E1118 22:13:52.944468   174 paddle_pass_builder.cc:139] GPU not support MKLDNN yet\n",
      "E1118 22:13:52.944620   174 paddle_pass_builder.cc:139] GPU not support MKLDNN yet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection has done!\n"
     ]
    }
   ],
   "source": [
    "# 执行预测器 -- 得到结果，返回result\n",
    "model_path = './models/ResNeXt101_32x8d_wsl/model'  # 根据需要自行修改模型路径\n",
    "param_path = './models/ResNeXt101_32x8d_wsl/params'\n",
    "result = main(model_path, param_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:14:43.136534Z",
     "iopub.status.busy": "2021-11-18T14:14:43.135900Z",
     "iopub.status.idle": "2021-11-18T14:14:43.160144Z",
     "shell.execute_reply": "2021-11-18T14:14:43.159574Z",
     "shell.execute_reply.started": "2021-11-18T14:14:43.136503Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 将结果拼接好，保存到csv文件中\n",
    "result = np.array(result).reshape(-1, 1)\n",
    "indexs = np.array(range(800)).reshape(-1, 1)\n",
    "\n",
    "save_datas = np.concatenate([indexs, result], axis=-1)\n",
    "savedfs = pds.DataFrame(save_datas)\n",
    "savedfs.to_csv('/home/aistudio/work/key.csv', header=None, index=None)  # 保存路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  训练-预测结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:16:42.341525Z",
     "iopub.status.busy": "2021-11-18T14:16:42.340752Z",
     "iopub.status.idle": "2021-11-18T14:16:45.395780Z",
     "shell.execute_reply": "2021-11-18T14:16:45.394973Z",
     "shell.execute_reply.started": "2021-11-18T14:16:42.341490Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/bf/f0f194d3379d3f3347478bd267f754fc68c11cbf2fe302a6ab69447b1417/beautifulsoup4-4.10.0-py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 4.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/72/a6/fd01694427f1c3fcadfdc5f1de901b813b9ac756f0806ef470cfed1de281/soupsieve-2.3.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.10.0 soupsieve-2.3.1\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, you need to use the persistence path as the following:\n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 下载clas--如果下载不是很快的，建议上传--有该文件夹之后不用下载解压\n",
    "!mkdir /home/aistudio/PaddleClas\n",
    "!cd /home/aistudio/PaddleClas\n",
    "!git clone https://github.com/PaddlePaddle/PaddleClas.git\n",
    "# 解压clas压缩包\n",
    "!unzip /home/aistudio/PaddleClas/paddlepaddle-PaddleClas-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-18T14:16:56.021402Z",
     "iopub.status.busy": "2021-11-18T14:16:56.020637Z",
     "iopub.status.idle": "2021-11-18T14:16:56.024608Z",
     "shell.execute_reply": "2021-11-18T14:16:56.023988Z",
     "shell.execute_reply.started": "2021-11-18T14:16:56.021369Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可:\n",
    "# Also add the following code, so that every time the environment (kernel) starts, just run the following code:\n",
    "import sys\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
